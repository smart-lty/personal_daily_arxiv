[
    {
        "title": "Dynamic Depth Decoding: Faster Speculative Decoding for LLMs",
        "summary": "The acceleration of Large Language Models (LLMs) with speculative decoding provides a significant runtime improvement without any loss of accuracy. Currently, EAGLE-2 is the state-of-the-art speculative decoding method, improving on EAGLE with a dynamic draft tree. We introduce Dynamic Depth Decoding (DDD), which optimises EAGLE-2's tree drafting method using a dynamic depth. This extends the average speedup that EAGLE-2 achieves over EAGLE by $44\\%$, giving DDD an average speedup of $3.16$x.",
        "authors": "Oscar Brown, Zhengjie Wang, Andrea Do, Nikhil Mathew, Cheng Yu",
        "published": "2024-08-30",
        "link": "http://arxiv.org/abs/2409.00142v1",
        "chinese_summary": "\u4f7f\u7528\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u884c\u65f6\u6027\u80fd\u3002\u76ee\u524d\uff0cEAGLE-2\u662f\u63a8\u6d4b\u89e3\u7801\u9886\u57df\u7684\u6700\u65b0\u6280\u672f\uff0c\u5b83\u5728EAGLE\u7684\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u52a8\u6001\u8349\u7a3f\u6811\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u52a8\u6001\u6df1\u5ea6\u89e3\u7801\uff08Dynamic Depth Decoding, DDD\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u6df1\u5ea6\u4f18\u5316\u4e86EAGLE-2\u7684\u6811\u8349\u7a3f\u65b9\u6cd5\u3002\u8fd9\u4f7f\u5f97DDD\u5728EAGLE-2\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574744%\u7684\u52a0\u901f\uff0c\u603b\u4f53\u5e73\u5747\u52a0\u901f\u8fbe\u5230\u4e863.16\u500d\u3002",
        "tldr_en": "Dynamic Depth Decoding (DDD) enhances EAGLE-2's speculative decoding, achieving a 44% higher average speedup of 3.16x without accuracy loss.",
        "tldr_zh": "\u52a8\u6001\u6df1\u5ea6\u89e3\u7801\uff08DDD\uff09\u901a\u8fc7\u4f18\u5316EAGLE-2\u7684\u52a8\u6001\u8349\u7a3f\u6811\u65b9\u6cd5\uff0c\u5c06\u5e73\u5747\u52a0\u901f\u6bd4\u63d0\u534744%\uff0c\u8fbe\u52303.16\u500d\u3002"
    },
    {
        "title": "Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation",
        "summary": "Lossless speculative decoding accelerates target large language model (LLM) inference by employing a lightweight draft model for generating tree-structured candidates, which are subsequently verified in parallel by the target LLM. Currently, effective approaches leverage feature-level rather than token-level autoregression within the draft model to facilitate more straightforward predictions and enhanced knowledge distillation. In this paper, we reassess these approaches and propose FSPAD (Feature Sampling and Partial Alignment Distillation for Lossless Speculative Decoding), which introduces two straightforward and effective components within the existing framework to boost lossless speculative decoding. Firstly, FSPAD utilizes token embeddings to sample features of the target LLM in high-dimensional space before feeding them into the draft model, due to the inherent uncertainty of the features preventing the draft model from obtaining the specific token output by the target LLM. Secondly, FSPAD introduces partial alignment distillation to weaken the draft model's connection between features and logits, aiming to reduce the conflict between feature alignment and logit confidence during training. Our experiments include both greedy and non-greedy decoding on the largest and smallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in multi-turn conversation, translation, summarization, question answering, mathematical reasoning, and retrieval-augmented generation. The results show that FSPAD outperforms the state-of-the-art method across all the aforementioned tasks and target LLMs.",
        "authors": "Lujun Gui, Bin Xiao, Lei Su, Weipeng Chen",
        "published": "2024-08-28",
        "link": "http://arxiv.org/abs/2408.15562v1",
        "chinese_summary": "\u65e0\u635f\u63a8\u6d4b\u89e3\u7801\u901a\u8fc7\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u751f\u6210\u6811\u7ed3\u6784\u5019\u9009\uff0c\u5e76\u7531\u76ee\u6807\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e76\u884c\u9a8c\u8bc1\uff0c\u4ece\u800c\u52a0\u901f\u76ee\u6807LLM\u7684\u63a8\u7406\u3002\u5f53\u524d\u6709\u6548\u7684\u65b9\u6cd5\u5229\u7528\u8349\u7a3f\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u7ea7\u800c\u975e\u4ee4\u724c\u7ea7\u81ea\u56de\u5f52\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u76f4\u63a5\u7684\u9884\u6d4b\u548c\u589e\u5f3a\u7684\u77e5\u8bc6\u84b8\u998f\u3002\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86FSPAD\uff08\u65e0\u635f\u63a8\u6d4b\u89e3\u7801\u7684\u7279\u5f81\u91c7\u6837\u4e0e\u90e8\u5206\u5bf9\u9f50\u84b8\u998f\uff09\uff0c\u5728\u73b0\u6709\u6846\u67b6\u4e2d\u5f15\u5165\u4e86\u4e24\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u7ec4\u4ef6\uff0c\u4ee5\u63d0\u5347\u65e0\u635f\u63a8\u6d4b\u89e3\u7801\u7684\u6548\u679c\u3002\u9996\u5148\uff0cFSPAD\u5229\u7528\u4ee4\u724c\u5d4c\u5165\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u91c7\u6837\u76ee\u6807LLM\u7684\u7279\u5f81\uff0c\u7136\u540e\u5c06\u5176\u8f93\u5165\u8349\u7a3f\u6a21\u578b\uff0c\u56e0\u4e3a\u7279\u5f81\u7684\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\u4f7f\u5f97\u8349\u7a3f\u6a21\u578b\u65e0\u6cd5\u901a\u8fc7\u8fd9\u4e9b\u7279\u5f81\u83b7\u5f97\u76ee\u6807LLM\u7684\u5177\u4f53\u4ee4\u724c\u8f93\u51fa\u3002\u5176\u6b21\uff0cFSPAD\u5f15\u5165\u4e86\u90e8\u5206\u5bf9\u9f50\u84b8\u998f\uff0c\u4ee5\u524a\u5f31\u8349\u7a3f\u6a21\u578b\u4e2d\u7279\u5f81\u4e0e\u5bf9\u6570\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u65e8\u5728\u51cf\u5c11\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7279\u5f81\u5bf9\u9f50\u4e0e\u5bf9\u6570\u7f6e\u4fe1\u5ea6\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u5305\u62ec\u5728Vicuna\u548cLLaMA3-Instruct\u7cfb\u5217\u7684\u6700\u5927\u548c\u6700\u5c0f\u6a21\u578b\u4e0a\u8fdb\u884c\u8d2a\u5a6a\u548c\u975e\u8d2a\u5a6a\u89e3\u7801\uff0c\u4ee5\u53ca\u591a\u8f6e\u5bf9\u8bdd\u3001\u7ffb\u8bd1\u3001\u6458\u8981\u3001\u95ee\u7b54\u3001\u6570\u5b66\u63a8\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b49\u4efb\u52a1\u3002\u7ed3\u679c\u663e\u793a\uff0cFSPAD\u5728\u6240\u6709\u4e0a\u8ff0\u4efb\u52a1\u548c\u76ee\u6807LLM\u4e2d\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002",
        "tldr_en": "FSPAD enhances lossless speculative decoding by sampling target LLM features and introducing partial alignment distillation, outperforming state-of-the-art methods across various tasks and models.",
        "tldr_zh": "FSPAD\u901a\u8fc7\u5f15\u5165\u7279\u5f81\u91c7\u6837\u548c\u90e8\u5206\u5bf9\u9f50\u84b8\u998f\uff0c\u4f18\u5316\u4e86\u65e0\u635f\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"
    },
    {
        "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
        "summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.",
        "authors": "Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao",
        "published": "2024-08-27",
        "link": "http://arxiv.org/abs/2408.15237v1",
        "chinese_summary": "\u7ebf\u6027RNN\u67b6\u6784\uff0c\u5982Mamba\uff0c\u5728\u8bed\u8a00\u5efa\u6a21\u65b9\u9762\u53ef\u4ee5\u4e0eTransformer\u6a21\u578b\u7ade\u4e89\uff0c\u540c\u65f6\u5177\u6709\u4f18\u52bf\u7684\u90e8\u7f72\u7279\u6027\u3002\u9274\u4e8e\u5f53\u524d\u5bf9\u8bad\u7ec3\u5927\u89c4\u6a21Transformer\u6a21\u578b\u7684\u5173\u6ce8\uff0c\u6211\u4eec\u8003\u8651\u4e86\u5c06\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u8f6c\u6362\u4e3a\u90e8\u7f72\u6a21\u578b\u7684\u6311\u6218\u3002\u6211\u4eec\u8bc1\u660e\u4e86\uff0c\u901a\u8fc7\u91cd\u7528\u6ce8\u610f\u529b\u5c42\u7684\u7ebf\u6027\u6295\u5f71\u6743\u91cd\uff0c\u5229\u7528\u5b66\u672fGPU\u8d44\u6e90\u5c06\u5927\u578bTransformer\u6a21\u578b\u84b8\u998f\u4e3a\u7ebf\u6027RNN\u662f\u53ef\u884c\u7684\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6df7\u5408\u6a21\u578b\uff0c\u5305\u542b\u4e86\u56db\u5206\u4e4b\u4e00\u7684\u6ce8\u610f\u529b\u5c42\uff0c\u5728\u804a\u5929\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u4e0e\u539f\u59cbTransformer\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u804a\u5929\u57fa\u51c6\u6d4b\u8bd5\u548c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u6df7\u5408Mamba\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u8bad\u7ec3\u4e86\u6570\u4e07\u4ebf\u4e2atoken\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u52a0\u901f\u4e86Mamba\u548c\u6df7\u5408\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u5728\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0c\u79fb\u9664\u8bb8\u591a\u539f\u59cb\u7684\u6ce8\u610f\u529b\u5c42\uff0c\u5e76\u66f4\u9ad8\u6548\u5730\u4ece\u751f\u6210\u7684\u6a21\u578b\u4e2d\u8fdb\u884c\u63a8\u7406\u3002\u6211\u4eec\u4eceLlama3-8B-Instruct\u84b8\u998f\u51fa\u7684\u9876\u7ea7\u6a21\u578b\uff0c\u5728AlpacaEval 2\u4e0a\u4ee529.61\u7684\u957f\u5ea6\u63a7\u5236\u80dc\u7387\u5bf9\u6297GPT-4\uff0c\u5e76\u5728MT-Bench\u4e0a\u8fbe\u52307.35\uff0c\u8d85\u8fc7\u4e86\u6700\u4f73\u6307\u4ee4\u8c03\u4f18\u7684\u7ebf\u6027RNN\u6a21\u578b\u3002",
        "tldr_en": "We demonstrate the feasibility of distilling large Transformers into efficient linear RNNs, achieving superior performance and faster inference with limited resources, outperforming both open-source models and GPT-4 in benchmarks.",
        "tldr_zh": "\u901a\u8fc7\u91cd\u7528\u6ce8\u610f\u529b\u5c42\u7684\u7ebf\u6027\u6295\u5f71\u6743\u91cd\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5728\u5b66\u672fGPU\u8d44\u6e90\u4e0b\uff0c\u5c06\u5927\u578bTransformer\u6a21\u578b\u84b8\u998f\u6210\u7ebf\u6027RNN\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u751f\u6210\u7684\u6df7\u5408\u6a21\u578b\u5728\u804a\u5929\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0e\u539fTransformer\u76f8\u5f53\uff0c\u5e76\u5728\u4e00\u822c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4ece\u5934\u8bad\u7ec3\u7684\u5f00\u6e90\u6df7\u5408Mamba\u6a21\u578b\uff0c\u540c\u65f6\u5f15\u5165\u786c\u4ef6\u611f\u77e5\u7684\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\u52a0\u901f\u63a8\u7406\u901f\u5ea6\u3002"
    },
    {
        "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding",
        "summary": "Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft models with sparse KV cache to address the KV bottleneck that scales with both sequence length and batch size. This finding underscores the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2x speedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available at https://github.com/Infini-AI-Lab/MagicDec/.",
        "authors": "Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen",
        "published": "2024-08-20",
        "link": "http://arxiv.org/abs/2408.11049v3",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u5982\u4ea4\u4e92\u5f0f\u804a\u5929\u673a\u5668\u4eba\u3001\u6587\u6863\u5206\u6790\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f46\u8981\u5728\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u8bf7\u6c42\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u63a8\u6d4b\u6027\u89e3\u7801\uff08SD\uff09\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u5ef6\u8fdf\uff0c\u4f46\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u5176\u6709\u6548\u6027\u4ec5\u9650\u4e8e\u5c0f\u6279\u91cf\u5927\u5c0f\u3002\u5728MagicDec\u4e2d\uff0c\u6211\u4eec\u60ca\u8bb6\u5730\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u9002\u4e2d\u81f3\u957f\u5e8f\u5217\u7684\u9ad8\u541e\u5410\u91cf\u63a8\u7406\u573a\u666f\u4e2d\uff0cSD\u4e5f\u80fd\u5b9e\u73b0\u52a0\u901f\u3002\u66f4\u6709\u8da3\u7684\u662f\uff0c\u6839\u636e\u6211\u4eec\u4e25\u8c28\u7684\u5206\u6790\uff0c\u667a\u80fd\u8349\u7a3f\u7b56\u7565\u53ef\u4ee5\u5728\u589e\u52a0\u6279\u91cf\u5927\u5c0f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u52a0\u901f\u3002MagicDec\u9996\u5148\u8bc6\u522b\u51fa\u968f\u7740\u6279\u91cf\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u74f6\u9888\u4f1a\u53d1\u751f\u53d8\u5316\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6d1e\u5bdf\u66f4\u6709\u6548\u5730\u90e8\u7f72\u63a8\u6d4b\u6027\u89e3\u7801\u4ee5\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u63a8\u7406\u3002\u7136\u540e\uff0c\u5b83\u5229\u7528\u5177\u6709\u7a00\u758fKV\u7f13\u5b58\u7684\u8349\u7a3f\u6a21\u578b\u6765\u89e3\u51b3\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u548c\u6279\u91cf\u5927\u5c0f\u6269\u5c55\u7684KV\u74f6\u9888\u3002\u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86\u63a8\u6d4b\u6027\u89e3\u7801\u5728\u957f\u4e0a\u4e0b\u6587\u670d\u52a1\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u541e\u5410\u91cf\u5e76\u51cf\u5c11\u5ef6\u8fdf\u3002\u5bf9\u4e8e\u9002\u4e2d\u81f3\u957f\u7684\u5e8f\u5217\uff0c\u6211\u4eec\u57288\u4e2aNVIDIA A100 GPU\u4e0a\u5904\u7406\u6279\u91cf\u5927\u5c0f\u4ece32\u5230256\u7684\u8bf7\u6c42\u65f6\uff0c\u5c55\u793a\u4e86LLaMA-2-7B-32K\u6700\u9ad82\u500d\u7684\u52a0\u901f\u548cLLaMA-3.1-8B\u6700\u9ad81.84\u500d\u7684\u52a0\u901f\u3002\u4ee3\u7801\u53ef\u5728https://github.com/Infini-AI-Lab/MagicDec/\u83b7\u53d6\u3002",
        "tldr_en": "MagicDec demonstrates that speculative decoding can achieve significant speedup for high throughput inference on long sequences, even with large batch sizes, by optimizing bottleneck shifts and leveraging sparse KV cache.",
        "tldr_zh": "MagicDec \u901a\u8fc7\u667a\u80fd\u8349\u7a3f\u7b56\u7565\u548c\u7a00\u758f KV \u7f13\u5b58\uff0c\u5728\u957f\u5e8f\u5217\u548c\u9ad8\u541e\u5410\u91cf\u63a8\u7406\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u901f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 2 \u500d\u7684\u52a0\u901f\u3002"
    },
    {
        "title": "Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling",
        "summary": "The rapid growth in the parameters of large language models (LLMs) has made inference latency a fundamental bottleneck, limiting broader application of LLMs. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm, leveraging the parallel capabilities of modern hardware. Some speculative decoding methods rely on additional structures to guess draft tokens, such as small models or parameter-efficient architectures, which need extra training before use. Alternatively, retrieval-based train-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. This approach stores candidate tokens in an adjacency matrix and employs a breadth-first search (BFS)-like algorithm on the matrix to construct a draft tree. The tree is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \\textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\\% and even a training method by 25\\%. It can be directly applied to any existing LLMs and tasks without the need for adaptation.",
        "authors": "Xianzhen Luo, Yixuan Wang, Qingfu Zhu, Zhiming Zhang, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che",
        "published": "2024-08-16",
        "link": "http://arxiv.org/abs/2408.08696v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53c2\u6570\u7684\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u63a8\u7406\u5ef6\u8fdf\u6210\u4e3a\u4e00\u4e2a\u6839\u672c\u6027\u7684\u74f6\u9888\uff0c\u9650\u5236\u4e86LLMs\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002\u63a8\u6d4b\u6027\u89e3\u7801\u901a\u8fc7\u731c\u6d4b-\u9a8c\u8bc1\u8303\u5f0f\uff0c\u5229\u7528\u73b0\u4ee3\u786c\u4ef6\u7684\u5e76\u884c\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u635f\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6cd5\u3002\u4e00\u4e9b\u63a8\u6d4b\u6027\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u989d\u5916\u7684\u7ed3\u6784\u6765\u731c\u6d4b\u8349\u7a3f\u6807\u8bb0\uff0c\u5982\u5c0f\u578b\u6a21\u578b\u6216\u53c2\u6570\u9ad8\u6548\u67b6\u6784\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4f7f\u7528\u524d\u9700\u8981\u989d\u5916\u7684\u8bad\u7ec3\u3002\u53e6\u4e00\u79cd\u9009\u62e9\u662f\u57fa\u4e8e\u68c0\u7d22\u7684\u65e0\u8bad\u7ec3\u6280\u672f\uff0c\u5b83\u4eec\u901a\u8fc7\u4ece\u73b0\u6709\u8bed\u6599\u5e93\u4e2d\u6784\u5efa\u5e93\u6216\u901a\u8fc7n-gram\u751f\u6210\u6765\u5b9e\u73b0\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9762\u4e34\u5b58\u50a8\u9700\u6c42\u5927\u3001\u68c0\u7d22\u8017\u65f6\u548c\u9002\u5e94\u6027\u6709\u9650\u7b49\u6311\u6218\u3002\u89c2\u5bdf\u5230\u89e3\u7801\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u5019\u9009\u6807\u8bb0\u5f88\u53ef\u80fd\u5728\u672a\u6765\u7684\u5e8f\u5217\u4e2d\u518d\u6b21\u51fa\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6807\u8bb0\u56de\u6536\uff08Token Recycling\uff09\u3002\u8be5\u65b9\u6cd5\u5c06\u5019\u9009\u6807\u8bb0\u5b58\u50a8\u5728\u4e00\u4e2a\u90bb\u63a5\u77e9\u9635\u4e2d\uff0c\u5e76\u91c7\u7528\u7c7b\u4f3c\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\uff08BFS\uff09\u7684\u7b97\u6cd5\u5728\u77e9\u9635\u4e0a\u6784\u5efa\u8349\u7a3f\u6811\u3002\u7136\u540e\u901a\u8fc7\u6811\u6ce8\u610f\u529b\u5bf9\u6811\u8fdb\u884c\u9a8c\u8bc1\u3002\u89e3\u7801\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u65b0\u5019\u9009\u6807\u8bb0\u968f\u540e\u7528\u4e8e\u66f4\u65b0\u77e9\u9635\u3002\u6807\u8bb0\u56de\u6536\u4ec5\u9700\u4e0d\u52302MB\u7684\u989d\u5916\u5b58\u50a8\uff0c\u5e76\u5728\u6240\u6709\u89c4\u6a21\u7684LLMs\u4e0a\u5b9e\u73b0\u4e86\u7ea62\u500d\u7684\u52a0\u901f\u3002\u5b83\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd530%\uff0c\u751a\u81f3\u4f18\u4e8e\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd525%\u3002\u5b83\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u4efb\u4f55\u73b0\u6709\u7684LLMs\u548c\u4efb\u52a1\uff0c\u65e0\u9700\u9002\u5e94\u3002",
        "tldr_en": "Token Recycling accelerates large language model inference by reusing candidate tokens in a BFS-like draft tree, requiring <2MB storage and achieving up to 2x speedup, outperforming existing methods.",
        "tldr_zh": "Token Recycling \u662f\u4e00\u79cd\u65e0\u635f\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u5019\u9009\u8bcd\u5e76\u6784\u5efa\u8349\u7a3f\u6811\uff0c\u5b9e\u73b0\u4e86\u7ea62\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u9002\u5e94\u73b0\u6709\u6a21\u578b\u548c\u4efb\u52a1\u3002"
    },
    {
        "title": "KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning",
        "summary": "Large Language Models (LLMs) exhibit high inference latency due to their autoregressive decoding nature. While the draft head in speculative decoding mitigates this issue, its full potential remains unexplored. In this paper, we introduce KOALA (K-layer Optimized Adversarial Learning Architecture), an orthogonal approach to the draft head. By transforming the conventional single-layer draft head into a multi-layer architecture and incorporating adversarial learning into the traditional supervised training, KOALA significantly improves the accuracy of the draft head in predicting subsequent tokens, thus more closely mirroring the functionality of LLMs. Although this improvement comes at the cost of slightly increased drafting overhead, KOALA substantially unlocks the draft head's potential, greatly enhancing speculative decoding. We conducted comprehensive evaluations of KOALA, including both autoregressive and non-autoregressive draft heads across various tasks, demonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is 10.57%-14.09% faster than the original draft heads.",
        "authors": "Kaiqi Zhang, Jing Zhao, Rui Chen",
        "published": "2024-08-15",
        "link": "http://arxiv.org/abs/2408.08146v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7531\u4e8e\u5176\u81ea\u56de\u5f52\u89e3\u7801\u7684\u7279\u6027\uff0c\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u5c3d\u7ba1\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u8349\u7a3f\u5934\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5176\u5168\u90e8\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u53d1\u6398\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86KOALA\uff08K\u5c42\u4f18\u5316\u5bf9\u6297\u5b66\u4e60\u67b6\u6784\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4e0e\u8349\u7a3f\u5934\u6b63\u4ea4\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u4f20\u7edf\u7684\u5355\u5c42\u8349\u7a3f\u5934\u8f6c\u53d8\u4e3a\u591a\u5c42\u67b6\u6784\uff0c\u5e76\u5c06\u5bf9\u6297\u5b66\u4e60\u878d\u5165\u4f20\u7edf\u7684\u76d1\u7763\u8bad\u7ec3\u4e2d\uff0cKOALA\u663e\u8457\u63d0\u9ad8\u4e86\u8349\u7a3f\u5934\u9884\u6d4b\u540e\u7eed\u6807\u8bb0\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u66f4\u63a5\u8fd1LLMs\u7684\u529f\u80fd\u3002\u5c3d\u7ba1\u8fd9\u4e00\u6539\u8fdb\u5e26\u6765\u4e86\u7565\u5fae\u589e\u52a0\u7684\u8349\u7a3f\u5f00\u9500\uff0c\u4f46KOALA\u5b9e\u8d28\u4e0a\u91ca\u653e\u4e86\u8349\u7a3f\u5934\u7684\u6f5c\u529b\uff0c\u6781\u5927\u5730\u589e\u5f3a\u4e86\u63a8\u6d4b\u89e3\u7801\u3002\u6211\u4eec\u5bf9KOALA\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u5305\u62ec\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5bf9\u81ea\u56de\u5f52\u548c\u975e\u81ea\u56de\u5f52\u8349\u7a3f\u5934\u7684\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5ef6\u8fdf\u52a0\u901f\u6bd4\u63d0\u9ad8\u4e860.24x-0.41x\uff0c\u6bd4\u539f\u59cb\u8349\u7a3f\u5934\u5feb10.57%-14.09%\u3002",
        "tldr_en": "KOALA, a multi-layer adversarial learning architecture, significantly enhances speculative decoding by improving draft head accuracy and reducing latency by up to 14.09%.",
        "tldr_zh": "KOALA\u901a\u8fc7\u591a\u5c42\u4f18\u5316\u5bf9\u6297\u5b66\u4e60\u67b6\u6784\u663e\u8457\u63d0\u5347\u63a8\u6d4b\u89e3\u7801\u4e2d\u8349\u7a3f\u5934\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u867d\u589e\u52a0\u5c11\u91cf\u5f00\u9500\uff0c\u4f46\u5927\u5e45\u63d0\u5347\u89e3\u7801\u6548\u7387\uff0c\u5b9e\u73b00.24x-0.41x\u7684\u5ef6\u8fdf\u52a0\u901f\u6bd4\u63d0\u5347\u3002"
    },
    {
        "title": "Coupling without Communication and Drafter-Invariant Speculative Decoding",
        "summary": "Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice wants to generate a sample $a\\sim P$ and Bob a sample $b \\sim Q$ such that $a = b$ with has as high of probability as possible. It is well-known that, by sampling from an optimal coupling between the distributions, Alice and Bob can achieve $Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation distance. What if Alice and Bob must solve this same problem without communicating at all? Perhaps surprisingly, with access to public randomness, they can still achieve $Pr[a=b] \\geq \\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)} \\geq 1-2D_{TV}(P,Q)$. In fact, this bound can be obtained using a simple protocol based on the Weighted MinHash algorithm. In this work, we explore the communication-free coupling problem in greater depth. First, we show that an equally simple protocol based on Gumbel sampling matches the worst-case guarantees of the Weighted MinHash approach, but tends to perform better in practice. Conversely, we prove that both approaches are actually sharp: no communication-free protocol can achieve $Pr[a=b]>\\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$ in the worst-case. Finally, we prove that, for distributions over $n$ items, there exists a scheme that uses just $O(\\log(n/\\epsilon))$ bits of communication to achieve $Pr[a = b] = 1 - D_{TV}(P,Q) - \\epsilon$, i.e. to essentially match optimal coupling. Beyond our theoretical results, we demonstrate an application of communication-free coupling to speculative decoding, a recent method for accelerating autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We show that communication-free protocols yield a variant of speculative decoding that we call Drafter-Invariant Speculative Decoding, which has the desirable property that the output of the method is fixed given a fixed random seed, regardless of what drafter is used for speculation.",
        "authors": "Majid Daliri, Christopher Musco, Ananda Theertha Suresh",
        "published": "2024-08-15",
        "link": "http://arxiv.org/abs/2408.07978v2",
        "chinese_summary": "\u5047\u8bbeAlice\u6709\u4e00\u4e2a\u5206\u5e03$P$\uff0cBob\u6709\u4e00\u4e2a\u5206\u5e03$Q$\u3002Alice\u5e0c\u671b\u751f\u6210\u4e00\u4e2a\u6837\u672c$a \\sim P$\uff0c\u800cBob\u751f\u6210\u4e00\u4e2a\u6837\u672c$b \\sim Q$\uff0c\u4f7f\u5f97$a = b$\u7684\u6982\u7387\u5c3d\u53ef\u80fd\u9ad8\u3002\u4f17\u6240\u5468\u77e5\uff0c\u901a\u8fc7\u4ece\u5206\u5e03\u4e4b\u95f4\u7684\u6700\u4f18\u8026\u5408\u4e2d\u91c7\u6837\uff0cAlice\u548cBob\u53ef\u4ee5\u5b9e\u73b0$Pr[a = b] = 1 - D_{TV}(P,Q)$\uff0c\u5176\u4e2d$D_{TV}(P,Q)$\u662f\u603b\u53d8\u5dee\u8ddd\u79bb\u3002\u5982\u679cAlice\u548cBob\u5fc5\u987b\u5728\u4e0d\u8fdb\u884c\u4efb\u4f55\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u89e3\u51b3\u76f8\u540c\u7684\u95ee\u9898\uff0c\u60c5\u51b5\u4f1a\u5982\u4f55\u5462\uff1f\u6216\u8bb8\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u901a\u8fc7\u4f7f\u7528\u516c\u5171\u968f\u673a\u6027\uff0c\u4ed6\u4eec\u4ecd\u7136\u53ef\u4ee5\u5b9e\u73b0$Pr[a=b] \\geq \\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)} \\geq 1-2D_{TV}(P,Q)$\u3002\u4e8b\u5b9e\u4e0a\uff0c\u8fd9\u4e2a\u754c\u9650\u53ef\u4ee5\u901a\u8fc7\u57fa\u4e8e\u52a0\u6743MinHash\u7b97\u6cd5\u7684\u7b80\u5355\u534f\u8bae\u83b7\u5f97\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u66f4\u6df1\u5165\u5730\u63a2\u8ba8\u4e86\u65e0\u901a\u4fe1\u8026\u5408\u95ee\u9898\u3002\u9996\u5148\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u4e2a\u57fa\u4e8eGumbel\u91c7\u6837\u7684\u540c\u6837\u7b80\u5355\u7684\u534f\u8bae\uff0c\u5b83\u4e0e\u52a0\u6743MinHash\u65b9\u6cd5\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u76f8\u5339\u914d\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u8868\u73b0\u66f4\u597d\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5b9e\u9645\u4e0a\u90fd\u662f\u5c16\u9510\u7684\uff1a\u6ca1\u6709\u4efb\u4f55\u65e0\u901a\u4fe1\u534f\u8bae\u53ef\u4ee5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u5b9e\u73b0$Pr[a=b]>\\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\uff0c\u5bf9\u4e8e$n$\u4e2a\u9879\u76ee\u7684\u5206\u5e03\uff0c\u5b58\u5728\u4e00\u4e2a\u65b9\u6848\uff0c\u53ea\u9700\u4f7f\u7528$O(\\log(n/\\epsilon))$\u6bd4\u7279\u7684\u901a\u4fe1\uff0c\u5c31\u80fd\u5b9e\u73b0$Pr[a = b] = 1 - D_{TV}(P,Q) - \\epsilon$\uff0c\u5373\u57fa\u672c\u4e0a\u5339\u914d\u6700\u4f18\u8026\u5408\u3002\u9664\u4e86\u6211\u4eec\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u65e0\u901a\u4fe1\u8026\u5408\u5728\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u5e94\u7528\uff0c\u8fd9\u662f\u4e00\u79cd\u6700\u8fd1\u7528\u4e8e\u52a0\u901f\u81ea\u56de\u5f52\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5[Leviathan, Kalman, Matias, ICML 2023]\u3002\u6211\u4eec\u5c55\u793a\u4e86\u65e0\u901a\u4fe1\u534f\u8bae\u4ea7\u751f\u4e86\u4e00\u79cd\u6211\u4eec\u79f0\u4e4b\u4e3aDrafter-Invariant Speculative Decoding\u7684\u63a8\u6d4b\u89e3\u7801\u53d8\u4f53\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4e00\u4e2a\u7406\u60f3\u7684\u7279\u6027\uff0c\u5373\u65e0\u8bba\u4f7f\u7528\u4f55\u79cd\u63a8\u6d4b\u8005\uff0c\u7ed9\u5b9a\u56fa\u5b9a\u7684\u968f\u673a\u79cd\u5b50\uff0c\u65b9\u6cd5\u7684\u8f93\u51fa\u662f\u56fa\u5b9a\u7684\u3002",
        "tldr_en": "Alice and Bob can achieve a high probability of matching samples without communication, using public randomness, with a bound of $\\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$, and can match optimal coupling with $O(\\log(n/\\epsilon))$ bits of communication.",
        "tldr_zh": "\u5728\u6ca1\u6709\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\uff0cAlice\u548cBob\u5229\u7528\u516c\u5171\u968f\u673a\u6027\u4ecd\u80fd\u8fbe\u5230$Pr[a=b] \\geq \\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$\u7684\u6982\u7387\u5339\u914d\uff0c\u4e14\u5b58\u5728\u901a\u4fe1\u65b9\u6848\u4ee5$O(\\log(n/\\epsilon))$\u6bd4\u7279\u5b9e\u73b0\u8fd1\u4f3c\u6700\u4f18\u8026\u5408\u3002"
    },
    {
        "title": "Parallel Speculative Decoding with Adaptive Draft Length",
        "summary": "Speculative decoding (SD), where an extra draft model is employed to provide multiple \\textit{draft} tokens first and then the original target model verifies these tokens in parallel, has shown great power for LLM inference acceleration. However, existing SD methods suffer from the mutual waiting problem, i.e., the target model gets stuck when the draft model is \\textit{guessing} tokens, and vice versa. This problem is directly incurred by the asynchronous execution of the draft model and the target model, and is exacerbated due to the fixed draft length in speculative decoding. To address these challenges, we propose a conceptually simple, flexible, and general framework to boost speculative decoding, namely \\textbf{P}arallel sp\\textbf{E}culative decoding with \\textbf{A}daptive d\\textbf{R}aft \\textbf{L}ength (PEARL). Specifically, PEARL proposes \\textit{pre-verify} to verify the first draft token in advance during the drafting phase, and \\textit{post-verify} to generate more draft tokens during the verification phase. PEARL parallels the drafting phase and the verification phase via applying the two strategies, and achieves adaptive draft length for different scenarios, which effectively alleviates the mutual waiting problem. Moreover, we theoretically demonstrate that the mean accepted tokens of PEARL is more than existing \\textit{draft-then-verify} works. Experiments on various text generation benchmarks demonstrate the effectiveness of our \\name, leading to a superior speedup performance up to \\textbf{3.79$\\times$} and \\textbf{1.52$\\times$}, compared to auto-regressive decoding and vanilla speculative decoding, respectively.",
        "authors": "Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, Winston Hu",
        "published": "2024-08-13",
        "link": "http://arxiv.org/abs/2408.11850v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\uff08Speculative Decoding, SD\uff09\u662f\u4e00\u79cd\u5229\u7528\u989d\u5916\u7684\u8349\u7a3f\u6a21\u578b\u9996\u5148\u751f\u6210\u591a\u4e2a\\textit{\u8349\u7a3f}\u6807\u8bb0\uff0c\u7136\u540e\u539f\u59cb\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\u8fd9\u4e9b\u6807\u8bb0\u7684\u65b9\u6cd5\uff0c\u5df2\u663e\u793a\u51fa\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u52a0\u901f\u7684\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684SD\u65b9\u6cd5\u5b58\u5728\u76f8\u4e92\u7b49\u5f85\u7684\u95ee\u9898\uff0c\u5373\u5f53\u8349\u7a3f\u6a21\u578b\u5728\\textit{\u731c\u6d4b}\u6807\u8bb0\u65f6\uff0c\u76ee\u6807\u6a21\u578b\u4f1a\u9677\u5165\u505c\u6ede\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u8fd9\u4e00\u95ee\u9898\u76f4\u63a5\u6e90\u4e8e\u8349\u7a3f\u6a21\u578b\u548c\u76ee\u6807\u6a21\u578b\u6267\u884c\u7684\u5f02\u6b65\u6027\uff0c\u5e76\u4e14\u7531\u4e8e\u63a8\u6d4b\u6027\u89e3\u7801\u4e2d\u56fa\u5b9a\u7684\u8349\u7a3f\u957f\u5ea6\u800c\u52a0\u5267\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u5ff5\u7b80\u5355\u3001\u7075\u6d3b\u4e14\u901a\u7528\u7684\u6846\u67b6\u6765\u63d0\u5347\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u5373\\textbf{P}\u5e76\u884c\\textbf{E}\u63a8\u6d4b\u6027\u89e3\u7801\u4e0e\\textbf{A}\u9002\u5e94\u6027\\textbf{R}\u8349\u7a3f\\textbf{L}\u957f\u5ea6\uff08PEARL\uff09\u3002\u5177\u4f53\u800c\u8a00\uff0cPEARL\u63d0\u51fa\u4e86\\textit{\u9884\u9a8c\u8bc1}\uff0c\u5728\u8349\u7a3f\u9636\u6bb5\u63d0\u524d\u9a8c\u8bc1\u7b2c\u4e00\u4e2a\u8349\u7a3f\u6807\u8bb0\uff0c\u4ee5\u53ca\\textit{\u540e\u9a8c\u8bc1}\uff0c\u5728\u9a8c\u8bc1\u9636\u6bb5\u751f\u6210\u66f4\u591a\u8349\u7a3f\u6807\u8bb0\u3002PEARL\u901a\u8fc7\u5e94\u7528\u8fd9\u4e24\u79cd\u7b56\u7565\u5e76\u884c\u5316\u8349\u7a3f\u9636\u6bb5\u548c\u9a8c\u8bc1\u9636\u6bb5\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u81ea\u9002\u5e94\u8349\u7a3f\u957f\u5ea6\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3\u4e86\u76f8\u4e92\u7b49\u5f85\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86PEARL\u7684\u5e73\u5747\u63a5\u53d7\u6807\u8bb0\u6570\u9ad8\u4e8e\u73b0\u6709\u7684\\textit{\u8349\u7a3f-\u9a8c\u8bc1}\u5de5\u4f5c\u3002\u5728\u5404\u79cd\u6587\u672c\u751f\u6210\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e0e\u81ea\u56de\u5f52\u89e3\u7801\u548c\u666e\u901a\u63a8\u6d4b\u6027\u89e3\u7801\u76f8\u6bd4\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe\\textbf{3.79$\\times$}\u548c\\textbf{1.52$\\times$}\u7684\u52a0\u901f\u6027\u80fd\u3002",
        "tldr_en": "PEARL introduces adaptive draft length and parallel verification to enhance speculative decoding, significantly reducing mutual waiting and achieving superior speedup in LLM inference.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPEARL\u7684\u5e76\u884c\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u9a8c\u8bc1\u548c\u540e\u9a8c\u8bc1\u7b56\u7565\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4e2d\u7684\u76f8\u4e92\u7b49\u5f85\u95ee\u9898\uff0c\u5e76\u5728\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.79\u500d\u548c1.52\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002"
    },
    {
        "title": "A Decoding Acceleration Framework for Industrial Deployable LLM-based Recommender Systems",
        "summary": "Recently, increasing attention has been paid to LLM-based recommender systems, but their deployment is still under exploration in the industry. Most deployments utilize LLMs as feature enhancers, generating augmentation knowledge in the offline stage. However, in recommendation scenarios, involving numerous users and items, even offline generation with LLMs consumes considerable time and resources. This generation inefficiency stems from the autoregressive nature of LLMs, and a promising direction for acceleration is speculative decoding, a Draft-then-Verify paradigm that increases the number of generated tokens per decoding step. In this paper, we first identify that recommendation knowledge generation is suitable for retrieval-based speculative decoding. Then, we discern two characteristics: (1) extensive items and users in RSs bring retrieval inefficiency, and (2) RSs exhibit high diversity tolerance for text generated by LLMs. Based on the above insights, we propose a Decoding Acceleration Framework for LLM-based Recommendation (dubbed DARE), with Customized Retrieval Pool to improve retrieval efficiency and Relaxed Verification to increase the acceptance rate of draft tokens, respectively. Extensive experiments demonstrate that DARE achieves a 3-5x speedup and is compatible with various frameworks and backbone LLMs. DARE has also been deployed to online advertising scenarios within a large-scale commercial environment, achieving a 3.45x speedup while maintaining the downstream performance.",
        "authors": "Yunjia Xi, Hangyu Wang, Bo Chen, Jianghao Lin, Menghui Zhu, Weiwen Liu, Ruiming Tang, Weinan Zhang, Yong Yu",
        "published": "2024-08-11",
        "link": "http://arxiv.org/abs/2408.05676v1",
        "chinese_summary": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u8350\u7cfb\u7edf\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u4f46\u5176\u5728\u5de5\u4e1a\u754c\u7684\u90e8\u7f72\u4ecd\u5904\u4e8e\u63a2\u7d22\u9636\u6bb5\u3002\u5927\u591a\u6570\u90e8\u7f72\u65b9\u6848\u5229\u7528LLM\u4f5c\u4e3a\u7279\u5f81\u589e\u5f3a\u5668\uff0c\u5728\u79bb\u7ebf\u9636\u6bb5\u751f\u6210\u589e\u5e7f\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u5728\u63a8\u8350\u573a\u666f\u4e2d\uff0c\u6d89\u53ca\u5927\u91cf\u7528\u6237\u548c\u7269\u54c1\uff0c\u5373\u4f7f\u79bb\u7ebf\u751f\u6210\u4e5f\u6d88\u8017\u5927\u91cf\u65f6\u95f4\u548c\u8d44\u6e90\u3002\u8fd9\u79cd\u751f\u6210\u6548\u7387\u4f4e\u4e0b\u7684\u6839\u6e90\u5728\u4e8eLLM\u7684\u81ea\u56de\u5f52\u7279\u6027\uff0c\u800c\u52a0\u901f\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u662f\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u8fd9\u662f\u4e00\u79cd\u201c\u5148\u8349\u7a3f\u540e\u9a8c\u8bc1\u201d\u7684\u8303\u5f0f\uff0c\u53ef\u4ee5\u589e\u52a0\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u751f\u6210\u7684token\u6570\u91cf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u786e\u5b9a\u63a8\u8350\u77e5\u8bc6\u751f\u6210\u9002\u5408\u57fa\u4e8e\u68c0\u7d22\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u4e24\u4e2a\u7279\u70b9\uff1a\uff081\uff09\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u7684\u7528\u6237\u548c\u7269\u54c1\u5e26\u6765\u4e86\u68c0\u7d22\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\uff082\uff09\u63a8\u8350\u7cfb\u7edf\u5bf9LLM\u751f\u6210\u7684\u6587\u672c\u5177\u6709\u8f83\u9ad8\u7684\u591a\u6837\u6027\u5bb9\u5fcd\u5ea6\u3002\u57fa\u4e8e\u4ee5\u4e0a\u6d1e\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u63a8\u8350\u89e3\u7801\u52a0\u901f\u6846\u67b6\uff08\u7b80\u79f0DARE\uff09\uff0c\u5206\u522b\u901a\u8fc7\u5b9a\u5236\u68c0\u7d22\u6c60\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u548c\u5bbd\u677e\u9a8c\u8bc1\u63d0\u9ad8\u8349\u7a3ftoken\u7684\u63a5\u53d7\u7387\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDARE\u5b9e\u73b0\u4e863-5\u500d\u7684\u52a0\u901f\uff0c\u5e76\u4e14\u517c\u5bb9\u5404\u79cd\u6846\u67b6\u548c\u9aa8\u5e72LLM\u3002DARE\u4e5f\u5df2\u90e8\u7f72\u5230\u5927\u89c4\u6a21\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u5e7f\u544a\u573a\u666f\uff0c\u5b9e\u73b0\u4e863.45\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0b\u6e38\u6027\u80fd\u3002",
        "tldr_en": "This paper introduces DARE, a decoding acceleration framework for LLM-based recommendation systems, which enhances retrieval efficiency and draft token acceptance rate, achieving a 3-5x speedup and successful deployment in online advertising.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u89e3\u7801\u52a0\u901f\u6846\u67b6DARE\uff0c\u901a\u8fc7\u5b9a\u5236\u68c0\u7d22\u6c60\u548c\u5bbd\u677e\u9a8c\u8bc1\u673a\u5236\uff0c\u5b9e\u73b0\u4e863-5\u500d\u7684\u52a0\u901f\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u5546\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u4fdd\u6301\u4e86\u4e0b\u6e38\u6027\u80fd\u3002"
    },
    {
        "title": "Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion",
        "summary": "Speculative decoding has emerged as a widely adopted method to accelerate large language model inference without sacrificing the quality of the model outputs. While this technique has facilitated notable speed improvements by enabling parallel sequence verification, its efficiency remains inherently limited by the reliance on incremental token generation in existing draft models. To overcome this limitation, this paper proposes an adaptation of speculative decoding which uses discrete diffusion models to generate draft sequences. This allows parallelization of both the drafting and verification steps, providing significant speed-ups to the inference process. Our proposed approach, Speculative Diffusion Decoding (SpecDiff), is validated on standard language generation benchmarks and empirically demonstrated to provide a up to 8.7x speed-up over standard generation processes and up to 2.5x speed-up over existing speculative decoding approaches.",
        "authors": "Jacob K Christopher, Brian R Bartoldson, Bhavya Kailkhura, Ferdinando Fioretto",
        "published": "2024-08-10",
        "link": "http://arxiv.org/abs/2408.05636v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u4f5c\u4e3a\u4e00\u79cd\u5e7f\u6cdb\u91c7\u7528\u7684\u65b9\u6cd5\uff0c\u5df2\u7ecf\u51fa\u73b0\uff0c\u5b83\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u3002\u5c3d\u7ba1\u8fd9\u9879\u6280\u672f\u901a\u8fc7\u5b9e\u73b0\u5e76\u884c\u5e8f\u5217\u9a8c\u8bc1\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\uff0c\u4f46\u5176\u6548\u7387\u4ecd\u7136\u53d7\u5230\u73b0\u6709\u8349\u7a3f\u6a21\u578b\u4e2d\u4f9d\u8d56\u589e\u91cf\u4ee4\u724c\u751f\u6210\u7684\u56fa\u6709\u9650\u5236\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u79bb\u6563\u6269\u6563\u6a21\u578b\u751f\u6210\u8349\u7a3f\u5e8f\u5217\u3002\u8fd9\u4f7f\u5f97\u8349\u7a3f\u548c\u9a8c\u8bc1\u6b65\u9aa4\u90fd\u80fd\u5e76\u884c\u5316\uff0c\u4ece\u800c\u663e\u8457\u52a0\u5feb\u4e86\u63a8\u7406\u8fc7\u7a0b\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5373\u63a8\u6d4b\u6027\u6269\u6563\u89e3\u7801\uff08SpecDiff\uff09\uff0c\u5728\u6807\u51c6\u8bed\u8a00\u751f\u6210\u57fa\u51c6\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5b9e\u8bc1\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u751f\u6210\u8fc7\u7a0b\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u5347\u4e86\u9ad8\u8fbe8.7\u500d\uff0c\u4e0e\u73b0\u6709\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u65b9\u6cd5\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u5347\u4e86\u9ad8\u8fbe2.5\u500d\u3002",
        "tldr_en": "This paper introduces Speculative Diffusion Decoding (SpecDiff), a novel adaptation of speculative decoding using discrete diffusion models to parallelize both drafting and verification steps, achieving up to 8.7x speed-up in language generation compared to standard methods and up to 2.5x over existing speculative decoding techniques.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5SpecDiff\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u8349\u7a3f\u751f\u6210\u548c\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901f\u5ea6\uff0c\u5728\u6807\u51c6\u8bed\u8a00\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe8.7\u500d\u7684\u6807\u51c6\u751f\u6210\u52a0\u901f\u548c2.5\u500d\u7684\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u3002"
    },
    {
        "title": "CREST: Effectively Compacting a Datastore For Retrieval-Based Speculative Decoding",
        "summary": "We present CREST (Compact Retrieval-Based Speculative Decoding), a redesign of REST that allows it to be effectively \"compacted\". REST is a drafting technique for speculative decoding based on retrieving exact n-gram matches of the most recent n tokens generated by the target LLM from a datastore. The key idea of CREST is to only store a subset of the smallest and most common n-grams in the datastore with the hope of achieving comparable performance with less storage space. We found that storing a subset of n-grams both reduces storage space and improves performance. CREST matches REST's accepted token length with 10.6-13.5x less storage space and achieves a 16.5-17.1% higher acceptance length than REST using the same storage space on the HumanEval and MT Bench benchmarks.",
        "authors": "Sophia Ho, Jinsol Park, Patrick Wang",
        "published": "2024-08-08",
        "link": "http://arxiv.org/abs/2408.04678v1",
        "chinese_summary": "\u6211\u4eec\u63d0\u51fa\u4e86CREST\uff08\u57fa\u4e8e\u68c0\u7d22\u7684\u7d27\u51d1\u63a8\u6d4b\u89e3\u7801\uff09\uff0c\u8fd9\u662f\u5bf9REST\u7684\u91cd\u65b0\u8bbe\u8ba1\uff0c\u4f7f\u5176\u80fd\u591f\u88ab\u6709\u6548\u5730\u201c\u538b\u7f29\u201d\u3002REST\u662f\u4e00\u79cd\u57fa\u4e8e\u4ece\u6570\u636e\u5b58\u50a8\u4e2d\u68c0\u7d22\u76ee\u6807\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u6700\u8fd1n\u4e2a\u6807\u8bb0\u7684\u7cbe\u786en-gram\u5339\u914d\u7684\u63a8\u6d4b\u89e3\u7801\u8d77\u8349\u6280\u672f\u3002CREST\u7684\u5173\u952e\u601d\u60f3\u662f\u4ec5\u5728\u6570\u636e\u5b58\u50a8\u4e2d\u5b58\u50a8\u6700\u5c0f\u4e14\u6700\u5e38\u89c1\u7684n-gram\u5b50\u96c6\uff0c\u4ee5\u671f\u5728\u51cf\u5c11\u5b58\u50a8\u7a7a\u95f4\u7684\u540c\u65f6\u5b9e\u73b0\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5b58\u50a8n-gram\u7684\u5b50\u96c6\u4e0d\u4ec5\u51cf\u5c11\u4e86\u5b58\u50a8\u7a7a\u95f4\uff0c\u8fd8\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u5728HumanEval\u548cMT Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCREST\u4ee510.6\u81f313.5\u500d\u7684\u5b58\u50a8\u7a7a\u95f4\u5b9e\u73b0\u4e86\u4e0eREST\u76f8\u540c\u7684\u63a5\u53d7\u6807\u8bb0\u957f\u5ea6\uff0c\u5e76\u4e14\u5728\u4f7f\u7528\u76f8\u540c\u5b58\u50a8\u7a7a\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u6bd4REST\u7684\u63a5\u53d7\u957f\u5ea6\u63d0\u9ad8\u4e8616.5%\u81f317.1%\u3002",
        "tldr_en": "CREST is a compacted version of REST that reduces storage by 10.6-13.5x and improves performance by 16.5-17.1% on benchmarks.",
        "tldr_zh": "CREST\u901a\u8fc7\u5b58\u50a8\u6700\u5e38\u89c1\u7684\u5c0f\u578bn-gram\u5b50\u96c6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5b58\u50a8\u7a7a\u95f4\u5e76\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u76f8\u6bd4REST\u5728\u76f8\u540c\u5b58\u50a8\u7a7a\u95f4\u4e0b\u63d0\u9ad8\u4e8616.5-17.1%\u7684\u63a5\u53d7\u957f\u5ea6\u3002"
    },
    {
        "title": "Clover-2: Accurate Inference for Regressive Lightweight Speculative Decoding",
        "summary": "Large Language Models (LLMs) frequently suffer from inefficiencies, largely attributable to the discord between the requirements of auto-regressive decoding and the architecture of contemporary GPUs. Recently, regressive lightweight speculative decoding has garnered attention for its notable efficiency improvements in text generation tasks. This approach utilizes a lightweight regressive draft model, like a Recurrent Neural Network (RNN) or a single transformer decoder layer, leveraging sequential information to iteratively predict potential tokens. Specifically, RNN draft models are computationally economical but tend to deliver lower accuracy, while attention decoder layer models exhibit the opposite traits. This paper presents Clover-2, an advanced iteration of Clover, an RNN-based draft model designed to achieve comparable accuracy to that of attention decoder layer models while maintaining minimal computational overhead. Clover-2 enhances the model architecture and incorporates knowledge distillation to increase Clover's accuracy and improve overall efficiency. We conducted experiments using the open-source Vicuna 7B and LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses existing methods across various model architectures, showcasing its efficacy and robustness.",
        "authors": "Bin Xiao, Lujun Gui, Lei Su, Weipeng Chen",
        "published": "2024-08-01",
        "link": "http://arxiv.org/abs/2408.00264v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6548\u7387\u65b9\u9762\u7ecf\u5e38\u9047\u5230\u95ee\u9898\uff0c\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\u7684\u9700\u6c42\u4e0e\u5f53\u4ee3GPU\u67b6\u6784\u4e4b\u95f4\u7684\u4e0d\u534f\u8c03\u3002\u6700\u8fd1\uff0c\u56de\u5f52\u8f7b\u91cf\u7ea7\u63a8\u6d4b\u89e3\u7801\u56e0\u5176\u663e\u8457\u63d0\u5347\u6587\u672c\u751f\u6210\u4efb\u52a1\u6548\u7387\u800c\u53d7\u5230\u5173\u6ce8\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u8f7b\u91cf\u7ea7\u56de\u5f52\u8349\u7a3f\u6a21\u578b\uff0c\u5982\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u6216\u5355\u4e2a\u53d8\u6362\u5668\u89e3\u7801\u5c42\uff0c\u5229\u7528\u5e8f\u5217\u4fe1\u606f\u8fed\u4ee3\u9884\u6d4b\u6f5c\u5728\u7684\u6807\u8bb0\u3002\u5177\u4f53\u800c\u8a00\uff0cRNN\u8349\u7a3f\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u4f46\u51c6\u786e\u6027\u8f83\u4f4e\uff0c\u800c\u6ce8\u610f\u529b\u89e3\u7801\u5c42\u6a21\u578b\u5219\u8868\u73b0\u51fa\u76f8\u53cd\u7684\u7279\u5f81\u3002\u672c\u6587\u4ecb\u7ecd\u4e86Clover-2\uff0c\u5b83\u662fClover\u7684\u8fdb\u9636\u7248\u672c\uff0cClover\u662f\u4e00\u79cd\u57fa\u4e8eRNN\u7684\u8349\u7a3f\u6a21\u578b\uff0c\u65e8\u5728\u5728\u4fdd\u6301\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e0e\u6ce8\u610f\u529b\u89e3\u7801\u5c42\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002Clover-2\u901a\u8fc7\u589e\u5f3a\u6a21\u578b\u67b6\u6784\u5e76\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u9ad8\u4e86Clover\u7684\u51c6\u786e\u6027\u5e76\u63d0\u5347\u4e86\u6574\u4f53\u6548\u7387\u3002\u6211\u4eec\u4f7f\u7528\u5f00\u6e90\u7684Vicuna 7B\u548cLLaMA3-Instruct 8B\u6a21\u578b\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cClover-2\u5728\u5404\u79cd\u6a21\u578b\u67b6\u6784\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002",
        "tldr_en": "Clover-2, an advanced RNN-based draft model, significantly improves text generation efficiency and accuracy by leveraging knowledge distillation and enhanced architecture, outperforming existing methods across diverse model architectures.",
        "tldr_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u56e0\u81ea\u56de\u5f52\u89e3\u7801\u9700\u6c42\u4e0e\u73b0\u4ee3GPU\u67b6\u6784\u4e0d\u5339\u914d\u800c\u6548\u7387\u4f4e\u4e0b\uff0c\u8fd1\u671f\u56de\u5f52\u8f7b\u91cf\u63a8\u6d4b\u89e3\u7801\u56e0\u5176\u663e\u8457\u63d0\u5347\u6587\u672c\u751f\u6210\u4efb\u52a1\u6548\u7387\u800c\u53d7\u5230\u5173\u6ce8\u3002Clover-2\u4f5c\u4e3a\u57fa\u4e8eRNN\u7684\u8f7b\u91cf\u8349\u7a3f\u6a21\u578b\u7684\u8fed\u4ee3\uff0c\u901a\u8fc7\u589e\u5f3a\u67b6\u6784\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6ce8\u610f\u529b\u89e3\u7801\u5c42\u6a21\u578b\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"
    },
    {
        "title": "Graph-Structured Speculative Decoding",
        "summary": "Speculative decoding has emerged as a promising technique to accelerate the inference of Large Language Models (LLMs) by employing a small language model to draft a hypothesis sequence, which is then validated by the LLM. The effectiveness of this approach heavily relies on the balance between performance and efficiency of the draft model. In our research, we focus on enhancing the proportion of draft tokens that are accepted to the final output by generating multiple hypotheses instead of just one. This allows the LLM more options to choose from and select the longest sequence that meets its standards. Our analysis reveals that hypotheses produced by the draft model share many common token sequences, suggesting a potential for optimizing computation. Leveraging this observation, we introduce an innovative approach utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This structure enables us to efficiently predict and merge recurring token sequences, vastly reducing the computational demands of the draft model. We term this approach Graph-structured Speculative Decoding (GSD). We apply GSD across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and observe a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly surpassing standard speculative decoding.",
        "authors": "Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan",
        "published": "2024-07-23",
        "link": "http://arxiv.org/abs/2407.16207v1",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u6280\u672f\uff0c\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5047\u8bbe\u5e8f\u5217\uff0c\u7136\u540e\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4ece\u800c\u52a0\u901f\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u8349\u7a3f\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u5728\u6211\u4eec\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u5047\u8bbe\u800c\u4e0d\u662f\u5355\u4e00\u5047\u8bbe\u6765\u63d0\u9ad8\u8349\u7a3f\u6807\u8bb0\u88ab\u6700\u7ec8\u8f93\u51fa\u63a5\u53d7\u7684\u6bd4\u4f8b\u3002\u8fd9\u4f7f\u5f97LLM\u6709\u66f4\u591a\u7684\u9009\u62e9\uff0c\u5e76\u53ef\u4ee5\u9009\u62e9\u7b26\u5408\u5176\u6807\u51c6\u7684\u6700\u957f\u5e8f\u5217\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u8349\u7a3f\u6a21\u578b\u751f\u6210\u7684\u5047\u8bbe\u5171\u4eab\u8bb8\u591a\u5e38\u89c1\u7684\u6807\u8bb0\u5e8f\u5217\uff0c\u8fd9\u6697\u793a\u4e86\u4f18\u5316\u8ba1\u7b97\u7684\u6f5c\u529b\u3002\u5229\u7528\u8fd9\u4e00\u89c2\u5bdf\u7ed3\u679c\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u6765\u7ba1\u7406\u8349\u7a3f\u5047\u8bbe\u3002\u8fd9\u79cd\u7ed3\u6784\u4f7f\u6211\u4eec\u80fd\u591f\u9ad8\u6548\u5730\u9884\u6d4b\u548c\u5408\u5e76\u91cd\u590d\u7684\u6807\u8bb0\u5e8f\u5217\uff0c\u6781\u5927\u5730\u51cf\u5c11\u4e86\u8349\u7a3f\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\u3002\u6211\u4eec\u5c06\u8fd9\u79cd\u65b9\u6cd5\u79f0\u4e3a\u56fe\u7ed3\u6784\u63a8\u6d4b\u6027\u89e3\u7801\uff08GSD\uff09\u3002\u6211\u4eec\u5728\u4e00\u7cfb\u5217LLM\u4e0a\u5e94\u7528\u4e86GSD\uff0c\u5305\u62ec\u4e00\u4e2a700\u4ebf\u53c2\u6570\u7684LLaMA-2\u6a21\u578b\uff0c\u5e76\u89c2\u5bdf\u5230\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\uff0c\u4ece1.73\u500d\u52301.96\u500d\uff0c\u660e\u663e\u8d85\u8fc7\u4e86\u6807\u51c6\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u3002",
        "tldr_en": "Graph-structured Speculative Decoding (GSD) accelerates Large Language Models by efficiently managing and merging recurring token sequences using a directed acyclic graph, achieving up to 1.96$\\times$ speedup.",
        "tldr_zh": "\u56fe\u7ed3\u6784\u63a8\u6d4b\u89e3\u7801\uff08GSD\uff09\u901a\u8fc7\u5229\u7528\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u9ad8\u6548\u9884\u6d4b\u548c\u5408\u5e76\u91cd\u590d\u7684\u4ee4\u724c\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u901f\u5ea6\uff0c\u76f8\u8f83\u4e8e\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u5b9e\u73b0\u4e861.73\u500d\u81f31.96\u500d\u7684\u52a0\u901f\u3002"
    },
    {
        "title": "Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference",
        "summary": "Transformer-based Large language models (LLMs) have demonstrated their power in various tasks, but their inference incurs significant time and energy costs. To accelerate LLM inference, speculative decoding uses a smaller model to propose one sequence of tokens, which are subsequently validated in batch by the target large model. Compared with autoregressive decoding, speculative decoding generates the same number of tokens with fewer runs of the large model, hence accelerating the overall inference by $1$-$2\\times$. However, greedy decoding is not the optimal decoding algorithm in terms of output perplexity, which is a direct measurement of the effectiveness of a decoding algorithm. An algorithm that has better output perplexity and even better efficiency than speculative decoding can be more useful in practice. To achieve this seemingly contradictory goal, we first introduce multi-token joint greedy decoding (MJGD), which greedily generates multiple tokens at each step based on their joint perplexity. We show that it leads to better perplexity for the whole output. But the computation cost of MJGD is infeasible in practice. So we further propose multi-token joint speculative decoding (MJSD), which approximates and accelerates the MJGD from two aspects: it approximates the joint distribution of the large model with that of a small model, and uses a verification step to guarantee the accuracy of approximation; then it uses beam decoding to accelerate the sequence generation from the joint distribution. Compared with vanilla speculative decoding, MJSD has two advantages: (1) it is an approximation of MJGD, thus achieving better output perplexity; (2) verification with joint likelihood allows it to accept the longest prefix sub-sequence of the draft tokens with valid perplexity, leading to better efficiency...",
        "authors": "Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun",
        "published": "2024-07-12",
        "link": "http://arxiv.org/abs/2407.09722v1",
        "chinese_summary": "\u57fa\u4e8eTransformer\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u5374\u4f34\u968f\u7740\u663e\u8457\u7684\u65f6\u95f4\u548c\u80fd\u6e90\u6d88\u8017\u3002\u4e3a\u4e86\u52a0\u901fLLM\u7684\u63a8\u7406\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\uff08speculative decoding\uff09\u91c7\u7528\u4e86\u4e00\u4e2a\u8f83\u5c0f\u7684\u6a21\u578b\u6765\u63d0\u51fa\u4e00\u4e32\u5019\u9009\u8bcd\u5143\uff0c\u7136\u540e\u7531\u76ee\u6807\u5927\u6a21\u578b\u6279\u91cf\u9a8c\u8bc1\u8fd9\u4e9b\u8bcd\u5143\u3002\u4e0e\u81ea\u56de\u5f52\u89e3\u7801\u76f8\u6bd4\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u901a\u8fc7\u51cf\u5c11\u5927\u6a21\u578b\u7684\u8fd0\u884c\u6b21\u6570\uff0c\u4ee5\u76f8\u540c\u7684\u8bcd\u5143\u6570\u91cf\u5b9e\u73b0\u4e861-2\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002\u7136\u800c\uff0c\u8d2a\u5a6a\u89e3\u7801\u5728\u8f93\u51fa\u56f0\u60d1\u5ea6\uff08perplexity\uff09\u65b9\u9762\u5e76\u975e\u6700\u4f18\u89e3\u7801\u7b97\u6cd5\uff0c\u800c\u56f0\u60d1\u5ea6\u662f\u8861\u91cf\u89e3\u7801\u7b97\u6cd5\u6548\u679c\u7684\u76f4\u63a5\u6307\u6807\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4e00\u79cd\u5728\u8f93\u51fa\u56f0\u60d1\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\u4e14\u6548\u7387\u751a\u81f3\u8d85\u8fc7\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u7b97\u6cd5\u5c06\u66f4\u5177\u5b9e\u7528\u4ef7\u503c\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u770b\u4f3c\u77db\u76fe\u7684\u76ee\u6807\uff0c\u6211\u4eec\u9996\u5148\u5f15\u5165\u4e86\u591a\u8bcd\u5143\u8054\u5408\u8d2a\u5a6a\u89e3\u7801\uff08MJGD\uff09\uff0c\u8be5\u65b9\u6cd5\u5728\u6bcf\u4e00\u6b65\u8d2a\u5a6a\u5730\u751f\u6210\u591a\u4e2a\u8bcd\u5143\uff0c\u57fa\u4e8e\u5b83\u4eec\u7684\u8054\u5408\u56f0\u60d1\u5ea6\u8fdb\u884c\u9009\u62e9\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u6539\u5584\u6574\u4f53\u8f93\u51fa\u7684\u56f0\u60d1\u5ea6\u3002\u4f46MJGD\u7684\u8ba1\u7b97\u6210\u672c\u5728\u5b9e\u8df5\u4e2d\u662f\u4e0d\u53ef\u884c\u7684\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u591a\u8bcd\u5143\u8054\u5408\u63a8\u6d4b\u6027\u89e3\u7801\uff08MJSD\uff09\uff0c\u8be5\u65b9\u6cd5\u4ece\u4e24\u4e2a\u65b9\u9762\u8fd1\u4f3c\u5e76\u52a0\u901f\u4e86MJGD\uff1a\u5b83\u901a\u8fc7\u4e00\u4e2a\u5c0f\u6a21\u578b\u8fd1\u4f3c\u5927\u6a21\u578b\u7684\u8054\u5408\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u9a8c\u8bc1\u6b65\u9aa4\u6765\u4fdd\u8bc1\u8fd1\u4f3c\u7684\u51c6\u786e\u6027\uff1b\u968f\u540e\uff0c\u5b83\u5229\u7528\u675f\u89e3\u7801\uff08beam decoding\uff09\u6765\u52a0\u901f\u4ece\u8054\u5408\u5206\u5e03\u4e2d\u751f\u6210\u5e8f\u5217\u3002\u4e0e\u4f20\u7edf\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u76f8\u6bd4\uff0cMJSD\u5177\u6709\u4e24\u5927\u4f18\u52bf\uff1a\uff081\uff09\u4f5c\u4e3aMJGD\u7684\u8fd1\u4f3c\uff0c\u5b83\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u8f93\u51fa\u56f0\u60d1\u5ea6\uff1b\uff082\uff09\u901a\u8fc7\u8054\u5408\u4f3c\u7136\u6027\u9a8c\u8bc1\uff0c\u5b83\u80fd\u591f\u63a5\u53d7\u5177\u6709\u6709\u6548\u56f0\u60d1\u5ea6\u7684\u8349\u7a3f\u8bcd\u5143\u4e2d\u6700\u957f\u7684\u524d\u7f00\u5b50\u5e8f\u5217\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387...",
        "tldr_en": "Multi-token joint speculative decoding (MJSD) accelerates large language model inference with better output perplexity by approximating joint distribution and using verification with joint likelihood.",
        "tldr_zh": "\u591a\u4ee4\u724c\u8054\u5408\u63a8\u6d4b\u89e3\u7801\uff08MJSD\uff09\u901a\u8fc7\u8fd1\u4f3c\u5927\u6a21\u578b\u7684\u8054\u5408\u5206\u5e03\u5e76\u7ed3\u5408\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u63d0\u9ad8\u4e86\u8f93\u51fa\u56f0\u60d1\u5ea6\u5e76\u52a0\u901f\u4e86\u63a8\u7406\u8fc7\u7a0b\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u63a8\u6d4b\u89e3\u7801\u66f4\u5177\u4f18\u52bf\u3002"
    },
    {
        "title": "Accelerating the inference of string generation-based chemical reaction models for industrial applications",
        "summary": "Template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest for industrial applications in computer-aided synthesis planning systems due to their state-of-the-art accuracy. However, they suffer from slow inference speed. We present a method to accelerate inference in autoregressive SMILES generators through speculative decoding by copying query string subsequences into target strings in the right places. We apply our method to the molecular transformer implemented in Pytorch Lightning and achieve over 3X faster inference in reaction prediction and single-step retrosynthesis, with no loss in accuracy.",
        "authors": "Mikhail Andronov, Natalia Andronova, Michael Wand, J\u00fcrgen Schmidhuber, Djork-Arn\u00e9 Clevert",
        "published": "2024-07-12",
        "link": "http://arxiv.org/abs/2407.09685v2",
        "chinese_summary": "\u65e0\u6a21\u677fSMILES\u5230SMILES\u7684\u7ffb\u8bd1\u6a21\u578b\u5728\u53cd\u5e94\u9884\u6d4b\u548c\u5355\u6b65\u9006\u5408\u6210\u4e2d\u56e0\u5176\u5353\u8d8a\u7684\u51c6\u786e\u6027\u800c\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u7cfb\u7edf\u7684\u5de5\u4e1a\u5e94\u7528\u4e2d\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63a8\u6d4b\u6027\u89e3\u7801\u6765\u52a0\u901f\u81ea\u56de\u5f52SMILES\u751f\u6210\u5668\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5373\u5728\u9002\u5f53\u7684\u4f4d\u7f6e\u5c06\u67e5\u8be2\u5b57\u7b26\u4e32\u7684\u5b50\u5e8f\u5217\u590d\u5236\u5230\u76ee\u6807\u5b57\u7b26\u4e32\u4e2d\u3002\u6211\u4eec\u5c06\u6b64\u65b9\u6cd5\u5e94\u7528\u4e8e\u57fa\u4e8ePytorch Lightning\u5b9e\u73b0\u7684\u5206\u5b50\u53d8\u6362\u5668\uff0c\u5e76\u5728\u53cd\u5e94\u9884\u6d4b\u548c\u5355\u6b65\u9006\u5408\u6210\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc73\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u672a\u635f\u5931\u4efb\u4f55\u51c6\u786e\u6027\u3002",
        "tldr_en": "We accelerate template-free SMILES translation for reaction prediction and retrosynthesis by 3X via speculative decoding, maintaining accuracy.",
        "tldr_zh": "\u901a\u8fc7\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u81ea\u56de\u5f52SMILES\u751f\u6210\u5668\u63a8\u7406\uff0c\u5b9e\u73b0\u53cd\u5e94\u9884\u6d4b\u548c\u5355\u6b65\u9006\u5408\u6210\u4e2d3\u500d\u4ee5\u4e0a\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002"
    },
    {
        "title": "S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models",
        "summary": "Deployment of autoregressive large language models (LLMs) is costly, and as these models increase in size, the associated costs will become even more considerable. Consequently, different methods have been proposed to accelerate the token generation process and reduce costs. Speculative decoding (SD) is among the most promising approaches to speed up the LLM decoding process by verifying multiple tokens in parallel and using an auxiliary smaller draft model to generate the possible tokens. In SD, usually, one draft model is used to serve a specific target model; however, in practice, LLMs are diverse, and we might need to deal with many target models or more than one target model simultaneously. In this scenario, it is not clear which draft model should be used for which target model, and searching among different draft models or training customized draft models can further increase deployment costs. In this paper, we first introduce a novel multi-target scenario for the deployment of draft models for faster inference. Then, we present a novel, more efficient sorted speculative decoding mechanism that outperforms regular baselines in multi-target settings. We evaluated our method on Spec-Bench in different settings, including base models such as Vicuna 7B, 13B, and LLama Chat 70B. Our results suggest that our draft models perform better than baselines for multiple target models at the same time.",
        "authors": "Parsa Kavehzadeh, Mohammadreza Pourreza, Mojtaba Valipour, Tinashu Zhu, Haoli Bai, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh",
        "published": "2024-07-02",
        "link": "http://arxiv.org/abs/2407.01955v1",
        "chinese_summary": "\u81ea\u56de\u5f52\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u90e8\u7f72\u6210\u672c\u9ad8\u6602\uff0c\u5e76\u4e14\u968f\u7740\u8fd9\u4e9b\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\uff0c\u76f8\u5173\u6210\u672c\u5c06\u53d8\u5f97\u66f4\u52a0\u53ef\u89c2\u3002\u56e0\u6b64\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u5404\u79cd\u65b9\u6cd5\u6765\u52a0\u901f\u4ee4\u724c\u751f\u6210\u8fc7\u7a0b\u5e76\u964d\u4f4e\u6210\u672c\u3002\u63a8\u6d4b\u6027\u89e3\u7801\uff08SD\uff09\u662f\u6700\u6709\u524d\u666f\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u5b83\u901a\u8fc7\u5e76\u884c\u9a8c\u8bc1\u591a\u4e2a\u4ee4\u724c\u5e76\u4f7f\u7528\u8f85\u52a9\u7684\u8f83\u5c0f\u8349\u7a3f\u6a21\u578b\u6765\u751f\u6210\u53ef\u80fd\u7684\u4ee4\u724c\uff0c\u4ece\u800c\u52a0\u901fLLM\u7684\u89e3\u7801\u8fc7\u7a0b\u3002\u5728SD\u4e2d\uff0c\u901a\u5e38\u4f7f\u7528\u4e00\u4e2a\u8349\u7a3f\u6a21\u578b\u6765\u670d\u52a1\u4e8e\u7279\u5b9a\u7684\u76ee\u6807\u6a21\u578b\uff1b\u7136\u800c\uff0c\u5728\u5b9e\u8df5\u4e2d\uff0cLLM\u662f\u591a\u6837\u5316\u7684\uff0c\u6211\u4eec\u53ef\u80fd\u9700\u8981\u5904\u7406\u591a\u4e2a\u76ee\u6807\u6a21\u578b\u6216\u540c\u65f6\u5904\u7406\u591a\u4e2a\u76ee\u6807\u6a21\u578b\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e0d\u6e05\u695a\u5e94\u8be5\u4e3a\u54ea\u4e2a\u76ee\u6807\u6a21\u578b\u4f7f\u7528\u54ea\u4e2a\u8349\u7a3f\u6a21\u578b\uff0c\u800c\u5728\u4e0d\u540c\u7684\u8349\u7a3f\u6a21\u578b\u4e2d\u641c\u7d22\u6216\u8bad\u7ec3\u5b9a\u5236\u7684\u8349\u7a3f\u6a21\u578b\u53ef\u80fd\u4f1a\u8fdb\u4e00\u6b65\u589e\u52a0\u90e8\u7f72\u6210\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u76ee\u6807\u573a\u666f\uff0c\u7528\u4e8e\u90e8\u7f72\u8349\u7a3f\u6a21\u578b\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u66f4\u9ad8\u6548\u7684\u6392\u5e8f\u63a8\u6d4b\u6027\u89e3\u7801\u673a\u5236\uff0c\u8be5\u673a\u5236\u5728\u591a\u76ee\u6807\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5e38\u89c4\u57fa\u7ebf\u3002\u6211\u4eec\u5728\u4e0d\u540c\u7684\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5305\u62ecVicuna 7B\u300113B\u548cLLama Chat 70B\u7b49\u57fa\u7840\u6a21\u578b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u8349\u7a3f\u6a21\u578b\u5728\u540c\u65f6\u5904\u7406\u591a\u4e2a\u76ee\u6807\u6a21\u578b\u65f6\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002",
        "tldr_en": "This paper introduces a novel multi-target draft model deployment strategy and an efficient sorted speculative decoding mechanism that outperforms baselines in accelerating large language model inference across diverse target models.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u573a\u666f\u4e0b\u66f4\u9ad8\u6548\u7684\u6392\u5e8f\u63a8\u6d4b\u89e3\u7801\u673a\u5236\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5728\u591a\u4e2a\u76ee\u6807\u6a21\u578b\u540c\u65f6\u8fd0\u884c\u65f6\uff0c\u5176\u8349\u7a3f\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002"
    },
    {
        "title": "SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding",
        "summary": "Large Language Models (LLMs) demonstrate remarkable emergent abilities across various tasks, yet fall short of complex reasoning and planning tasks. The tree-search-based reasoning methods address this by surpassing the capabilities of chain-of-thought prompting, encouraging exploration of intermediate steps. However, such methods introduce significant inference latency due to the systematic exploration and evaluation of multiple thought paths. This paper introduces SeeD, a novel and efficient inference framework to optimize runtime speed and GPU memory management concurrently. By employing a scheduled speculative execution, SeeD efficiently handles multiple iterations for the thought generation and the state evaluation, leveraging a rounds-scheduled strategy to manage draft model dispatching. Extensive experimental evaluations on three reasoning datasets demonstrate superior speedup performance of SeeD, providing a viable path for batched inference in training-free speculative decoding.",
        "authors": "Zhenglin Wang, Jialong Wu, Yilong Lai, Congzhi Zhang, Deyu Zhou",
        "published": "2024-06-26",
        "link": "http://arxiv.org/abs/2406.18200v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6d8c\u73b0\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u4ecd\u663e\u4e0d\u8db3\u3002\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u63a8\u7406\u65b9\u6cd5\u901a\u8fc7\u8d85\u8d8a\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u80fd\u529b\uff0c\u9f13\u52b1\u63a2\u7d22\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7531\u4e8e\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u548c\u8bc4\u4f30\u591a\u4e2a\u601d\u7ef4\u8def\u5f84\uff0c\u5f15\u5165\u4e86\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u672c\u6587\u4ecb\u7ecd\u4e86SeeD\uff0c\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\uff0c\u65e8\u5728\u540c\u65f6\u4f18\u5316\u8fd0\u884c\u65f6\u901f\u5ea6\u548cGPU\u5185\u5b58\u7ba1\u7406\u3002\u901a\u8fc7\u91c7\u7528\u8ba1\u5212\u6027\u7684\u63a8\u6d4b\u6267\u884c\uff0cSeeD\u80fd\u591f\u9ad8\u6548\u5904\u7406\u601d\u7ef4\u751f\u6210\u548c\u72b6\u6001\u8bc4\u4f30\u7684\u591a\u6b21\u8fed\u4ee3\uff0c\u5229\u7528\u8f6e\u6b21\u8c03\u5ea6\u7b56\u7565\u6765\u7ba1\u7406\u8349\u7a3f\u6a21\u578b\u7684\u8c03\u5ea6\u3002\u5728\u4e09\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cSeeD\u5177\u6709\u4f18\u8d8a\u7684\u52a0\u901f\u6027\u80fd\uff0c\u4e3a\u65e0\u8bad\u7ec3\u7684\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u6279\u91cf\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\u3002",
        "tldr_en": "SeeD optimizes runtime speed and GPU memory for efficient speculative decoding in large language models, outperforming traditional methods in reasoning tasks.",
        "tldr_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8272\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u63a8\u7406\u65b9\u6cd5\u901a\u8fc7\u63a2\u7d22\u4e2d\u95f4\u6b65\u9aa4\u8d85\u8d8a\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u4f46\u5f15\u5165\u4e86\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u672c\u6587\u63d0\u51faSeeD\uff0c\u4e00\u79cd\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u5ea6\u63a8\u6d4b\u6267\u884c\u4f18\u5316\u8fd0\u884c\u901f\u5ea6\u548cGPU\u5185\u5b58\u7ba1\u7406\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u4e09\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u663e\u8457\u52a0\u901f\uff0c\u4e3a\u65e0\u8bad\u7ec3\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u6279\u91cf\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"
    },
    {
        "title": "Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training",
        "summary": "Existing speculative decoding methods typically require additional model structure and training processes to assist the model for draft token generation. This makes the migration of acceleration methods to the new model more costly and more demanding on device memory. To address this problem, we propose the Make Some Noise (MSN) training framework as a replacement for the supervised fine-tuning stage of the large language model. The training method simply introduces some noise at the input for the model to learn the denoising task. It significantly enhances the parallel decoding capability of the model without affecting the original task capability. In addition, we propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further improve the inference speed of MSN models. Experiments in both the general and code domains have shown that MSN can improve inference speed by 2.3-2.7x times without compromising model performance. The MSN model also achieves comparable acceleration ratios to the SOTA model with additional model structure on Spec-Bench.",
        "authors": "Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che",
        "published": "2024-06-25",
        "link": "http://arxiv.org/abs/2406.17404v1",
        "chinese_summary": "\u73b0\u6709\u7684\u6295\u673a\u89e3\u7801\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u6a21\u578b\u7ed3\u6784\u548c\u8bad\u7ec3\u8fc7\u7a0b\u6765\u8f85\u52a9\u6a21\u578b\u8fdb\u884c\u8349\u7a3f\u4ee4\u724c\u751f\u6210\u3002\u8fd9\u4f7f\u5f97\u52a0\u901f\u65b9\u6cd5\u5411\u65b0\u6a21\u578b\u7684\u8fc1\u79fb\u6210\u672c\u66f4\u9ad8\uff0c\u5bf9\u8bbe\u5907\u5185\u5b58\u7684\u8981\u6c42\u4e5f\u66f4\u4e3a\u4e25\u683c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Make Some Noise\uff08MSN\uff09\u8bad\u7ec3\u6846\u67b6\uff0c\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u8be5\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u5728\u8f93\u5165\u4e2d\u5f15\u5165\u4e00\u4e9b\u566a\u58f0\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\u53bb\u566a\u4efb\u52a1\u3002\u5b83\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5e76\u884c\u89e3\u7801\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u539f\u6709\u4efb\u52a1\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u7684\u68c0\u7d22\u589e\u5f3a\u96c5\u53ef\u6bd4\uff08TR-Jacobi\uff09\u89e3\u7801\u7b56\u7565\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8MSN\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u3002\u5728\u901a\u7528\u548c\u4ee3\u7801\u9886\u57df\u7684\u5b9e\u9a8c\u5747\u8868\u660e\uff0cMSN\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u53ef\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u53472.3-2.7\u500d\u3002MSN\u6a21\u578b\u5728Spec-Bench\u4e0a\u7684\u52a0\u901f\u6bd4\u4e5f\u4e0e\u5177\u6709\u989d\u5916\u6a21\u578b\u7ed3\u6784\u7684SOTA\u6a21\u578b\u76f8\u5f53\u3002",
        "tldr_en": "We propose the Make Some Noise (MSN) training framework and TR-Jacobi decoding strategy to enhance parallel decoding and inference speed of large language models without additional model structure, achieving up to 2.7x faster inference with comparable performance to state-of-the-art models.",
        "tldr_zh": "\u63d0\u51faMake Some Noise (MSN)\u8bad\u7ec3\u6846\u67b6\u548c\u57fa\u4e8e\u6811\u7684\u68c0\u7d22\u589e\u5f3aJacobi (TR-Jacobi)\u89e3\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u884c\u89e3\u7801\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.3-2.7\u500d\uff0c\u52a0\u901f\u6bd4\u5ab2\u7f8eSOTA\u6a21\u578b\u3002"
    },
    {
        "title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure",
        "summary": "Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a \"draft and then verify\" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which fail to adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft trees. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree.",
        "authors": "Jikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang",
        "published": "2024-06-25",
        "link": "http://arxiv.org/abs/2406.17276v2",
        "chinese_summary": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5176\u4e00\u6b65\u4e00\u751f\u6210\u7684\u63a8\u7406\u6a21\u5f0f\u9650\u5236\u4e86\u6548\u7387\uff0c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u4e0d\u65ad\u6269\u5927\uff0c\u8fd9\u4e00\u95ee\u9898\u53d8\u5f97\u6108\u53d1\u7d27\u8feb\u3002\u63a8\u6d4b\u6027\u89e3\u7801\u91c7\u7528\u201c\u5148\u8349\u62df\u540e\u9a8c\u8bc1\u201d\u7684\u673a\u5236\uff0c\u5141\u8bb8\u4e00\u6b65\u751f\u6210\u591a\u4e2a\u8bcd\u5143\uff0c\u5b9e\u73b0\u65e0\u635f\u52a0\u901f\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u56fa\u5b9a\u7684\u542f\u53d1\u5f0f\u8349\u62df\u7ed3\u6784\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u60c5\u51b5\u4ee5\u6700\u5927\u5316\u9a8c\u8bc1\u9636\u6bb5\u7684\u63a5\u53d7\u957f\u5ea6\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e00\u56f0\u5883\uff0c\u6211\u4eec\u63d0\u51fa\u4e86OPT-Tree\u7b97\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u81ea\u9002\u5e94\u4e14\u53ef\u6269\u5c55\u7684\u8349\u62df\u6811\u3002\u8be5\u7b97\u6cd5\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\u641c\u7d22\u80fd\u591f\u6700\u5927\u5316\u63a5\u53d7\u957f\u5ea6\u6570\u5b66\u671f\u671b\u7684\u6700\u4f18\u6811\u7ed3\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOPT-Tree\u4f18\u4e8e\u73b0\u6709\u7684\u8349\u62df\u7ed3\u6784\uff0c\u4e0e\u81ea\u56de\u5f52\u89e3\u7801\u76f8\u6bd4\uff0c\u6700\u9ad8\u53ef\u5b9e\u73b03.2\u500d\u7684\u52a0\u901f\u6bd4\u3002\u82e5\u8349\u62df\u6a21\u578b\u8db3\u591f\u5f3a\u5927\u4e14\u8282\u70b9\u9884\u7b97\u5145\u8db3\uff0c\u5355\u6b65\u53ef\u751f\u6210\u8d85\u8fc7\u5341\u4e2a\u8bcd\u5143\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u8be6\u89c1https://github.com/Jikai0Wang/OPT-Tree\u3002",
        "tldr_en": "OPT-Tree algorithm constructs adaptive draft trees for speculative decoding, significantly accelerating autoregressive language models by up to 3.2x and enabling multi-token generation per step.",
        "tldr_zh": "OPT-Tree\u7b97\u6cd5\u901a\u8fc7\u6784\u5efa\u81ea\u9002\u5e94\u53ef\u6269\u5c55\u7684\u8349\u7a3f\u6811\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6700\u5927\u5316\u63a5\u53d7\u957f\u5ea6\u7684\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u6700\u9ad8\u52a0\u901f\u6bd4\u8fbe3.2\u500d\u3002"
    },
    {
        "title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters",
        "summary": "Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which are leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup of inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.",
        "authors": "Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun",
        "published": "2024-06-24",
        "link": "http://arxiv.org/abs/2406.16758v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\uff0c\u5e76\u6269\u5c55\u4e86\u5176\u5728\u5404\u79cd\u5546\u4e1a\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u53d7\u5230\u9ad8\u63a8\u7406\u65f6\u95f4\u7684\u9650\u5236\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u5728\u63a8\u6d4b\u6027\u89e3\u7801\u4e2d\u8bad\u7ec3\u8f85\u52a9\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u7528\u4e8e\u8d77\u8349\uff0c\u7136\u540e\u7531\u76ee\u6807LLM\u9a8c\u8bc1\u5176\u672a\u6765\u6807\u8bb0\u3002\u6211\u4eec\u5c55\u793a\u4e86\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b56\u7565\u4f18\u5316\u7684\u7279\u5b9a\u8bed\u8a00\u8349\u7a3f\u6a21\u578b\uff0c\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u8457\u52a0\u5feb\u4e86\u63a8\u7406\u65f6\u95f4\u3002\u6211\u4eec\u5728\u591a\u79cd\u8bed\u8a00\u7684\u63a8\u7406\u65f6\u95f4\u3001\u9886\u57df\u5916\u52a0\u901f\u4ee5\u53caGPT-4o\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6a21\u578b\u3002",
        "tldr_en": "This paper introduces a speculative decoding approach with language-specific draft models, significantly reducing inference time in multilingual settings by leveraging targeted pretrain-and-finetune strategies.",
        "tldr_zh": "\u672c\u6587\u901a\u8fc7\u4f18\u5316\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u63a8\u6d4b\u89e3\u7801\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5546\u4e1a\u5e94\u7528\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u3002"
    },
    {
        "title": "Optimizing Speculative Decoding for Serving Large Language Models Using Goodput",
        "summary": "Reducing the inference latency of large language models (LLMs) is crucial, and speculative decoding (SD) stands out as one of the most effective techniques. Rather than letting the LLM generate all tokens directly, speculative decoding employs effective proxies to predict potential outputs, which are then verified by the LLM without compromising the generation quality. Yet, deploying SD in real online LLM serving systems (with continuous batching) does not always yield improvement -- under higher request rates or low speculation accuracy, it paradoxically increases latency. Furthermore, there is no best speculation length work for all workloads under different system loads. Based on the observations, we develop a dynamic framework SmartSpec. SmartSpec dynamically determines the best speculation length for each request (from 0, i.e., no speculation, to many tokens) -- hence the associated speculative execution costs -- based on a new metric called goodput, which characterizes the current observed load of the entire system and the speculation accuracy. We show that SmartSpec consistently reduces average request latency by up to 3.2x compared to non-speculative decoding baselines across different sizes of target models, draft models, request rates, and datasets. Moreover, SmartSpec can be applied to different styles of speculative decoding, including traditional, model-based approaches as well as model-free methods like prompt lookup and tree-style decoding.",
        "authors": "Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang",
        "published": "2024-06-20",
        "link": "http://arxiv.org/abs/2406.14066v2",
        "chinese_summary": "\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u5ef6\u8fdf\u81f3\u5173\u91cd\u8981\uff0c\u800c\u63a8\u6d4b\u6027\u89e3\u7801\uff08Speculative Decoding, SD\uff09\u4f5c\u4e3a\u6700\u6709\u6548\u7684\u6280\u672f\u4e4b\u4e00\u8131\u9896\u800c\u51fa\u3002\u63a8\u6d4b\u6027\u89e3\u7801\u4e0d\u662f\u8ba9LLM\u76f4\u63a5\u751f\u6210\u6240\u6709\u6807\u8bb0\uff0c\u800c\u662f\u5229\u7528\u6709\u6548\u7684\u4ee3\u7406\u6765\u9884\u6d4b\u6f5c\u5728\u7684\u8f93\u51fa\uff0c\u7136\u540e\u7531LLM\u8fdb\u884c\u9a8c\u8bc1\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u7684\u5728\u7ebfLLM\u670d\u52a1\u7cfb\u7edf\uff08\u5177\u6709\u6301\u7eed\u6279\u5904\u7406\u529f\u80fd\uff09\u4e2d\u90e8\u7f72SD\u5e76\u4e0d\u603b\u80fd\u5e26\u6765\u6539\u8fdb\u2014\u2014\u5728\u8bf7\u6c42\u7387\u8f83\u9ad8\u6216\u63a8\u6d4b\u51c6\u786e\u6027\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u53cd\u800c\u4f1a\u589e\u52a0\u5ef6\u8fdf\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u4e0d\u540c\u7cfb\u7edf\u8d1f\u8f7d\u4e0b\u7684\u6240\u6709\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u6ca1\u6709\u6700\u4f73\u7684\u63a8\u6d4b\u957f\u5ea6\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c2\u5bdf\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u52a8\u6001\u6846\u67b6SmartSpec\u3002SmartSpec\u6839\u636e\u4e00\u4e2a\u65b0\u7684\u6307\u6807\u2014\u2014\u79f0\u4e3a\u201c\u6709\u6548\u4ea7\u51fa\u201d\uff08goodput\uff09\uff0c\u8be5\u6307\u6807\u8868\u5f81\u4e86\u6574\u4e2a\u7cfb\u7edf\u7684\u5f53\u524d\u89c2\u6d4b\u8d1f\u8f7d\u548c\u63a8\u6d4b\u51c6\u786e\u6027\u2014\u2014\u52a8\u6001\u5730\u4e3a\u6bcf\u4e2a\u8bf7\u6c42\u786e\u5b9a\u6700\u4f73\u7684\u63a8\u6d4b\u957f\u5ea6\uff08\u4ece0\uff0c\u5373\u4e0d\u8fdb\u884c\u63a8\u6d4b\uff0c\u5230\u591a\u4e2a\u6807\u8bb0\uff09\uff0c\u4ece\u800c\u786e\u5b9a\u76f8\u5173\u7684\u63a8\u6d4b\u6267\u884c\u6210\u672c\u3002\u6211\u4eec\u5c55\u793a\u4e86SmartSpec\u5728\u4e0d\u540c\u5927\u5c0f\u7684\u76ee\u6807\u6a21\u578b\u3001\u8349\u7a3f\u6a21\u578b\u3001\u8bf7\u6c42\u7387\u548c\u6570\u636e\u96c6\u4e0b\uff0c\u76f8\u6bd4\u975e\u63a8\u6d4b\u6027\u89e3\u7801\u57fa\u7ebf\uff0c\u6301\u7eed\u5730\u5c06\u5e73\u5747\u8bf7\u6c42\u5ef6\u8fdf\u51cf\u5c11\u4e86\u9ad8\u8fbe3.2\u500d\u3002\u6b64\u5916\uff0cSmartSpec\u53ef\u4ee5\u5e94\u7528\u4e8e\u4e0d\u540c\u98ce\u683c\u7684\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u5305\u62ec\u4f20\u7edf\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u4ee5\u53ca\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u5982\u63d0\u793a\u67e5\u627e\u548c\u6811\u72b6\u89e3\u7801\u3002",
        "tldr_en": "SmartSpec dynamically adjusts speculation length based on system load and accuracy to reduce LLM inference latency up to 3.2x across various models and workloads.",
        "tldr_zh": "SmartSpec \u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u89e3\u7801\u957f\u5ea6\uff0c\u57fa\u4e8e\u7cfb\u7edf\u8d1f\u8f7d\u548c\u63a8\u6d4b\u51c6\u786e\u6027\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\uff0c\u76f8\u6bd4\u975e\u63a8\u6d4b\u89e3\u7801\u57fa\u7ebf\u6700\u591a\u53ef\u964d\u4f4e3.2\u500d\u5ef6\u8fdf\u3002"
    },
    {
        "title": "Amphista: Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style",
        "summary": "Large Language Models (LLMs) inherently use autoregressive decoding, which lacks parallelism in inference and results in significantly slow inference speeds, especially when hardware parallel accelerators and memory bandwidth are not fully utilized. In this work, we propose Amphista, a speculative decoding algorithm that adheres to a non-autoregressive decoding paradigm. Owing to the increased parallelism, our method demonstrates higher efficiency in inference compared to autoregressive methods. Specifically, Amphista models an Auto-embedding Block capable of parallel inference, incorporating bi-directional attention to enable interaction between different drafting heads. Additionally, Amphista implements Staged Adaptation Layers to facilitate the transition of semantic information from the base model's autoregressive inference to the drafting heads' non-autoregressive speculation, thereby achieving paradigm transformation and feature fusion. We conduct a series of experiments on a suite of Vicuna models using MT-Bench and Spec-Bench. For the Vicuna 33B model, Amphista achieves up to 2.75$\\times$ and 1.40$\\times$ wall-clock acceleration compared to vanilla autoregressive decoding and Medusa, respectively, while preserving lossless generation quality.",
        "authors": "Zeping Li, Xinlong Yang, Ziheng Gao, Ji Liu, Zhuang Liu, Dong Li, Jinzhang Peng, Lu Tian, Emad Barsoum",
        "published": "2024-06-19",
        "link": "http://arxiv.org/abs/2406.13170v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u672c\u8d28\u4e0a\u91c7\u7528\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u8fd9\u79cd\u89e3\u7801\u65b9\u5f0f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u5e76\u884c\u6027\uff0c\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u663e\u8457\u51cf\u6162\uff0c\u5c24\u5176\u662f\u5728\u786c\u4ef6\u5e76\u884c\u52a0\u901f\u5668\u548c\u5185\u5b58\u5e26\u5bbd\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u60c5\u51b5\u4e0b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Amphista\uff0c\u8fd9\u662f\u4e00\u79cd\u9075\u5faa\u975e\u81ea\u56de\u5f52\u89e3\u7801\u8303\u5f0f\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u7b97\u6cd5\u3002\u7531\u4e8e\u589e\u52a0\u4e86\u5e76\u884c\u6027\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63a8\u7406\u6548\u7387\u4e0a\u76f8\u6bd4\u81ea\u56de\u5f52\u65b9\u6cd5\u8868\u73b0\u66f4\u9ad8\u3002\u5177\u4f53\u800c\u8a00\uff0cAmphista\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u80fd\u591f\u5e76\u884c\u63a8\u7406\u7684\u81ea\u52a8\u5d4c\u5165\u5757\uff0c\u5e76\u7ed3\u5408\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u4e0d\u540c\u8349\u7a3f\u5934\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u6b64\u5916\uff0cAmphista\u8fd8\u5b9e\u73b0\u4e86\u5206\u9636\u6bb5\u9002\u5e94\u5c42\uff0c\u4ee5\u4fc3\u8fdb\u8bed\u4e49\u4fe1\u606f\u4ece\u57fa\u7840\u6a21\u578b\u7684\u81ea\u56de\u5f52\u63a8\u7406\u5411\u8349\u7a3f\u5934\u7684\u975e\u81ea\u56de\u5f52\u63a8\u6d4b\u8fc7\u6e21\uff0c\u4ece\u800c\u5b9e\u73b0\u8303\u5f0f\u8f6c\u6362\u548c\u7279\u5f81\u878d\u5408\u3002\u6211\u4eec\u5728\u4e00\u7cfb\u5217Vicuna\u6a21\u578b\u4e0a\u4f7f\u7528MT-Bench\u548cSpec-Bench\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5bf9\u4e8eVicuna 33B\u6a21\u578b\uff0cAmphista\u76f8\u6bd4\u539f\u59cb\u81ea\u56de\u5f52\u89e3\u7801\u548cMedusa\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.75\u500d\u548c1.40\u500d\u7684\u65f6\u949f\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u65e0\u635f\u751f\u6210\u8d28\u91cf\u3002",
        "tldr_en": "Amphista introduces a non-autoregressive speculative decoding algorithm, enhancing inference efficiency through increased parallelism and feature fusion, achieving significant speedups in Vicuna models without compromising generation quality.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51faAmphista\u7b97\u6cd5\uff0c\u901a\u8fc7\u975e\u81ea\u56de\u5f52\u89e3\u7801\u5b9e\u73b0\u5e76\u884c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u5b9e\u9a8c\u8868\u660e\u5728Vicuna 33B\u6a21\u578b\u4e0a\u6700\u9ad8\u53ef\u5b9e\u73b02.75\u500d\u52a0\u901f\uff0c\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u65e0\u635f\u3002"
    },
    {
        "title": "Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding",
        "summary": "Large Language Models (LLMs) demonstrate impressive performance in diverse applications, yet they face significant drawbacks, including high inference latency, expensive training cost, and generation of hallucination. Collaborative decoding between large and small language models (SLMs) offers a novel approach to address these challenges. Inspired by dual-process cognitive theory, we integrate these methods into a unified framework termed Fast and Slow Generating (FS-GEN). This paper explores several techniques within the FS-GEN framework, including speculative decoding, contrastive decoding, and emulator or proxy fine-tuning. We provide a comprehensive analysis of these methodologies, offering insights into their similarities and differences under this framework. Our study delves into the differential knowledge capabilities of LLMs versus SLMs through the FS-GEN lens, revealing that fewer than 20% of collaborative interactions are required across various methods. These interactions adhere to a scaling law relative to the parameter ratios, thereby facilitating predictable collaboration. Furthermore, we investigate the specific positions where collaboration is most effective from an uncertainty perspective, yielding novel insights that could refine FS-GEN methods. Our findings reveal that the essential difference between models of different sizes lies in the uncertainty of the next token prediction, where interventions by larger models are most needed to assist the smaller ones. Code for Reproduction: https://github.com/TsinghuaC3I/FS-GEN",
        "authors": "Kaiyan Zhang, Jianyu Wang, Ning Ding, Biqing Qi, Ermo Hua, Xingtai Lv, Bowen Zhou",
        "published": "2024-06-18",
        "link": "http://arxiv.org/abs/2406.12295v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6837\u5316\u7684\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u4e5f\u9762\u4e34\u7740\u663e\u8457\u7684\u7f3a\u9677\uff0c\u5305\u62ec\u9ad8\u63a8\u7406\u5ef6\u8fdf\u3001\u6602\u8d35\u7684\u8bad\u7ec3\u6210\u672c\u4ee5\u53ca\u751f\u6210\u5e7b\u89c9\u7684\u95ee\u9898\u3002\u5927\u6a21\u578b\u4e0e\u5c0f\u6a21\u578b\uff08SLMs\uff09\u4e4b\u95f4\u7684\u534f\u4f5c\u89e3\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u53d7\u53cc\u8fc7\u7a0b\u8ba4\u77e5\u7406\u8bba\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\uff0c\u79f0\u4e3a\u201c\u5feb\u4e0e\u6162\u751f\u6210\u201d\uff08FS-GEN\uff09\u3002\u672c\u6587\u63a2\u8ba8\u4e86FS-GEN\u6846\u67b6\u5185\u7684\u51e0\u79cd\u6280\u672f\uff0c\u5305\u62ec\u63a8\u6d4b\u89e3\u7801\u3001\u5bf9\u6bd4\u89e3\u7801\u4ee5\u53ca\u6a21\u62df\u5668\u6216\u4ee3\u7406\u7684\u5fae\u8c03\u3002\u6211\u4eec\u5bf9\u8be5\u6846\u67b6\u4e0b\u7684\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5b83\u4eec\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u7684\u89c1\u89e3\u3002\u6211\u4eec\u7684\u7814\u7a76\u901a\u8fc7FS-GEN\u7684\u89c6\u89d2\u6df1\u5165\u63a2\u8ba8\u4e86LLMs\u4e0eSLMs\u5728\u77e5\u8bc6\u80fd\u529b\u4e0a\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4e0d\u540c\u65b9\u6cd5\u4e2d\u6240\u9700\u7684\u534f\u4f5c\u4ea4\u4e92\u4e0d\u523020%\u3002\u8fd9\u4e9b\u4ea4\u4e92\u9075\u5faa\u76f8\u5bf9\u4e8e\u53c2\u6570\u6bd4\u4f8b\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u53ef\u9884\u6d4b\u7684\u534f\u4f5c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4ece\u4e0d\u786e\u5b9a\u6027\u7684\u89d2\u5ea6\u7814\u7a76\u4e86\u534f\u4f5c\u6700\u6709\u6548\u7684\u5177\u4f53\u4f4d\u7f6e\uff0c\u5f97\u51fa\u4e86\u53ef\u80fd\u6539\u8fdbFS-GEN\u65b9\u6cd5\u7684\u65b0\u89c1\u89e3\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e4b\u95f4\u7684\u672c\u8d28\u533a\u522b\u5728\u4e8e\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6b64\u65f6\u5927\u6a21\u578b\u7684\u5e72\u9884\u6700\u4e3a\u5fc5\u8981\uff0c\u4ee5\u534f\u52a9\u5c0f\u6a21\u578b\u3002\u91cd\u73b0\u4ee3\u7801\uff1ahttps://github.com/TsinghuaC3I/FS-GEN",
        "tldr_en": "This paper introduces Fast and Slow Generating (FS-GEN), a unified framework leveraging collaborative decoding between Large Language Models (LLMs) and Small Language Models (SLMs) to address LLMs' drawbacks, revealing that less than 20% of interactions are needed for effective collaboration, primarily in areas of high uncertainty in token prediction.",
        "tldr_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6837\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9762\u4e34\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u8bad\u7ec3\u6210\u672c\u6602\u8d35\u53ca\u751f\u6210\u5e7b\u89c9\u7b49\u95ee\u9898\u3002\u57fa\u4e8e\u53cc\u8fc7\u7a0b\u8ba4\u77e5\u7406\u8bba\uff0c\u6211\u4eec\u63d0\u51fa\u201c\u5feb\u6162\u751f\u6210\u201d\uff08FS-GEN\uff09\u6846\u67b6\uff0c\u6574\u5408\u5927\u6a21\u578b\u4e0e\u5c0f\u6a21\u578b\uff08SLMs\uff09\u7684\u534f\u540c\u89e3\u7801\uff0c\u63a2\u7d22\u63a8\u6d4b\u89e3\u7801\u3001\u5bf9\u6bd4\u89e3\u7801\u53ca\u6a21\u62df\u5668\u5fae\u8c03\u7b49\u6280\u672f\uff0c\u63ed\u793a\u534f\u540c\u4ea4\u4e92\u4ec5\u9700\u4e0d\u523020%\uff0c\u4e14\u9075\u5faa\u53c2\u6570\u6bd4\u4f8b\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u6709\u6548\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002"
    },
    {
        "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models",
        "summary": "Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.",
        "authors": "Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Yingyan Celine Lin",
        "published": "2024-06-11",
        "link": "http://arxiv.org/abs/2406.07368v2",
        "chinese_summary": "\u81ea\u56de\u5f52\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\uff0c\u4f46\u9762\u4e34\u7740\u4e24\u4e2a\u91cd\u5927\u74f6\u9888\uff1a\uff081\uff09\u968f\u7740token\u6570\u91cf\u7684\u589e\u52a0\uff0c\u6ce8\u610f\u529b\u6a21\u5757\u7684\u590d\u6742\u5ea6\u5448\u4e8c\u6b21\u589e\u957f\uff1b\uff082\uff09\u7531\u4e8e\u81ea\u56de\u5f52LLMs\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u91c7\u7528\u987a\u5e8f\u5904\u7406\u65b9\u5f0f\uff0c\u5bfc\u81f4\u6548\u7387\u53d7\u9650\u3002\u5c3d\u7ba1\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u63a8\u6d4b\u6027\u89e3\u7801\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b83\u4eec\u5728\u589e\u5f3a\u81ea\u56de\u5f52LLMs\u65b9\u9762\u7684\u9002\u7528\u6027\u548c\u534f\u540c\u6f5c\u529b\u4ecd\u4e0d\u786e\u5b9a\u3002\u6211\u4eec\u9996\u6b21\u5bf9\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u81ea\u56de\u5f52LLMs\u4e2d\u7684\u6709\u6548\u6027\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e\u63a8\u6d4b\u6027\u89e3\u7801\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u589e\u5f3a\u6280\u672f\uff0c\u786e\u4fdd\u5176\u4e0e\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u517c\u5bb9\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684LLMs\u8bad\u7ec3\u548c\u670d\u52a1\u3002\u901a\u8fc7\u6d89\u53ca\u4e03\u79cd\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u548c\u4e94\u79cd\u57fa\u4e8e\u7f16\u7801\u5668/\u89e3\u7801\u5668\u7684LLMs\u7684\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u4e00\u81f4\u9a8c\u8bc1\u4e86\u6211\u4eec\u589e\u5f3a\u7684\u7ebf\u6027\u5316LLMs\u7684\u6709\u6548\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728LLaMA\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe6.67\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e\uff0c\u5e76\u4e14\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f8\u6bd4\u4e4b\u524d\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u52a0\u901f\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728https://github.com/GATECH-EIC/Linearized-LLM\u83b7\u53d6\u3002",
        "tldr_en": "We comprehensively study linear attention methods for autoregressive LLMs, integrating them with speculative decoding to enhance efficiency, achieving up to 6.67 perplexity reduction and 2\u00d7 speedup in generation.",
        "tldr_zh": "\u81ea\u56de\u5f52\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9762\u4e34\u6ce8\u610f\u529b\u6a21\u5757\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u751f\u6210\u65f6\u987a\u5e8f\u5904\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u74f6\u9888\u3002\u6211\u4eec\u9996\u6b21\u5168\u9762\u7814\u7a76\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u5bf9\u81ea\u56de\u5f52LLMs\u7684\u6709\u6548\u6027\uff0c\u5e76\u7ed3\u5408\u63a8\u6d4b\u89e3\u7801\uff0c\u63d0\u51fa\u786e\u4fdd\u4e0e\u63a8\u6d4b\u89e3\u7801\u517c\u5bb9\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347LLMs\u7684\u8bad\u7ec3\u548c\u670d\u52a1\u6548\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u56f0\u60d1\u5ea6\u964d\u4f4e6.67\u548c\u751f\u6210\u901f\u5ea6\u63d0\u53472\u500d\u3002"
    },
    {
        "title": "Proofread: Fixes All Errors with One Tap",
        "summary": "The impressive capabilities in Large Language Models (LLMs) provide a powerful approach to reimagine users' typing experience. This paper demonstrates Proofread, a novel Gboard feature powered by a server-side LLM in Gboard, enabling seamless sentence-level and paragraph-level corrections with a single tap. We describe the complete system in this paper, from data generation, metrics design to model tuning and deployment. To obtain models with sufficient quality, we implement a careful data synthetic pipeline tailored to online use cases, design multifaceted metrics, employ a two-stage tuning approach to acquire the dedicated LLM for the feature: the Supervised Fine Tuning (SFT) for foundational quality, followed by the Reinforcement Learning (RL) tuning approach for targeted refinement. Specifically, we find sequential tuning on Rewrite and proofread tasks yields the best quality in SFT stage, and propose global and direct rewards in the RL tuning stage to seek further improvement. Extensive experiments on a human-labeled golden set showed our tuned PaLM2-XS model achieved 85.56\\% good ratio. We launched the feature to Pixel 8 devices by serving the model on TPU v5 in Google Cloud, with thousands of daily active users. Serving latency was significantly reduced by quantization, bucket inference, text segmentation, and speculative decoding. Our demo could be seen in \\href{https://youtu.be/4ZdcuiwFU7I}{Youtube}.",
        "authors": "Renjie Liu, Yanxiang Zhang, Yun Zhu, Haicheng Sun, Yuanbo Zhang, Michael Xuelin Huang, Shanqing Cai, Lei Meng, Shumin Zhai",
        "published": "2024-06-06",
        "link": "http://arxiv.org/abs/2406.04523v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u663e\u8457\u80fd\u529b\u4e3a\u91cd\u65b0\u6784\u60f3\u7528\u6237\u7684\u6253\u5b57\u4f53\u9a8c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u65b9\u6cd5\u3002\u672c\u6587\u5c55\u793a\u4e86Proofread\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531Gboard\u670d\u52a1\u5668\u7aefLLM\u9a71\u52a8\u7684\u65b0\u578bGboard\u529f\u80fd\uff0c\u80fd\u591f\u901a\u8fc7\u4e00\u6b21\u70b9\u51fb\u5b9e\u73b0\u65e0\u7f1d\u7684\u53e5\u5b50\u7ea7\u548c\u6bb5\u843d\u7ea7\u4fee\u6b63\u3002\u6211\u4eec\u5728\u672c\u6587\u4e2d\u63cf\u8ff0\u4e86\u5b8c\u6574\u7684\u7cfb\u7edf\uff0c\u4ece\u6570\u636e\u751f\u6210\u3001\u6307\u6807\u8bbe\u8ba1\u5230\u6a21\u578b\u8c03\u4f18\u548c\u90e8\u7f72\u3002\u4e3a\u4e86\u83b7\u5f97\u5177\u6709\u8db3\u591f\u8d28\u91cf\u7684\u6a21\u578b\uff0c\u6211\u4eec\u5b9e\u65bd\u4e86\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5728\u7ebf\u4f7f\u7528\u573a\u666f\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u8bbe\u8ba1\u4e86\u591a\u65b9\u9762\u7684\u6307\u6807\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8c03\u4f18\u65b9\u6cd5\u6765\u83b7\u53d6\u8be5\u529f\u80fd\u7684\u4e13\u7528LLM\uff1a\u9996\u5148\u662f\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u5960\u5b9a\u57fa\u7840\u8d28\u91cf\uff0c\u968f\u540e\u662f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8c03\u4f18\u65b9\u6cd5\u4ee5\u8fdb\u884c\u9488\u5bf9\u6027\u4f18\u5316\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u53d1\u73b0\u5bf9\u91cd\u5199\u548c\u6821\u5bf9\u4efb\u52a1\u8fdb\u884c\u987a\u5e8f\u8c03\u4f18\u5728SFT\u9636\u6bb5\u4ea7\u751f\u4e86\u6700\u4f73\u8d28\u91cf\uff0c\u5e76\u5728RL\u8c03\u4f18\u9636\u6bb5\u63d0\u51fa\u4e86\u5168\u5c40\u548c\u76f4\u63a5\u5956\u52b1\u4ee5\u5bfb\u6c42\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002\u5728\u4eba\u5de5\u6807\u6ce8\u7684\u91d1\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u8c03\u4f18\u7684PaLM2-XS\u6a21\u578b\u8fbe\u5230\u4e8685.56%\u7684\u826f\u597d\u6bd4\u7387\u3002\u6211\u4eec\u5c06\u8be5\u529f\u80fd\u53d1\u5e03\u5230Pixel 8\u8bbe\u5907\u4e0a\uff0c\u901a\u8fc7\u5728Google Cloud\u7684TPU v5\u4e0a\u63d0\u4f9b\u6a21\u578b\u670d\u52a1\uff0c\u62e5\u6709\u6570\u5343\u540d\u6bcf\u65e5\u6d3b\u8dc3\u7528\u6237\u3002\u901a\u8fc7\u91cf\u5316\u3001\u5206\u6876\u63a8\u7406\u3001\u6587\u672c\u5206\u5272\u548c\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u670d\u52a1\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u3002\u6211\u4eec\u7684\u6f14\u793a\u53ef\u4ee5\u5728\\href{https://youtu.be/4ZdcuiwFU7I}{Youtube}\u4e0a\u89c2\u770b\u3002",
        "tldr_en": "This paper introduces Proofread, a Gboard feature leveraging a server-side LLM for seamless sentence and paragraph corrections, optimized through a two-stage tuning process and deployed on Pixel 8 devices with high user engagement and reduced latency.",
        "tldr_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578bGboard\u529f\u80fdProofread\uff0c\u901a\u8fc7\u5355\u6b21\u70b9\u51fb\u5b9e\u73b0\u53e5\u5b50\u548c\u6bb5\u843d\u7ea7\u522b\u7684\u65e0\u7f1d\u6821\u6b63\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u4ece\u6570\u636e\u751f\u6210\u3001\u6307\u6807\u8bbe\u8ba1\u5230\u6a21\u578b\u8c03\u4f18\u548c\u90e8\u7f72\u7684\u5b8c\u6574\u7cfb\u7edf\u6d41\u7a0b\u3002"
    },
    {
        "title": "Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson Sampling Control Mechanism",
        "summary": "The recent advancements in large language models (LLMs) have been extraordinary, yet the escalating inference costs associated with them present challenges in real-world applications. To address these challenges, we propose a novel approach called Early-exiting Speculative Decoding (EESD) with lossless acceleration. Specifically, EESD utilizes a segment of the LLM to generate draft tokens, incorporating Early-exiting structures after the first N layers. To enhance the quality of draft tokens, a self-distillation method is integrated. This early-exiting design not only reduces deployment and training costs but also significantly accelerates the token generation speed. Moreover, we introduce a novel sampling mechanism that leverages Thompson Sampling to regulate the generation processes, automatically determining the quantity of draft tokens in each round. The original LLM is then employed to validate these draft tokens through a single forward pass, and thus guarantees that the final output text maintains a distribution consistent with vanilla auto-regressive decoding. The experimental results on both 13B and 70B models demonstrate that our approach decodes tokens at a markedly accelerated rate compared to prior methods, showing the effectiveness of our approach.",
        "authors": "Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai",
        "published": "2024-06-06",
        "link": "http://arxiv.org/abs/2406.03853v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u4ee4\u4eba\u77a9\u76ee\uff0c\u4f46\u5176\u4e0d\u65ad\u6500\u5347\u7684\u63a8\u7406\u6210\u672c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e26\u6765\u4e86\u6311\u6218\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65e0\u635f\u52a0\u901f\u7684\u65e9\u671f\u9000\u51fa\u63a8\u6d4b\u89e3\u7801\uff08Early-exiting Speculative Decoding, EESD\uff09\u7684\u65b0\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0cEESD\u5229\u7528LLM\u7684\u4e00\u90e8\u5206\u751f\u6210\u8349\u7a3f\u4ee4\u724c\uff0c\u5e76\u5728\u524dN\u5c42\u4e4b\u540e\u5f15\u5165\u65e9\u671f\u9000\u51fa\u7ed3\u6784\u3002\u4e3a\u63d0\u5347\u8349\u7a3f\u4ee4\u724c\u7684\u8d28\u91cf\uff0c\u6211\u4eec\u6574\u5408\u4e86\u81ea\u84b8\u998f\u65b9\u6cd5\u3002\u8fd9\u79cd\u65e9\u671f\u9000\u51fa\u8bbe\u8ba1\u4e0d\u4ec5\u964d\u4f4e\u4e86\u90e8\u7f72\u548c\u8bad\u7ec3\u6210\u672c\uff0c\u8fd8\u663e\u8457\u52a0\u5feb\u4e86\u4ee4\u724c\u751f\u6210\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u673a\u5236\uff0c\u5229\u7528\u6c64\u666e\u68ee\u91c7\u6837\u6765\u8c03\u63a7\u751f\u6210\u8fc7\u7a0b\uff0c\u81ea\u52a8\u786e\u5b9a\u6bcf\u8f6e\u751f\u6210\u7684\u8349\u7a3f\u4ee4\u724c\u6570\u91cf\u3002\u968f\u540e\uff0c\u539f\u59cbLLM\u901a\u8fc7\u4e00\u6b21\u524d\u5411\u4f20\u9012\u9a8c\u8bc1\u8fd9\u4e9b\u8349\u7a3f\u4ee4\u724c\uff0c\u4ece\u800c\u786e\u4fdd\u6700\u7ec8\u8f93\u51fa\u6587\u672c\u4e0e\u666e\u901a\u81ea\u56de\u5f52\u89e3\u7801\u7684\u5206\u5e03\u4e00\u81f4\u3002\u572813B\u548c70B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89e3\u7801\u4ee4\u724c\u65f6\u663e\u8457\u63d0\u901f\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002",
        "tldr_en": "We propose Early-exiting Speculative Decoding (EESD) with lossless acceleration to address the high inference costs of large language models, leveraging a segment of the LLM to generate draft tokens and a novel sampling mechanism to significantly speed up token generation while maintaining output quality.",
        "tldr_zh": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65e9\u671f\u9000\u51fa\u63a8\u6d4b\u89e3\u7801\uff08EESD\uff09\u7684\u65e0\u635f\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u90e8\u5206\u751f\u6210\u8349\u7a3f\u4ee4\u724c\uff0c\u7ed3\u5408\u65e9\u671f\u9000\u51fa\u7ed3\u6784\u548c\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u90e8\u7f72\u548c\u8bad\u7ec3\u6210\u672c\u5e76\u52a0\u901f\u4ee4\u724c\u751f\u6210\u901f\u5ea6\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u6c64\u666e\u68ee\u91c7\u6837\u7684\u91c7\u6837\u673a\u5236\uff0c\u786e\u4fdd\u6700\u7ec8\u8f93\u51fa\u6587\u672c\u4e0e\u4f20\u7edf\u81ea\u56de\u5f52\u89e3\u7801\u5206\u5e03\u4e00\u81f4\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u572813B\u548c70B\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"
    },
    {
        "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices",
        "summary": "As large language models gain widespread adoption, running them efficiently becomes crucial. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD. When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities. SpecExec takes the most probable tokens continuation from the draft model to build a \"cache\" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights.",
        "authors": "Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin",
        "published": "2024-06-04",
        "link": "http://arxiv.org/abs/2406.02532v2",
        "chinese_summary": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u4f55\u9ad8\u6548\u8fd0\u884c\u5b83\u4eec\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u6700\u8fd1\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u7814\u7a76\u91c7\u7528\u4e86\u63a8\u6d4b\u6027\u89e3\u7801\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u6781\u901f\u52a0\u901f\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u8fd9\u4e9b\u7814\u7a76\u9690\u542b\u5730\u4e3a\u9ad8\u7aef\u6570\u636e\u4e2d\u5fc3\u786c\u4ef6\u8bbe\u8ba1\u4e86\u7b97\u6cd5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u76f8\u53cd\u7684\u95ee\u9898\uff1a\u6211\u4eec\u80fd\u5728\u6d88\u8d39\u7ea7\u673a\u5668\u4e0a\u4ee5\u591a\u5feb\u7684\u901f\u5ea6\u8fd0\u884c\u5927\u8bed\u8a00\u6a21\u578b\uff1f\u6d88\u8d39\u7ea7GPU\u5df2\u7ecf\u65e0\u6cd5\u5bb9\u7eb3\u5f53\u524d\u6700\u5927\u7684\u6a21\u578b\uff08500\u4ebf+\u53c2\u6570\uff09\uff0c\u5fc5\u987b\u5c06\u5b83\u4eec\u5378\u8f7d\u5230RAM\u6216SSD\u4e2d\u3002\u5f53\u4f7f\u7528\u5378\u8f7d\u53c2\u6570\u8fd0\u884c\u65f6\uff0c\u63a8\u7406\u5f15\u64ce\u53ef\u4ee5\u540c\u65f6\u5904\u7406\u6570\u767e\u6216\u6570\u5343\u4e2a\u6807\u8bb0\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e00\u4e2a\u6807\u8bb0\uff0c\u8fd9\u4f7f\u5176\u81ea\u7136\u9002\u5408\u63a8\u6d4b\u6027\u89e3\u7801\u3002\u6211\u4eec\u63d0\u51fa\u4e86SpecExec\uff08\u63a8\u6d4b\u6027\u6267\u884c\uff09\uff0c\u4e00\u79cd\u7b80\u5355\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6bcf\u6b21\u76ee\u6807\u6a21\u578b\u8fed\u4ee3\u4e2d\u751f\u6210\u591a\u8fbe20\u4e2a\u6807\u8bb0\uff0c\u9002\u7528\u4e8e\u6d41\u884c\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u3002SpecExec\u5229\u7528\u4e86\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6807\u8bb0\u6982\u7387\u5206\u5e03\u7684\u9ad8\u5cf0\u6027\u548c\u6a21\u578b\u8f93\u51fa\u6982\u7387\u4e4b\u95f4\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002SpecExec\u4ece\u8349\u7a3f\u6a21\u578b\u4e2d\u83b7\u53d6\u6700\u53ef\u80fd\u7684\u6807\u8bb0\u5ef6\u7eed\uff0c\u4e3a\u76ee\u6807\u6a21\u578b\u6784\u5efa\u4e00\u4e2a\u201c\u7f13\u5b58\u201d\u6811\uff0c\u7136\u540e\u5728\u4e00\u6b21\u904d\u5386\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002\u901a\u8fc7\u4f7f\u7528SpecExec\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u4f7f\u7528RAM\u5378\u8f7d\u7684500\u4ebf+\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c4\u4f4d\u91cf\u5316\u65f6\u4e3a\u6bcf\u79d24-6\u4e2a\u6807\u8bb0\uff0c16\u4f4d\u6743\u91cd\u65f6\u4e3a\u6bcf\u79d22-3\u4e2a\u6807\u8bb0\u3002",
        "tldr_en": "This work introduces SpecExec, a parallel decoding method enabling efficient inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading, achieving up to 6 tokens per second with 4-bit quantization.",
        "tldr_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6cd5SpecExec\uff0c\u901a\u8fc7\u63a8\u6d4b\u89e3\u7801\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5728\u5185\u5b58\u5378\u8f7d\u6761\u4ef6\u4e0b\u5bf9500\u4ebf\u53c2\u6570\u4ee5\u4e0a\u6a21\u578b\u7684\u63a8\u7406\u52a0\u901f\uff0c\u8fbe\u5230\u6bcf\u79d24-6\u4e2atoken\u7684\u5904\u7406\u901f\u5ea6\u3002"
    },
    {
        "title": "S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs",
        "summary": "Speculative decoding (SD) has attracted a significant amount of research attention due to the substantial speedup it can achieve for LLM inference. However, despite the high speedups they offer, speculative decoding methods often achieve optimal performance on high-end devices or with a substantial GPU memory overhead. Given limited memory and the necessity of quantization, a high-performing model on a high-end GPU can slow down by up to 7 times. To this end, we propose Skippy Simultaneous Speculative Decoding (or S3D), a cost-effective self-speculative SD method based on simultaneous multi-token decoding and mid-layer skipping. When compared against recent effective open-source SD systems, our method has achieved one of the top performance-memory ratios while requiring minimal architecture changes and training data. Leveraging our memory efficiency, we created a smaller yet more effective SD model based on Phi-3. It is 1.4 to 2 times faster than the quantized EAGLE model and operates in half-precision while using less VRAM.",
        "authors": "Wei Zhong, Manasa Bharadwaj",
        "published": "2024-05-30",
        "link": "http://arxiv.org/abs/2405.20314v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\uff08Speculative Decoding, SD\uff09\u56e0\u5176\u80fd\u591f\u663e\u8457\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u8fc7\u7a0b\u800c\u53d7\u5230\u4e86\u5927\u91cf\u7814\u7a76\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u901f\u7684\u63a8\u7406\u52a0\u901f\uff0c\u5b83\u4eec\u901a\u5e38\u5728\u9ad8\u7aef\u8bbe\u5907\u4e0a\u6216\u5728\u9700\u8981\u5927\u91cfGPU\u5185\u5b58\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u624d\u80fd\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002\u5728\u5185\u5b58\u6709\u9650\u4e14\u5fc5\u987b\u8fdb\u884c\u91cf\u5316\u7684\u524d\u63d0\u4e0b\uff0c\u9ad8\u7aefGPU\u4e0a\u7684\u9ad8\u6027\u80fd\u6a21\u578b\u53ef\u80fd\u4f1a\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u964d\u4f4e\u591a\u8fbe7\u500d\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Skippy\u540c\u65f6\u63a8\u6d4b\u6027\u89e3\u7801\uff08\u6216\u79f0S3D\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u540c\u65f6\u591a\u4ee4\u724c\u89e3\u7801\u548c\u4e2d\u5c42\u8df3\u8fc7\u7684\u7ecf\u6d4e\u578b\u81ea\u63a8\u6d4b\u6027SD\u65b9\u6cd5\u3002\u4e0e\u8fd1\u671f\u6709\u6548\u7684\u5f00\u6e90SD\u7cfb\u7edf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd-\u5185\u5b58\u6bd4\u7387\u4e0a\u8fbe\u5230\u4e86\u9876\u5c16\u6c34\u5e73\uff0c\u540c\u65f6\u4ec5\u9700\u6700\u5c0f\u7684\u67b6\u6784\u6539\u52a8\u548c\u8bad\u7ec3\u6570\u636e\u3002\u501f\u52a9\u6211\u4eec\u7684\u5185\u5b58\u6548\u7387\uff0c\u6211\u4eec\u57fa\u4e8ePhi-3\u521b\u5efa\u4e86\u4e00\u4e2a\u66f4\u5c0f\u4f46\u66f4\u6709\u6548\u7684SD\u6a21\u578b\u3002\u8be5\u6a21\u578b\u6bd4\u91cf\u5316\u540e\u7684EAGLE\u6a21\u578b\u5feb1.4\u81f32\u500d\uff0c\u5e76\u4e14\u5728\u534a\u7cbe\u5ea6\u4e0b\u8fd0\u884c\u65f6\u4f7f\u7528\u7684VRAM\u66f4\u5c11\u3002",
        "tldr_en": "We propose Skippy Simultaneous Speculative Decoding (S3D), a cost-effective self-speculative method achieving top performance-memory ratios with minimal changes, enabling faster Phi-3-based SD models that outperform quantized EAGLE models in speed and VRAM usage.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86Skippy Simultaneous Speculative Decoding (S3D)\uff0c\u4e00\u79cd\u57fa\u4e8e\u540c\u65f6\u591a\u4ee4\u724c\u89e3\u7801\u548c\u4e2d\u5c42\u8df3\u8fc7\u7684\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u5f00\u6e90\u7cfb\u7edf\uff0c\u5728\u6027\u80fd-\u5185\u5b58\u6bd4\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u4ec5\u9700\u6700\u5c0f\u67b6\u6784\u6539\u52a8\u548c\u8bad\u7ec3\u6570\u636e\uff0c\u57fa\u4e8ePhi-3\u7684\u6a21\u578b\u6bd4\u91cf\u5316EAGLE\u6a21\u578b\u5feb1.4\u81f32\u500d\uff0c\u4f7f\u7528\u534a\u7cbe\u5ea6\u4e14VRAM\u6d88\u8017\u66f4\u5c11\u3002"
    },
    {
        "title": "SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths",
        "summary": "Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model. Its performance depends on a hyperparameter K -- the candidate length, i.e., the number of candidate tokens for the target model to verify in each round. However, previous methods often use simple heuristics to choose K, which may result in sub-optimal performance. We study the choice of the candidate length K and formulate it as a Markov Decision Process. We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value. Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold. We implement SpecDec++ and apply it to the llama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (an additional 7.2% improvement over the baseline speculative decoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively.",
        "authors": "Kaixuan Huang, Xudong Guo, Mengdi Wang",
        "published": "2024-05-30",
        "link": "http://arxiv.org/abs/2405.19715v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u901a\u8fc7\u5229\u7528\u66f4\u5c0f\u3001\u66f4\u5feb\u7684\u8349\u7a3f\u6a21\u578b\u6765\u51cf\u5c11\u76ee\u6807\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u5176\u6027\u80fd\u53d6\u51b3\u4e8e\u4e00\u4e2a\u8d85\u53c2\u6570K\u2014\u2014\u5019\u9009\u957f\u5ea6\uff0c\u5373\u76ee\u6807\u6a21\u578b\u5728\u6bcf\u4e00\u8f6e\u4e2d\u9700\u8981\u9a8c\u8bc1\u7684\u5019\u9009\u8bcd\u5143\u6570\u91cf\u3002\u7136\u800c\uff0c\u4ee5\u5f80\u7684\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u9009\u62e9K\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u6211\u4eec\u7814\u7a76\u4e86\u5019\u9009\u957f\u5ea6K\u7684\u9009\u62e9\uff0c\u5e76\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002\u6211\u4eec\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8be5\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u6700\u4f18\u7b56\u7565\u91c7\u53d6\u9608\u503c\u7b56\u7565\u7684\u5f62\u5f0f\uff0c\u5373\u5f53\u83b7\u5f97\u62d2\u7edd\u7684\u6982\u7387\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\u65f6\uff0c\u5f53\u524d\u7684\u63a8\u6d4b\u5e94\u505c\u6b62\u5e76\u8fdb\u884c\u9a8c\u8bc1\u3002\u53d7\u6b64\u7406\u8bba\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SpecDec++\uff0c\u8fd9\u662f\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u589e\u5f3a\u7248\u672c\uff0c\u80fd\u591f\u52a8\u6001\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u5019\u9009\u957f\u5ea6\u3002\u6211\u4eec\u5728\u8349\u7a3f\u6a21\u578b\u4e0a\u589e\u52a0\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u63a5\u53d7\u9884\u6d4b\u5934\uff0c\u7528\u4e8e\u9884\u6d4b\u5019\u9009\u8bcd\u5143\u7684\u6761\u4ef6\u63a5\u53d7\u6982\u7387\u3002\u5f53\u9884\u6d4b\u5230\u81f3\u5c11\u6709\u4e00\u4e2a\u8bcd\u5143\u88ab\u62d2\u7edd\u7684\u6982\u7387\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\u65f6\uff0cSpecDec++\u5c06\u505c\u6b62\u5f53\u524d\u7684\u63a8\u6d4b\u3002\u6211\u4eec\u5b9e\u73b0\u4e86SpecDec++\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8ellama-2-chat 7B & 70B\u6a21\u578b\u5bf9\u3002\u6211\u4eec\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u5728Alpaca\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e862.04\u500d\u7684\u52a0\u901f\uff08\u6bd4\u57fa\u7ebf\u63a8\u6d4b\u6027\u89e3\u7801\u989d\u5916\u63d0\u5347\u4e867.2%\uff09\u3002\u5728GSM8K\u548cHumanEval\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e862.26\u500d\uff08\u63d0\u53479.4%\uff09\u548c2.23\u500d\uff08\u63d0\u534711.1%\uff09\u7684\u52a0\u901f\u3002",
        "tldr_en": "SpecDec++ enhances speculative decoding by adaptively determining candidate length via a Markov Decision Process-based threshold policy, achieving significant speedups across datasets.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u786e\u5b9a\u5019\u9009\u957f\u5ea6\u7684\u589e\u5f3a\u7248\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5SpecDec++\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\u6548\u679c\u3002"
    },
    {
        "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution",
        "summary": "Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B.",
        "authors": "Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin",
        "published": "2024-05-29",
        "link": "http://arxiv.org/abs/2405.19325v2",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4e3a\u5176\u751f\u6210\u5185\u5bb9\u63d0\u4f9b\u5f52\u5c5e\u7684\u80fd\u529b\u3002\u534a\u53c2\u6570\u5316\u8bed\u8a00\u6a21\u578b\uff08\u5982kNN-LM\uff09\u901a\u8fc7\u5229\u7528\u975e\u53c2\u6570\u5316\u6570\u636e\u5b58\u50a8\u4e2d\u7684\u6700\u8fd1\u90bb\u5339\u914d\u6765\u4f18\u5316\u7ed9\u5b9a\u63d0\u793a\u7684\u6a21\u578b\u8f93\u51fa\uff0c\u4ece\u800c\u5e94\u5bf9\u8fd9\u4e9b\u5c40\u9650\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u63a8\u7406\u901f\u5ea6\u8f83\u6162\uff0c\u4e14\u751f\u6210\u7684\u6587\u672c\u4e0d\u591f\u6d41\u7545\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u534a\u53c2\u6570\u5316\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5\u2014\u2014\u6700\u8fd1\u90bb\u63a8\u6d4b\u89e3\u7801\uff08NEST\uff09\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u4efb\u610f\u957f\u5ea6\u7684\u771f\u5b9e\u6587\u672c\u7247\u6bb5\u878d\u5165\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u4e2d\uff0c\u5e76\u4e3a\u5176\u6765\u6e90\u63d0\u4f9b\u5f52\u5c5e\u3002NEST\u5728\u6bcf\u6b21\u63a8\u7406\u6b65\u9aa4\u4e2d\u8fdb\u884c\u8bcd\u5143\u7ea7\u522b\u7684\u68c0\u7d22\uff0c\u4ee5\u8ba1\u7b97\u534a\u53c2\u6570\u5316\u6df7\u5408\u5206\u5e03\uff0c\u5e76\u5728\u8bed\u6599\u5e93\u4e2d\u8bc6\u522b\u6709\u524d\u666f\u7684\u7247\u6bb5\u5ef6\u7eed\u3002\u968f\u540e\uff0c\u5b83\u91c7\u7528\u4e00\u79cd\u8fd1\u4f3c\u7684\u63a8\u6d4b\u89e3\u7801\u7a0b\u5e8f\uff0c\u63a5\u53d7\u68c0\u7d22\u5230\u7684\u7247\u6bb5\u524d\u7f00\u6216\u751f\u6210\u65b0\u8bcd\u5143\u3002NEST\u5728\u591a\u79cd\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u548c\u5f52\u5c5e\u7387\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684kNN-LM\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0a\u4e0b\u6587\u68c0\u7d22\u589e\u5f3a\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u6b64\u5916\uff0cNEST\u5927\u5e45\u63d0\u5347\u4e86\u751f\u6210\u901f\u5ea6\uff0c\u5e94\u7528\u4e8eLlama-2-Chat 70B\u65f6\uff0c\u63a8\u7406\u65f6\u95f4\u52a0\u901f\u8fbe1.8\u500d\u3002",
        "tldr_en": "NEST introduces a novel semi-parametric approach for faster, more accurate, and attributed language model generations by integrating real-world text spans and enhancing inference speed.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNearest Neighbor Speculative Decoding (NEST)\u7684\u65b0\u578b\u534a\u53c2\u6570\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u6b65\u9aa4\u4e2d\u8fdb\u884c\u4ee4\u724c\u7ea7\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u5f52\u5c5e\u7387\uff0c\u5e76\u5728\u751f\u6210\u901f\u5ea6\u4e0a\u5b9e\u73b0\u4e861.8\u500d\u7684\u52a0\u901f\u3002"
    },
    {
        "title": "Faster Cascades via Speculative Decoding",
        "summary": "Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for \"hard\" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines.",
        "authors": "Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, Sanjiv Kumar",
        "published": "2024-05-29",
        "link": "http://arxiv.org/abs/2405.19261v1",
        "chinese_summary": "\u7ea7\u8054\u548c\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e24\u79cd\u5e38\u89c1\u7684\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\u7684\u65b9\u6cd5\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u6d89\u53ca\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b\u7684\u4ea4\u9519\u4f7f\u7528\uff0c\u4f46\u901a\u8fc7\u6839\u672c\u4e0d\u540c\u7684\u673a\u5236\uff1a\u7ea7\u8054\u91c7\u7528\u4e00\u79cd\u5ef6\u8fdf\u89c4\u5219\uff0c\u4ec5\u5728\u5904\u7406\u201c\u56f0\u96be\u201d\u8f93\u5165\u65f6\u8c03\u7528\u8f83\u5927\u7684\u6a21\u578b\uff0c\u800c\u63a8\u6d4b\u6027\u89e3\u7801\u5219\u4f7f\u7528\u63a8\u6d4b\u6267\u884c\uff0c\u4e3b\u8981\u5728\u5e76\u884c\u9a8c\u8bc1\u6a21\u5f0f\u4e0b\u8c03\u7528\u8f83\u5927\u7684\u6a21\u578b\u3002\u8fd9\u4e9b\u673a\u5236\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u4f18\u52bf\uff1a\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u7ea7\u8054\u5f80\u5f80\u80fd\u591f\u4ea7\u751f\u6bd4\u8f83\u5927\u6a21\u578b\u66f4\u597d\u7684\u8d28\u91cf\uff0c\u800c\u4ece\u7406\u8bba\u4e0a\u8bb2\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u63d0\u4f9b\u4e86\u8d28\u91cf\u4e2d\u7acb\u6027\u7684\u4fdd\u8bc1\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u8bbe\u8ba1\u65b0\u7684\u63a8\u6d4b\u6027\u7ea7\u8054\u6280\u672f\uff0c\u5229\u7528\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u8fd9\u4e9b\u6280\u672f\u901a\u8fc7\u63a8\u6d4b\u6267\u884c\u6765\u5b9e\u73b0\u5176\u5ef6\u8fdf\u89c4\u5219\u3002\u6211\u4eec\u63cf\u8ff0\u4e86\u63a8\u6d4b\u6027\u7ea7\u8054\u7684\u6700\u4f18\u5ef6\u8fdf\u89c4\u5219\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u5bf9\u6700\u4f18\u89c4\u5219\u7684\u63d2\u4ef6\u8fd1\u4f3c\u3002\u901a\u8fc7\u5728\u57fa\u51c6\u8bed\u8a00\u4efb\u52a1\u4e0a\u4f7f\u7528T5\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6210\u672c-\u8d28\u91cf\u6743\u8861\u65b9\u9762\u4f18\u4e8e\u7ea7\u8054\u548c\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002",
        "tldr_en": "This paper introduces speculative cascading, a novel technique that combines the benefits of cascades and speculative decoding by using speculative execution for deferral rules, achieving superior cost-quality trade-offs in language model inference.",
        "tldr_zh": "\u672c\u6587\u901a\u8fc7\u7ed3\u5408\u7ea7\u8054\u548c\u63a8\u6d4b\u89e3\u7801\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u63a8\u6d4b\u7ea7\u8054\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u901a\u8fc7\u63a8\u6d4b\u6267\u884c\u7684\u5ef6\u8fdf\u89c4\u5219\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u7ea7\u8054\u548c\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6210\u672c-\u8d28\u91cf\u6743\u8861\u3002"
    },
    {
        "title": "Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference",
        "summary": "The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.",
        "authors": "Hao Mark Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko, Stylianos I. Venieris, Hongxiang Fan",
        "published": "2024-05-28",
        "link": "http://arxiv.org/abs/2405.18628v2",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u56de\u5f52\u89e3\u7801\u5728\u5176\u786c\u4ef6\u6027\u80fd\u4e0a\u5bfc\u81f4\u4e86\u663e\u8457\u7684\u5f00\u9500\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u79cd\u7528\u4e8e\u591a\u6807\u8bb0\u751f\u6210\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u6280\u672f\uff0c\u4f46\u8fd9\u4e9b\u52aa\u529b\u4e3b\u8981\u96c6\u4e2d\u5728\u63d0\u9ad8\u5904\u7406\u901f\u5ea6\u5982\u541e\u5410\u91cf\u4e0a\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u5b83\u4eec\u5f80\u5f80\u5ffd\u89c6\u4e86\u5176\u4ed6\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u7684\u6307\u6807\uff0c\u5982\u5185\u5b58\u6d88\u8017\u548c\u8bad\u7ec3\u6210\u672c\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e76\u884c\u63d0\u793a\u89e3\u7801\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700$0.0002\\%$\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u4f7f\u5f97\u5728\u5355\u4e2aA100-40GB GPU\u4e0a\u4ec5\u970016\u5c0f\u65f6\u5373\u53ef\u5b8c\u6210\u9ad8\u6548\u8bad\u7ec3\u3002\u53d7\u4eba\u7c7b\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8fc7\u7a0b\u7684\u542f\u53d1\uff0c$PPD$\u901a\u8fc7\u4f7f\u7528\u591a\u4e2a\u63d0\u793a\u6807\u8bb0\u5e76\u884c\u5730\u8fd1\u4f3c\u751f\u6210\u672a\u6765\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u3002\u8fd9\u79cd\u65b9\u6cd5\u90e8\u5206\u6062\u590d\u4e86\u591a\u6807\u8bb0\u751f\u6210\u6240\u9700\u7684\u6761\u4ef6\u4f9d\u8d56\u4fe1\u606f\uff0c\u4ece\u800c\u4f7f\u957f\u8ddd\u79bb\u9884\u6d4b\u7684\u63a5\u53d7\u7387\u63d0\u9ad8\u4e86\u591a\u8fbe28%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684\u52a8\u6001\u7a00\u758f\u6811\u6280\u672f\uff0c\u8be5\u6280\u672f\u81ea\u9002\u5e94\u5730\u4f18\u5316\u6b64\u89e3\u7801\u65b9\u6848\uff0c\u4ee5\u5145\u5206\u5229\u7528\u4e0d\u540cGPU\u7684\u8ba1\u7b97\u80fd\u529b\u3002\u901a\u8fc7\u5728\u4eceMobileLlama\u5230Vicuna-13B\u7684\u4e00\u7cfb\u5217LLM\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u9ad8\u8fbe2.49$\\times$\u7684\u52a0\u901f\uff0c\u5e76\u4fdd\u6301\u4e86\u4ec5$0.0004\\%$\u7684\u6781\u4f4e\u8fd0\u884c\u65f6\u5185\u5b58\u5f00\u9500\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u5e76\u884c\u63d0\u793a\u89e3\u7801\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6b63\u4ea4\u4f18\u5316\u65b9\u6cd5\uff0c\u4e0e\u73b0\u6709\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u534f\u540c\u96c6\u6210\uff0c\u8fdb\u4e00\u6b65\u663e\u793a\u51fa\u9ad8\u8fbe1.22$\\times$\u7684\u901f\u5ea6\u63d0\u5347\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/hmarkc/parallel-prompt-decoding\u83b7\u53d6\u3002",
        "tldr_en": "We introduce a novel parallel prompt decoding (PPD) technique for Large Language Models (LLMs) that significantly reduces training costs and memory overhead, achieving up to 2.49\u00d7 speedup and 28% higher acceptance rates for long-range predictions, with potential for further enhancements when combined with existing speculative decoding methods.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u97000.0002%\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u65b0\u578b\u5e76\u884c\u63d0\u793a\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u786c\u4ef6\u6027\u80fd\u4e0a\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u5185\u5b58\u6d88\u8017\u548c\u9ad8\u63a5\u53d7\u7387\u3002"
    },
    {
        "title": "Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation",
        "summary": "The rapid development of large language models (LLMs) has led to significant advancements in code completion tasks. While larger models have higher accuracy, they also cost much more to run. Meanwhile, model cascading has been proven effective to conserve computational resources while enhancing accuracy in LLMs on natural language generation tasks. It generates output with the smallest model in a set, and only queries the larger models when it fails to meet predefined quality criteria. However, this strategy has not been used in code completion tasks, primarily because assessing the quality of code completions differs substantially from assessing natural language, where the former relies heavily on the functional correctness. To address this, we propose letting each model generate and execute a set of test cases for their solutions, and use the test results as the cascading threshold. We show that our model cascading strategy reduces computational costs while increases accuracy compared to generating the output with a single model. We also introduce a heuristics to determine the optimal combination of the number of solutions, test cases, and test lines each model should generate, based on the budget. Compared to speculative decoding, our method works on black-box models, having the same level of cost-accuracy trade-off, yet providing much more choices based on the server's budget. Ours is the first work to optimize cost-accuracy trade-off for LLM code generation with model cascading.",
        "authors": "Boyuan Chen, Mingzhi Zhu, Brendan Dolan-Gavitt, Muhammad Shafique, Siddharth Garg",
        "published": "2024-05-24",
        "link": "http://arxiv.org/abs/2405.15842v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5728\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u867d\u7136\u8f83\u5927\u7684\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u4eec\u7684\u8fd0\u884c\u6210\u672c\u4e5f\u66f4\u9ad8\u3002\u540c\u65f6\uff0c\u6a21\u578b\u7ea7\u8054\u5df2\u88ab\u8bc1\u660e\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e2d\u80fd\u591f\u6709\u6548\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u5b83\u4f7f\u7528\u6a21\u578b\u96c6\u4e2d\u6700\u5c0f\u7684\u6a21\u578b\u751f\u6210\u8f93\u51fa\uff0c\u4ec5\u5728\u672a\u8fbe\u5230\u9884\u5b9a\u4e49\u8d28\u91cf\u6807\u51c6\u65f6\u624d\u67e5\u8be2\u66f4\u5927\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7b56\u7565\u5c1a\u672a\u5e94\u7528\u4e8e\u4ee3\u7801\u8865\u5168\u4efb\u52a1\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bc4\u4f30\u4ee3\u7801\u8865\u5168\u8d28\u91cf\u4e0e\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u5927\u76f8\u5f84\u5ead\uff0c\u524d\u8005\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u529f\u80fd\u6b63\u786e\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u8ba9\u6bcf\u4e2a\u6a21\u578b\u4e3a\u5176\u89e3\u51b3\u65b9\u6848\u751f\u6210\u5e76\u6267\u884c\u4e00\u7ec4\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u5c06\u6d4b\u8bd5\u7ed3\u679c\u4f5c\u4e3a\u7ea7\u8054\u9608\u503c\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6a21\u578b\u7ea7\u8054\u7b56\u7565\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u76f8\u6bd4\u4e8e\u4ec5\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u751f\u6210\u8f93\u51fa\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6839\u636e\u9884\u7b97\u786e\u5b9a\u6bcf\u4e2a\u6a21\u578b\u5e94\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u6570\u91cf\u3001\u6d4b\u8bd5\u7528\u4f8b\u6570\u91cf\u548c\u6d4b\u8bd5\u884c\u6570\u7684\u6700\u4f73\u7ec4\u5408\u3002\u4e0e\u63a8\u6d4b\u6027\u89e3\u7801\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u9ed1\u76d2\u6a21\u578b\uff0c\u5177\u6709\u76f8\u540c\u7684\u6210\u672c-\u51c6\u786e\u6027\u6743\u8861\u6c34\u5e73\uff0c\u4f46\u6839\u636e\u670d\u52a1\u5668\u9884\u7b97\u63d0\u4f9b\u4e86\u66f4\u591a\u7684\u9009\u62e9\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u662f\u9996\u4e2a\u901a\u8fc7\u6a21\u578b\u7ea7\u8054\u4f18\u5316LLM\u4ee3\u7801\u751f\u6210\u6210\u672c-\u51c6\u786e\u6027\u6743\u8861\u7684\u7814\u7a76\u3002",
        "tldr_en": "We propose a novel model cascading strategy for code completion that reduces computational costs and increases accuracy by using test case execution results as cascading thresholds, optimizing the cost-accuracy trade-off for large language models in code generation.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u7ea7\u8054\u7b56\u7565\uff0c\u901a\u8fc7\u8ba9\u6bcf\u4e2a\u6a21\u578b\u751f\u6210\u5e76\u6267\u884c\u6d4b\u8bd5\u7528\u4f8b\u6765\u8bc4\u4f30\u4ee3\u7801\u5b8c\u6210\u8d28\u91cf\uff0c\u4ece\u800c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f18\u5316\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6210\u672c-\u51c6\u786e\u6027\u6743\u8861\u3002"
    },
    {
        "title": "A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models",
        "summary": "Despite the crucial importance of accelerating text generation in large language models (LLMs) for efficiently producing content, the sequential nature of this process often leads to high inference latency, posing challenges for real-time applications. Various techniques have been proposed and developed to address these challenges and improve efficiency. This paper presents a comprehensive survey of accelerated generation techniques in autoregressive language models, aiming to understand the state-of-the-art methods and their applications. We categorize these techniques into several key areas: speculative decoding, early exiting mechanisms, and non-autoregressive methods. We discuss each category's underlying principles, advantages, limitations, and recent advancements. Through this survey, we aim to offer insights into the current landscape of techniques in LLMs and provide guidance for future research directions in this critical area of natural language processing.",
        "authors": "Mahsa Khoshnoodi, Vinija Jain, Mingye Gao, Malavika Srikanth, Aman Chadha",
        "published": "2024-05-15",
        "link": "http://arxiv.org/abs/2405.13019v2",
        "chinese_summary": "\u5c3d\u7ba1\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u52a0\u901f\u6587\u672c\u751f\u6210\u5bf9\u4e8e\u9ad8\u6548\u5185\u5bb9\u4ea7\u51fa\u5177\u6709\u81f3\u5173\u91cd\u8981\u7684\u610f\u4e49\uff0c\u4f46\u8fd9\u4e00\u8fc7\u7a0b\u7684\u987a\u5e8f\u6027\u5f80\u5f80\u5bfc\u81f4\u9ad8\u63a8\u7406\u5ef6\u8fdf\uff0c\u7ed9\u5b9e\u65f6\u5e94\u7528\u5e26\u6765\u6311\u6218\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u5e76\u63d0\u5347\u6548\u7387\uff0c\u591a\u79cd\u6280\u672f\u5df2\u88ab\u63d0\u51fa\u5e76\u53d1\u5c55\u3002\u672c\u6587\u5bf9\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u52a0\u901f\u751f\u6210\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u65e8\u5728\u7406\u89e3\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u53ca\u5176\u5e94\u7528\u3002\u6211\u4eec\u5c06\u8fd9\u4e9b\u6280\u672f\u5f52\u7c7b\u4e3a\u51e0\u4e2a\u5173\u952e\u9886\u57df\uff1a\u63a8\u6d4b\u6027\u89e3\u7801\u3001\u65e9\u671f\u9000\u51fa\u673a\u5236\u548c\u975e\u81ea\u56de\u5f52\u65b9\u6cd5\u3002\u6211\u4eec\u63a2\u8ba8\u4e86\u6bcf\u4e00\u7c7b\u6280\u672f\u7684\u57fa\u672c\u539f\u7406\u3001\u4f18\u52bf\u3001\u5c40\u9650\u6027\u53ca\u6700\u65b0\u8fdb\u5c55\u3002\u901a\u8fc7\u6b64\u6b21\u7efc\u8ff0\uff0c\u6211\u4eec\u65e8\u5728\u6d1e\u5bdfLLMs\u4e2d\u6280\u672f\u7684\u5f53\u524d\u683c\u5c40\uff0c\u5e76\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8fd9\u4e00\u5173\u952e\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u6307\u5bfc\u3002",
        "tldr_en": "This paper surveys accelerated text generation techniques in autoregressive language models, categorizing them into speculative decoding, early exiting, and non-autoregressive methods to guide future NLP research.",
        "tldr_zh": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e2d\u52a0\u901f\u6587\u672c\u751f\u6210\u6280\u672f\uff0c\u6db5\u76d6\u63a8\u6d4b\u89e3\u7801\u3001\u65e9\u671f\u9000\u51fa\u673a\u5236\u548c\u975e\u81ea\u56de\u5f52\u65b9\u6cd5\uff0c\u65e8\u5728\u63a2\u8ba8\u6700\u65b0\u6280\u672f\u53ca\u5176\u5e94\u7528\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002"
    },
    {
        "title": "EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models",
        "summary": "Speculative decoding emerges as a pivotal technique for enhancing the inference speed of Large Language Models (LLMs). Despite recent research aiming to improve prediction efficiency, multi-sample speculative decoding has been overlooked due to varying numbers of accepted tokens within a batch in the verification phase. Vanilla method adds padding tokens in order to ensure that the number of new tokens remains consistent across samples. However, this increases the computational and memory access overhead, thereby reducing the speedup ratio. We propose a novel method that can resolve the issue of inconsistent tokens accepted by different samples without necessitating an increase in memory or computing overhead. Furthermore, our proposed method can handle the situation where the prediction tokens of different samples are inconsistent without the need to add padding tokens. Sufficient experiments demonstrate the efficacy of our method. Our code is available at https://github.com/niyunsheng/EMS-SD.",
        "authors": "Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, Yunhe Wang",
        "published": "2024-05-13",
        "link": "http://arxiv.org/abs/2405.07542v1",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u4f5c\u4e3a\u4e00\u79cd\u5173\u952e\u6280\u672f\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u901f\u5ea6\u3002\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u81f4\u529b\u4e8e\u63d0\u9ad8\u9884\u6d4b\u6548\u7387\uff0c\u4f46\u7531\u4e8e\u9a8c\u8bc1\u9636\u6bb5\u5185\u6279\u6b21\u4e2d\u5404\u6837\u672c\u63a5\u53d7\u4ee4\u724c\u6570\u91cf\u4e0d\u4e00\uff0c\u591a\u6837\u672c\u63a8\u6d4b\u6027\u89e3\u7801\u4e00\u76f4\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u6dfb\u52a0\u586b\u5145\u4ee4\u724c\u6765\u786e\u4fdd\u5404\u6837\u672c\u65b0\u751f\u6210\u4ee4\u724c\u6570\u91cf\u4e00\u81f4\uff0c\u4f46\u8fd9\u589e\u52a0\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u52a0\u901f\u6bd4\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u5185\u5b58\u6216\u8ba1\u7b97\u5f00\u9500\u7684\u524d\u63d0\u4e0b\uff0c\u89e3\u51b3\u4e0d\u540c\u6837\u672c\u63a5\u53d7\u4ee4\u724c\u6570\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u80fd\u5904\u7406\u4e0d\u540c\u6837\u672c\u9884\u6d4b\u4ee4\u724c\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\uff0c\u65e0\u9700\u6dfb\u52a0\u586b\u5145\u4ee4\u724c\u3002\u5145\u5206\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u8be6\u89c1https://github.com/niyunsheng/EMS-SD\u3002",
        "tldr_en": "We introduce a novel speculative decoding method that eliminates padding tokens and maintains consistent speedup without increasing memory or computational overhead.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u589e\u52a0\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5373\u53ef\u89e3\u51b3\u591a\u6837\u672c\u63a8\u6d4b\u89e3\u7801\u4e2d\u63a5\u53d7\u4ee4\u724c\u6570\u91cf\u4e0d\u4e00\u81f4\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"
    },
    {
        "title": "Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models",
        "summary": "Speculative decoding is commonly used for reducing the inference latency of large language models. Its effectiveness depends highly on the speculation lookahead (SL)-the number of tokens generated by the draft model at each iteration. In this work we show that the common practice of using the same SL for all iterations (static SL) is suboptimal. We introduce DISCO (DynamIc SpeCulation lookahead Optimization), a novel method for dynamically selecting the SL. Our experiments with four datasets show that DISCO reaches an average speedup of 10% compared to the best static SL baseline, while generating the exact same text.",
        "authors": "Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy Schwartz",
        "published": "2024-05-07",
        "link": "http://arxiv.org/abs/2405.04304v4",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u5e38\u7528\u4e8e\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u5176\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u63a8\u6d4b\u6027\u524d\u77bb\uff08Speculation Lookahead, SL\uff09\u2014\u2014\u5373\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u8349\u7a3f\u6a21\u578b\u751f\u6210\u7684\u6807\u8bb0\u6570\u91cf\u3002\u672c\u6587\u4e2d\u6211\u4eec\u6307\u51fa\uff0c\u5728\u6240\u6709\u8fed\u4ee3\u4e2d\u4f7f\u7528\u76f8\u540c\u7684SL\uff08\u9759\u6001SL\uff09\u7684\u505a\u6cd5\u5e76\u975e\u6700\u4f18\u3002\u6211\u4eec\u63d0\u51fa\u4e86DISCO\uff08\u52a8\u6001\u63a8\u6d4b\u6027\u524d\u77bb\u4f18\u5316\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u9009\u62e9SL\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5bf9\u56db\u4e2a\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0DISCO\u76f8\u8f83\u4e8e\u6700\u4f73\u7684\u9759\u6001SL\u57fa\u7ebf\uff0c\u5e73\u5747\u52a0\u901f\u8fbe\u5230\u4e8610%\uff0c\u540c\u65f6\u751f\u6210\u4e86\u5b8c\u5168\u76f8\u540c\u7684\u6587\u672c\u3002",
        "tldr_en": "DISCO dynamically optimizes speculation lookahead (SL) for reducing inference latency, achieving a 10% average speedup over static SL baselines with identical text output.",
        "tldr_zh": "\u52a8\u6001\u63a8\u6d4b\u524d\u77bb\u4f18\u5316\uff08DISCO\uff09\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u63a8\u6d4b\u524d\u77bb\u503c\uff0c\u76f8\u6bd4\u9759\u6001\u524d\u77bb\u65b9\u6cd5\u5e73\u5747\u63d0\u901f10%\uff0c\u4e14\u751f\u6210\u76f8\u540c\u6587\u672c\u3002"
    },
    {
        "title": "Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge",
        "summary": "Large language models (LLMs) suffer from low efficiency as the mismatch between the requirement of auto-regressive decoding and the design of most contemporary GPUs. Specifically, billions to trillions of parameters must be loaded to the GPU cache through its limited memory bandwidth for computation, but only a small batch of tokens is actually computed. Consequently, the GPU spends most of its time on memory transfer instead of computation. Recently, parallel decoding, a type of speculative decoding algorithms, is becoming more popular and has demonstrated impressive efficiency improvement in generation. It introduces extra decoding heads to large models, enabling them to predict multiple subsequent tokens simultaneously and verify these candidate continuations in a single decoding step. However, this approach deviates from the training objective of next token prediction used during pre-training, resulting in a low hit rate for candidate tokens. In this paper, we propose a new speculative decoding algorithm, Clover, which integrates sequential knowledge into the parallel decoding process. This enhancement improves the hit rate of speculators and thus boosts the overall efficiency. Clover transmits the sequential knowledge from pre-speculated tokens via the Regressive Connection, then employs an Attention Decoder to integrate these speculated tokens. Additionally, Clover incorporates an Augmenting Block that modifies the hidden states to better align with the purpose of speculative generation rather than next token prediction. The experiment results demonstrate that Clover outperforms the baseline by up to 91% on Baichuan-Small and 146% on Baichuan-Large, respectively, and exceeds the performance of the previously top-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on Baichuan-Large, respectively.",
        "authors": "Bin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin Cui",
        "published": "2024-05-01",
        "link": "http://arxiv.org/abs/2405.00263v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u662f\u7531\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\u7684\u9700\u6c42\u4e0e\u5927\u591a\u6570\u5f53\u4ee3GPU\u8bbe\u8ba1\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u6240\u5bfc\u81f4\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6570\u5341\u4ebf\u5230\u6570\u4e07\u4ebf\u7684\u53c2\u6570\u5fc5\u987b\u901a\u8fc7\u6709\u9650\u7684\u5185\u5b58\u5e26\u5bbd\u52a0\u8f7d\u5230GPU\u7f13\u5b58\u4e2d\u8fdb\u884c\u8ba1\u7b97\uff0c\u4f46\u5b9e\u9645\u4e0a\u53ea\u6709\u4e00\u5c0f\u6279\u6b21\u7684\u4ee4\u724c\u88ab\u5b9e\u9645\u8ba1\u7b97\u3002\u56e0\u6b64\uff0cGPU\u5927\u90e8\u5206\u65f6\u95f4\u90fd\u82b1\u5728\u5185\u5b58\u4f20\u8f93\u4e0a\uff0c\u800c\u4e0d\u662f\u8ba1\u7b97\u4e0a\u3002\u6700\u8fd1\uff0c\u4e00\u79cd\u540d\u4e3a\u5e76\u884c\u89e3\u7801\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u7b97\u6cd5\u9010\u6e10\u6d41\u884c\u8d77\u6765\uff0c\u5e76\u5728\u751f\u6210\u6548\u7387\u4e0a\u5c55\u793a\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u5b83\u4e3a\u5927\u578b\u6a21\u578b\u5f15\u5165\u4e86\u989d\u5916\u7684\u89e3\u7801\u5934\uff0c\u4f7f\u5176\u80fd\u591f\u540c\u65f6\u9884\u6d4b\u591a\u4e2a\u540e\u7eed\u4ee4\u724c\uff0c\u5e76\u5728\u5355\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\u9a8c\u8bc1\u8fd9\u4e9b\u5019\u9009\u7684\u5ef6\u7eed\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u504f\u79bb\u4e86\u9884\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u5bfc\u81f4\u5019\u9009\u4ee4\u724c\u7684\u547d\u4e2d\u7387\u8f83\u4f4e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u7b97\u6cd5\uff0c\u540d\u4e3aClover\uff0c\u5b83\u5c06\u987a\u5e8f\u77e5\u8bc6\u6574\u5408\u5230\u5e76\u884c\u89e3\u7801\u8fc7\u7a0b\u4e2d\u3002\u8fd9\u4e00\u6539\u8fdb\u63d0\u9ad8\u4e86\u63a8\u6d4b\u5668\u7684\u547d\u4e2d\u7387\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6574\u4f53\u6548\u7387\u3002Clover\u901a\u8fc7\u56de\u5f52\u8fde\u63a5\u5c06\u987a\u5e8f\u77e5\u8bc6\u4ece\u9884\u63a8\u6d4b\u7684\u4ee4\u724c\u4f20\u9012\uff0c\u7136\u540e\u4f7f\u7528\u6ce8\u610f\u529b\u89e3\u7801\u5668\u6574\u5408\u8fd9\u4e9b\u63a8\u6d4b\u7684\u4ee4\u724c\u3002\u6b64\u5916\uff0cClover\u8fd8\u5305\u542b\u4e00\u4e2a\u589e\u5f3a\u5757\uff0c\u7528\u4e8e\u4fee\u6539\u9690\u85cf\u72b6\u6001\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u7b26\u5408\u63a8\u6d4b\u751f\u6210\u7684\u76ee\u7684\uff0c\u800c\u4e0d\u662f\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cClover\u5728Baichuan-Small\u4e0a\u6bd4\u57fa\u7ebf\u9ad8\u51fa\u6700\u591a91%\uff0c\u5728Baichuan-Large\u4e0a\u9ad8\u51fa146%\uff0c\u5e76\u4e14\u5728Baichuan-Small\u4e0a\u6bd4\u4e4b\u524d\u8868\u73b0\u6700\u597d\u7684\u65b9\u6cd5Medusa\u9ad8\u51fa\u6700\u591a37%\uff0c\u5728Baichuan-Large\u4e0a\u9ad8\u51fa57%\u3002",
        "tldr_en": "Clover, a new speculative decoding algorithm, integrates sequential knowledge to enhance parallel decoding efficiency, outperforming existing methods on Baichuan models.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aClover\u7684\u65b0\u578b\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u987a\u5e8f\u77e5\u8bc6\u5230\u5e76\u884c\u89e3\u7801\u8fc7\u7a0b\u4e2d\uff0c\u63d0\u9ad8\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u547d\u4e2d\u7387\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"
    },
    {
        "title": "Accelerating Production LLMs with Combined Token/Embedding Speculators",
        "summary": "This technical report describes the design and training of novel speculative decoding draft models, for accelerating the inference speeds of large language models in a production environment. By conditioning draft predictions on both context vectors and sampled tokens, we can train our speculators to efficiently predict high-quality n-grams, which the base model then accepts or rejects. This allows us to effectively predict multiple tokens per inference forward pass, accelerating wall-clock inference speeds of highly optimized base model implementations by a factor of 2-3x. We explore these initial results and describe next steps for further improvements.",
        "authors": "Davis Wertheimer, Joshua Rosenkranz, Thomas Parnell, Sahil Suneja, Pavithra Ranganathan, Raghu Ganti, Mudhakar Srivatsa",
        "published": "2024-04-29",
        "link": "http://arxiv.org/abs/2404.19124v2",
        "chinese_summary": "\u672c\u6280\u672f\u62a5\u544a\u63cf\u8ff0\u4e86\u65b0\u578b\u63a8\u6d4b\u89e3\u7801\u8349\u7a3f\u6a21\u578b\u7684\u8bbe\u8ba1\u548c\u8bad\u7ec3\uff0c\u65e8\u5728\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u3002\u901a\u8fc7\u5c06\u8349\u7a3f\u9884\u6d4b\u6761\u4ef6\u8bbe\u7f6e\u4e3a\u4e0a\u4e0b\u6587\u5411\u91cf\u548c\u91c7\u6837\u4ee4\u724c\uff0c\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u63a8\u6d4b\u5668\u9ad8\u6548\u9884\u6d4b\u9ad8\u8d28\u91cf\u7684n-gram\uff0c\u7136\u540e\u7531\u57fa\u7840\u6a21\u578b\u63a5\u53d7\u6216\u62d2\u7edd\u3002\u8fd9\u4f7f\u5f97\u6211\u4eec\u80fd\u591f\u5728\u6bcf\u6b21\u63a8\u7406\u524d\u5411\u4f20\u9012\u4e2d\u6709\u6548\u9884\u6d4b\u591a\u4e2a\u4ee4\u724c\uff0c\u5c06\u9ad8\u5ea6\u4f18\u5316\u7684\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u7684\u63a8\u7406\u901f\u5ea6\u52a0\u901f2-3\u500d\u3002\u6211\u4eec\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u521d\u6b65\u7ed3\u679c\uff0c\u5e76\u63cf\u8ff0\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u4e0b\u4e00\u6b65\u63aa\u65bd\u3002",
        "tldr_en": "This report details the design and training of speculative decoding models to accelerate large language model inference by up to 2-3x through efficient n-gram prediction and acceptance/rejection by the base model.",
        "tldr_zh": "\u672c\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u63a8\u6d4b\u89e3\u7801\u8349\u7a3f\u6a21\u578b\u7684\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u5411\u91cf\u548c\u91c7\u6837\u4ee4\u724c\uff0c\u6709\u6548\u9884\u6d4b\u9ad8\u8d28\u91cfn-gram\uff0c\u4f7f\u57fa\u7840\u6a21\u578b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u63a8\u7406\u901f\u5ea6\u63d0\u53472-3\u500d\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u65b9\u5411\u3002"
    },
    {
        "title": "Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting",
        "summary": "Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution. However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly. Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model. We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability. It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model. To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens. Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold. Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo. Under single-sequence verification, Kangaroo achieves speedups up to $1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional parameters (67M compared to 591M). The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.",
        "authors": "Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, Yunhe Wang",
        "published": "2024-04-29",
        "link": "http://arxiv.org/abs/2404.18911v1",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u5728\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5b9a\u7684\u91c7\u6837\u5206\u5e03\u3002\u7136\u800c\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u72ec\u7acb\u7684\u8349\u7a3f\u6a21\u578b\u6765\u8fbe\u5230\u4ee4\u4eba\u6ee1\u610f\u7684\u4ee4\u724c\u63a5\u53d7\u7387\u53ef\u80fd\u4f1a\u5e26\u6765\u9ad8\u6602\u7684\u6210\u672c\u3002\u53d7\u65e9\u671f\u9000\u51fa\u673a\u5236\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u63a8\u6d4b\u6027\u89e3\u7801\u6846\u67b6\u2014\u2014**\u888b\u9f20\uff08Kangaroo\uff09**\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e00\u4e2a\u56fa\u5b9a\u7684\u6d45\u5c42\u5b50\u7f51\u7edc\u4f5c\u4e3a\u81ea\u8349\u7a3f\u6a21\u578b\uff0c\u5269\u4f59\u7684\u5c42\u5219\u4f5c\u4e3a\u66f4\u5927\u7684\u76ee\u6807\u6a21\u578b\u3002\u6211\u4eec\u5728\u5b50\u7f51\u7edc\u4e4b\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u9002\u914d\u5668\u6a21\u5757\uff0c\u4ee5\u5f25\u5408\u5b50\u7f51\u7edc\u4e0e\u5b8c\u6574\u6a21\u578b\u8868\u793a\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u81ea\u8349\u7a3f\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u76f8\u5bf9\u4e8e\u5927\u578b\u6a21\u578b\u53ef\u80fd\u4e0d\u518d\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff0c\u56e0\u6b64\u9700\u8981\u5728\u6700\u5c0f\u5316\u5c0f\u578b\u6a21\u578b\u8349\u7a3f\u6b65\u9aa4\u7684\u540c\u65f6\u63d0\u9ad8\u4ee4\u724c\u63a5\u53d7\u7387\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u989d\u5916\u7684\u65e9\u671f\u9000\u51fa\u673a\u5236\u6765\u751f\u6210\u8349\u7a3f\u4ee4\u724c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e00\u65e6\u5f53\u524d\u4ee4\u724c\u7684\u7f6e\u4fe1\u5ea6\u4f4e\u4e8e\u67d0\u4e2a\u9608\u503c\uff0c\u6211\u4eec\u5728\u8349\u7a3f\u9636\u6bb5\u5c31\u4f1a\u505c\u6b62\u5c0f\u578b\u6a21\u578b\u7684\u540e\u7eed\u9884\u6d4b\u3002\u5728Spec-Bench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u888b\u9f20\u7684\u6709\u6548\u6027\u3002\u5728\u5355\u5e8f\u5217\u9a8c\u8bc1\u4e0b\uff0c\u888b\u9f20\u5728Spec-Bench\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.68\u500d\u7684\u52a0\u901f\uff0c\u8d85\u8fc7\u4e86Medusa-1\uff0c\u51cf\u5c11\u4e8688.7%\u7684\u989d\u5916\u53c2\u6570\uff0867M\u5bf9\u6bd4591M\uff09\u3002\u888b\u9f20\u7684\u4ee3\u7801\u53ef\u5728https://github.com/Equationliu/Kangaroo\u83b7\u53d6\u3002",
        "tldr_en": "We propose Kangaroo, a self-speculative decoding framework using a fixed shallow sub-network and an adapter module to accelerate large language model inference, achieving up to 1.68x speedup with 88.7% fewer parameters compared to Medusa-1.",
        "tldr_zh": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKangaroo\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u56fa\u5b9a\u6d45\u5c42\u5b50\u7f51\u7edc\u4f5c\u4e3a\u81ea\u8349\u7a3f\u6a21\u578b\uff0c\u7ed3\u5408\u8f7b\u91cf\u9002\u914d\u5668\u6a21\u5757\u548c\u65e9\u671f\u9000\u51fa\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901f\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u5728Spec-Bench\u4e0a\u6700\u9ad8\u53ef\u5b9e\u73b01.68\u500d\u52a0\u901f\uff0c\u4e14\u53c2\u6570\u6548\u7387\u4f18\u4e8eMedusa-1\u3002"
    },
    {
        "title": "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding",
        "summary": "We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.",
        "authors": "Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu",
        "published": "2024-04-25",
        "link": "http://arxiv.org/abs/2404.16710v2",
        "chinese_summary": "\u6211\u4eec\u63d0\u51fa\u4e86LayerSkip\uff0c\u8fd9\u662f\u4e00\u79cd\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002\u9996\u5148\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u5c42\u7ea7dropout\uff0c\u65e9\u671f\u5c42\u7684dropout\u7387\u8f83\u4f4e\uff0c\u800c\u540e\u671f\u5c42\u7684dropout\u7387\u8f83\u9ad8\uff0c\u5e76\u4e14\u6240\u6709transformer\u5c42\u5171\u4eab\u76f8\u540c\u7684\u63d0\u524d\u9000\u51fa\u635f\u5931\u3002\u5176\u6b21\uff0c\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u8fd9\u79cd\u8bad\u7ec3\u65b9\u6cd5\u63d0\u9ad8\u4e86\u65e9\u671f\u5c42\u63d0\u524d\u9000\u51fa\u7684\u51c6\u786e\u6027\uff0c\u800c\u65e0\u9700\u5411\u6a21\u578b\u4e2d\u6dfb\u52a0\u4efb\u4f55\u8f85\u52a9\u5c42\u6216\u6a21\u5757\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\uff0c\u5176\u4e2d\u6211\u4eec\u5728\u65e9\u671f\u5c42\u9000\u51fa\uff0c\u5e76\u5229\u7528\u6a21\u578b\u7684\u5269\u4f59\u5c42\u8fdb\u884c\u9a8c\u8bc1\u548c\u4fee\u6b63\u3002\u6211\u4eec\u63d0\u51fa\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u76f8\u6bd4\u5176\u4ed6\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5177\u6709\u66f4\u5c0f\u7684\u5185\u5b58\u5360\u7528\uff0c\u5e76\u53d7\u76ca\u4e8e\u8349\u7a3f\u548c\u9a8c\u8bc1\u9636\u6bb5\u7684\u5171\u4eab\u8ba1\u7b97\u548c\u6fc0\u6d3b\u3002\u6211\u4eec\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u8bad\u7ec3\u4e0a\u5bf9\u4e0d\u540c\u5927\u5c0f\u7684Llama\u6a21\u578b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff1a\u4ece\u5934\u5f00\u59cb\u7684\u9884\u8bad\u7ec3\u3001\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u7279\u5b9a\u6570\u636e\u57df\u7684\u5fae\u8c03\u4ee5\u53ca\u7279\u5b9a\u4efb\u52a1\u7684\u5fae\u8c03\u3002\u6211\u4eec\u5b9e\u73b0\u4e86\u6211\u4eec\u7684\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728CNN/DM\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e0a\u6700\u9ad82.16\u500d\u7684\u52a0\u901f\uff0c\u5728\u7f16\u7801\u4efb\u52a1\u4e0a1.82\u500d\u7684\u52a0\u901f\uff0c\u4ee5\u53ca\u5728TOPv2\u8bed\u4e49\u89e3\u6790\u4efb\u52a1\u4e0a2.0\u500d\u7684\u52a0\u901f\u3002",
        "tldr_en": "LayerSkip accelerates large language model inference via layer dropout training, self-speculative decoding, and shared compute, achieving up to 2.16x speedup across tasks.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86LayerSkip\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bad\u7ec3\u65f6\u7684\u5c42\u4e22\u5f03\u548c\u65e9\u671f\u9000\u51fa\u635f\u5931\uff0c\u4ee5\u53ca\u63a8\u7406\u65f6\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u589e\u52a0\u989d\u5916\u5c42\u6216\u6a21\u5757\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u65e9\u671f\u9000\u51fa\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002"
    },
    {
        "title": "BASS: Batched Attention-optimized Speculative Sampling",
        "summary": "Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.",
        "authors": "Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, Anoop Deoras",
        "published": "2024-04-24",
        "link": "http://arxiv.org/abs/2404.15778v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u4f5c\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u5df2\u7ecf\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6258\u7ba1\u4e2d\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u65b9\u9762\u5d2d\u9732\u5934\u89d2\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u5b9e\u73b0\u90fd\u96c6\u4e2d\u5728\u751f\u6210\u5355\u4e2a\u5e8f\u5217\u4e0a\u3002\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u751f\u6210\u5f0fAI\u5e94\u7528\u901a\u5e38\u9700\u8981\u591a\u4e2a\u54cd\u5e94\uff0c\u5982\u4f55\u5728\u6279\u5904\u7406\u73af\u5883\u4e2d\u6267\u884c\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u540c\u65f6\u4fdd\u6301\u5176\u5ef6\u8fdf\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u975e\u540c\u5c0f\u53ef\u7684\u6311\u6218\u3002\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u79cd\u6279\u91cf\u63a8\u6d4b\u6027\u89e3\u7801\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728\u591a\u5e8f\u5217\u751f\u6210\u5ef6\u8fdf\u65b9\u9762\u8fbe\u5230\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\uff0c\u5e76\u5c55\u793a\u4e86\u5353\u8d8a\u7684GPU\u5229\u7528\u7387\u4ee5\u53ca\u5728\u65f6\u95f4\u9884\u7b97\u5185\u7684\u751f\u6210\u8d28\u91cf\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u5355\u4e2aA100 GPU\u4e0a\u76847.8B\u89c4\u6a21\u6a21\u578b\u548c8\u7684\u6279\u91cf\u5927\u5c0f\uff0c\u6bcf\u4e2a\u5e8f\u5217\u7684\u751f\u6210\u901f\u5ea6\u5e73\u5747\u4e3a\u6bcf\u4ee4\u724c5.8\u6beb\u79d2\uff0c\u603b\u4f53\u541e\u5410\u91cf\u4e3a\u6bcf\u79d21.1K\u4ee4\u724c\u3002\u8fd9\u4e9b\u7ed3\u679c\u4ee3\u8868\u4e86\u6700\u5148\u8fdb\u7684\u5ef6\u8fdf\uff0c\u5e76\u4e14\u6bd4\u4f18\u5316\u7684\u5e38\u89c4\u89e3\u7801\u5feb2.15\u500d\u3002\u5728\u5e38\u89c4\u89e3\u7801\u65e0\u6cd5\u5b8c\u6210\u7684\u65f6\u9650\u5185\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u5e8f\u5217\uff0cHumanEval Pass@First\u8fbe\u523043%\uff0cPass@All\u8fbe\u523061%\uff0c\u8fdc\u8fdc\u8d85\u8fc7\u4e86\u5355\u5e8f\u5217\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u53ef\u884c\u6027\u3002\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u7684\u5cf0\u503cGPU\u5229\u7528\u7387\u9ad8\u8fbe15.8%\uff0c\u662f\u5e38\u89c4\u89e3\u7801\u6700\u9ad8\u503c\u76843\u500d\u4ee5\u4e0a\uff0c\u5927\u7ea6\u662f\u5355\u5e8f\u5217\u63a8\u6d4b\u6027\u89e3\u7801\u768410\u500d\u3002",
        "tldr_en": "This paper introduces batched speculative decoding, achieving state-of-the-art multi-sequence generation latency with 2.15X speed-up, superior GPU utilization, and higher quality within a time budget.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6279\u91cf\u63a8\u6d4b\u89e3\u7801\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5e8f\u5217\u751f\u6210\u7684\u5ef6\u8fdf\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.15\u500d\u7684\u52a0\u901f\uff0c\u5e76\u5728GPU\u5229\u7528\u7387\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u5353\u8d8a\u3002"
    },
    {
        "title": "Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models",
        "summary": "With the increasingly giant scales of (causal) large language models (LLMs), the inference efficiency comes as one of the core concerns along the improved performance. In contrast to the memory footprint, the latency bottleneck seems to be of greater importance as there can be billions of requests to a LLM (e.g., GPT-4) per day. The bottleneck is mainly due to the autoregressive innateness of LLMs, where tokens can only be generated sequentially during decoding. To alleviate the bottleneck, the idea of speculative execution, which originates from the field of computer architecture, is introduced to LLM decoding in a \\textit{draft-then-verify} style. Under this regime, a sequence of tokens will be drafted in a fast pace by utilizing some heuristics, and then the tokens shall be verified in parallel by the LLM. As the costly sequential inference is parallelized, LLM decoding speed can be significantly boosted. Driven by the success of LLMs in recent couple of years, a growing literature in this direction has emerged. Yet, there lacks a position survey to summarize the current landscape and draw a roadmap for future development of this promising area. To meet this demand, we present the very first survey paper that reviews and unifies literature of speculative execution in LLMs (e.g., blockwise parallel decoding, speculative decoding, etc.) in a comprehensive framework and a systematic taxonomy. Based on the taxonomy, we present a critical review and comparative analysis of the current arts. Finally we highlight various key challenges and future directions to further develop the area.",
        "authors": "Chen Zhang, Zhuorui Liu, Dawei Song",
        "published": "2024-04-23",
        "link": "http://arxiv.org/abs/2404.14897v1",
        "chinese_summary": "\u968f\u7740\uff08\u56e0\u679c\uff09\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c4\u6a21\u7684\u65e5\u76ca\u5e9e\u5927\uff0c\u63a8\u7406\u6548\u7387\u6210\u4e3a\u4e0e\u6027\u80fd\u63d0\u5347\u5e76\u5217\u7684\u6838\u5fc3\u5173\u6ce8\u70b9\u4e4b\u4e00\u3002\u76f8\u8f83\u4e8e\u5185\u5b58\u5360\u7528\uff0c\u5ef6\u8fdf\u74f6\u9888\u663e\u5f97\u66f4\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u6bcf\u5929\u53ef\u80fd\u6709\u6570\u5341\u4ebf\u6b21\u8bf7\u6c42\u6d8c\u5411\u4e00\u4e2aLLM\uff08\u4f8b\u5982GPT-4\uff09\u3002\u8fd9\u4e00\u74f6\u9888\u4e3b\u8981\u6e90\u4e8eLLMs\u7684\u81ea\u56de\u5f52\u672c\u8d28\uff0c\u5373\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\uff0c\u6807\u8bb0\u53ea\u80fd\u6309\u987a\u5e8f\u751f\u6210\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e00\u74f6\u9888\uff0c\u6e90\u81ea\u8ba1\u7b97\u673a\u4f53\u7cfb\u7ed3\u6784\u7684\u63a8\u6d4b\u6267\u884c\u601d\u60f3\u88ab\u5f15\u5165\u5230LLM\u89e3\u7801\u4e2d\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u201c\u5148\u8349\u62df\u540e\u9a8c\u8bc1\u201d\u7684\u65b9\u5f0f\u3002\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0c\u901a\u8fc7\u5229\u7528\u67d0\u4e9b\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5feb\u901f\u8349\u62df\u51fa\u4e00\u4e32\u6807\u8bb0\uff0c\u7136\u540e\u7531LLM\u5e76\u884c\u9a8c\u8bc1\u8fd9\u4e9b\u6807\u8bb0\u3002\u7531\u4e8e\u6602\u8d35\u7684\u987a\u5e8f\u63a8\u7406\u8fc7\u7a0b\u5f97\u4ee5\u5e76\u884c\u5316\uff0cLLM\u7684\u89e3\u7801\u901f\u5ea6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u3002\u8fd1\u5e74\u6765\uff0c\u968f\u7740LLMs\u7684\u5de8\u5927\u6210\u529f\uff0c\u8fd9\u4e00\u9886\u57df\u7684\u6587\u732e\u6570\u91cf\u4e0d\u65ad\u589e\u957f\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u7f3a\u4e4f\u4e00\u4efd\u7efc\u8ff0\u6765\u603b\u7ed3\u5f53\u524d\u7684\u7814\u7a76\u73b0\u72b6\u5e76\u4e3a\u8fd9\u4e00\u5145\u6ee1\u524d\u666f\u7684\u9886\u57df\u7ed8\u5236\u672a\u6765\u53d1\u5c55\u8def\u7ebf\u56fe\u3002\u4e3a\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\uff0c\u6211\u4eec\u64b0\u5199\u4e86\u9996\u7bc7\u7efc\u8ff0\u8bba\u6587\uff0c\u5168\u9762\u56de\u987e\u5e76\u7edf\u4e00\u4e86LLMs\u4e2d\u63a8\u6d4b\u6267\u884c\u7684\u76f8\u5173\u6587\u732e\uff08\u5982\u5206\u5757\u5e76\u884c\u89e3\u7801\u3001\u63a8\u6d4b\u89e3\u7801\u7b49\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\u548c\u7cfb\u7edf\u5206\u7c7b\u4f53\u7cfb\u3002\u57fa\u4e8e\u8fd9\u4e00\u5206\u7c7b\u4f53\u7cfb\uff0c\u6211\u4eec\u5bf9\u5f53\u524d\u7684\u7814\u7a76\u6210\u679c\u8fdb\u884c\u4e86\u6279\u5224\u6027\u8bc4\u8ff0\u548c\u6bd4\u8f83\u5206\u6790\u3002\u6700\u540e\uff0c\u6211\u4eec\u6307\u51fa\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u5404\u79cd\u5173\u952e\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u3002",
        "tldr_en": "This survey paper provides a comprehensive review and taxonomy of speculative execution techniques in large language models (LLMs) to enhance decoding efficiency, highlighting current challenges and future directions.",
        "tldr_zh": "\u968f\u7740\u56e0\u679c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u63a8\u7406\u6548\u7387\u6210\u4e3a\u6838\u5fc3\u5173\u6ce8\u70b9\u4e4b\u4e00\uff0c\u5176\u4e2d\u5ef6\u8fdf\u74f6\u9888\u5c24\u4e3a\u91cd\u8981\uff0c\u6e90\u4e8eLLMs\u7684\u81ea\u56de\u5f52\u7279\u6027\u3002\u4e3a\u7f13\u89e3\u6b64\u74f6\u9888\uff0c\u5f15\u5165\u8ba1\u7b97\u673a\u67b6\u6784\u9886\u57df\u7684\u63a8\u6d4b\u6267\u884c\u6982\u5ff5\uff0c\u91c7\u7528\u201c\u8349\u62df-\u9a8c\u8bc1\u201d\u65b9\u5f0f\uff0c\u901a\u8fc7\u5e76\u884c\u9a8c\u8bc1\u52a0\u901fLLM\u89e3\u7801\u3002\u8fd1\u5e74\u6765\uff0c\u76f8\u5173\u7814\u7a76\u8fc5\u901f\u589e\u957f\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7efc\u8ff0\u3002\u672c\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0LLMs\u4e2d\u7684\u63a8\u6d4b\u6267\u884c\u6280\u672f\uff0c\u63d0\u51fa\u7cfb\u7edf\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u5f53\u524d\u8fdb\u5c55\u53ca\u672a\u6765\u6311\u6218\u3002"
    },
    {
        "title": "Parallel Decoding via Hidden Transfer for Lossless Large Language Model Acceleration",
        "summary": "Large language models (LLMs) have recently shown remarkable performance across a wide range of tasks. However, the substantial number of parameters in LLMs contributes to significant latency during model inference. This is particularly evident when utilizing autoregressive decoding methods, which generate one token in a single forward process, thereby not fully capitalizing on the parallel computing capabilities of GPUs. In this paper, we propose a novel parallel decoding approach, namely \\textit{hidden transfer}, which decodes multiple successive tokens simultaneously in a single forward pass. The idea is to transfer the intermediate hidden states of the previous context to the \\textit{pseudo} hidden states of the future tokens to be generated, and then the pseudo hidden states will pass the following transformer layers thereby assimilating more semantic information and achieving superior predictive accuracy of the future tokens.   Besides, we use the novel tree attention mechanism to simultaneously generate and verify multiple candidates of output sequences, which ensure the lossless generation and further improves the generation efficiency of our method. Experiments demonstrate the effectiveness of our method. We conduct a lot of analytic experiments to prove our motivation. In terms of acceleration metrics, we outperform all the single-model acceleration techniques, including Medusa and Self-Speculative decoding.",
        "authors": "Pengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, Dongyan Zhao",
        "published": "2024-04-18",
        "link": "http://arxiv.org/abs/2404.12022v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e2d\u8fd1\u671f\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u3002\u7136\u800c\uff0cLLMs\u4e2d\u5927\u91cf\u7684\u53c2\u6570\u5bfc\u81f4\u4e86\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u663e\u8457\u5ef6\u8fdf\u3002\u5728\u4f7f\u7528\u81ea\u56de\u5f52\u89e3\u7801\u65b9\u6cd5\u65f6\uff0c\u8fd9\u4e00\u95ee\u9898\u5c24\u4e3a\u660e\u663e\uff0c\u56e0\u4e3a\u81ea\u56de\u5f52\u89e3\u7801\u6bcf\u6b21\u524d\u5411\u8fc7\u7a0b\u53ea\u751f\u6210\u4e00\u4e2atoken\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528GPU\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u5373\\textit{\u9690\u85cf\u4f20\u9012}\uff0c\u8be5\u65b9\u6cd5\u5728\u4e00\u6b21\u524d\u5411\u8fc7\u7a0b\u4e2d\u540c\u65f6\u89e3\u7801\u591a\u4e2a\u8fde\u7eed\u7684token\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u524d\u6587\u4e0a\u4e0b\u6587\u7684\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u4f20\u9012\u7ed9\u5f85\u751f\u6210\u672a\u6765token\u7684\\textit{\u4f2a}\u9690\u85cf\u72b6\u6001\uff0c\u968f\u540e\u4f2a\u9690\u85cf\u72b6\u6001\u901a\u8fc7\u540e\u7eed\u7684transformer\u5c42\uff0c\u4ece\u800c\u5438\u6536\u66f4\u591a\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5b9e\u73b0\u5bf9\u672a\u6765token\u7684\u66f4\u4f18\u9884\u6d4b\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u65b0\u9896\u7684\u6811\u72b6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u540c\u65f6\u751f\u6210\u548c\u9a8c\u8bc1\u591a\u4e2a\u8f93\u51fa\u5e8f\u5217\u7684\u5019\u9009\uff0c\u786e\u4fdd\u751f\u6210\u8fc7\u7a0b\u7684\u65e0\u635f\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u751f\u6210\u6548\u7387\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5206\u6790\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u6211\u4eec\u7684\u52a8\u673a\u3002\u5728\u52a0\u901f\u6307\u6807\u65b9\u9762\uff0c\u6211\u4eec\u4f18\u4e8e\u6240\u6709\u5355\u4e00\u6a21\u578b\u7684\u52a0\u901f\u6280\u672f\uff0c\u5305\u62ecMedusa\u548c\u81ea\u63a8\u6d4b\u89e3\u7801\u3002",
        "tldr_en": "We introduce a novel parallel decoding approach called \"hidden transfer\" that decodes multiple tokens simultaneously in a single forward pass, leveraging pseudo hidden states and a tree attention mechanism to enhance predictive accuracy and generation efficiency, outperforming existing acceleration techniques.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u9690\u85cf\u8f6c\u79fb\u201d\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u540c\u65f6\u89e3\u7801\u591a\u4e2a\u8fde\u7eed\u4ee4\u724c\uff0c\u5e76\u7ed3\u5408\u65b0\u9896\u7684\u6811\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u548c\u9a8c\u8bc1\u591a\u4e2a\u5019\u9009\u8f93\u51fa\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"
    },
    {
        "title": "TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding",
        "summary": "With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\\times$ on our optimized offloading system. Additionally, TriForce performs 4.86$\\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.",
        "authors": "Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen",
        "published": "2024-04-18",
        "link": "http://arxiv.org/abs/2404.11912v3",
        "chinese_summary": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u5185\u5bb9\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u5bf9\u9ad8\u6548\u957f\u5e8f\u5217\u63a8\u7406\u652f\u6301\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002\u7136\u800c\uff0c\u5b58\u50a8\u4ee5\u907f\u514d\u91cd\u65b0\u8ba1\u7b97\u7684\u5173\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u5df2\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u74f6\u9888\uff0c\u5176\u5927\u5c0f\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u3002\u7531\u4e8eLLMs\u7684\u81ea\u56de\u5f52\u7279\u6027\uff0c\u6bcf\u6b21\u751f\u6210\u4ee4\u724c\u65f6\u90fd\u9700\u8981\u52a0\u8f7d\u6574\u4e2aKV\u7f13\u5b58\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6838\u5fc3\u5229\u7528\u7387\u4f4e\u548c\u5ef6\u8fdf\u9ad8\u3002\u5c3d\u7ba1\u63d0\u51fa\u4e86\u5404\u79cdKV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u6709\u6240\u4e0b\u964d\u3002\u6211\u4eec\u5f15\u5165\u4e86TriForce\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u957f\u5e8f\u5217\u751f\u6210\u5c42\u6b21\u5316\u63a8\u6d4b\u89e3\u7801\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u539f\u59cb\u6a21\u578b\u6743\u91cd\u548c\u901a\u8fc7\u68c0\u7d22\u7684\u52a8\u6001\u7a00\u758fKV\u7f13\u5b58\u4f5c\u4e3a\u8349\u7a3f\u6a21\u578b\uff0c\u4f5c\u4e3a\u5c42\u6b21\u7ed3\u6784\u4e2d\u7684\u4e2d\u95f4\u5c42\uff0c\u5e76\u7531\u4e00\u4e2a\u66f4\u5c0f\u7684\u6a21\u578b\u8fdb\u4e00\u6b65\u63a8\u6d4b\u4ee5\u51cf\u5c11\u5176\u8349\u7a3f\u5ef6\u8fdf\u3002TriForce\u4e0d\u4ec5\u5728A100 GPU\u4e0a\u5b9e\u73b0\u4e86Llama2-7B-128K\u9ad8\u8fbe2.31\u500d\u7684\u52a0\u901f\uff0c\u800c\u4e14\u5728\u5904\u7406\u66f4\u957f\u4e0a\u4e0b\u6587\u65f6\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u3002\u5728\u4e24\u5757RTX 4090 GPU\u7684\u5378\u8f7d\u8bbe\u7f6e\u4e0b\uff0cTriForce\u5b9e\u73b0\u4e860.108\u79d2/\u4ee4\u724c\u7684\u901f\u5ea6\u2014\u2014\u4ec5\u6bd4A100\u4e0a\u7684\u81ea\u56de\u5f52\u57fa\u7ebf\u6162\u4e00\u534a\uff0c\u800c\u5728\u6211\u4eec\u4f18\u5316\u7684\u5378\u8f7d\u7cfb\u7edf\u4e0a\u8fbe\u5230\u4e867.78\u500d\u7684\u52a0\u901f\u3002\u6b64\u5916\uff0cTriForce\u5728\u5355\u5757RTX 4090 GPU\u4e0a\u7684\u8868\u73b0\u6bd4DeepSpeed-Zero-Inference\u5feb4.86\u500d\u3002TriForce\u7684\u9c81\u68d2\u6027\u901a\u8fc7\u5176\u5728\u5404\u79cd\u6e29\u5ea6\u4e0b\u59cb\u7ec8\u51fa\u8272\u7684\u8868\u73b0\u5f97\u4ee5\u4f53\u73b0\u3002\u4ee3\u7801\u53ef\u5728https://github.com/Infini-AI-Lab/TriForce\u83b7\u53d6\u3002",
        "tldr_en": "TriForce introduces a scalable hierarchical speculative decoding system for efficient long-sequence generation, achieving up to 2.31$\\times$ speedup on Llama2-7B-128K with robust performance across various conditions.",
        "tldr_zh": "TriForce \u662f\u4e00\u79cd\u5206\u5c42\u63a8\u6d4b\u89e3\u7801\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u539f\u59cb\u6a21\u578b\u6743\u91cd\u548c\u52a8\u6001\u7a00\u758f KV \u7f13\u5b58\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u751f\u6210\u7684\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 2.31 \u500d\u7684\u52a0\u901f\uff0c\u5e76\u5728\u5904\u7406\u66f4\u957f\u4e0a\u4e0b\u6587\u65f6\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u3002"
    },
    {
        "title": "On Speculative Decoding for Multimodal Large Language Models",
        "summary": "Inference with Multimodal Large Language Models (MLLMs) is slow due to their large-language-model backbone which suffers from memory bandwidth bottleneck and generates tokens auto-regressively. In this paper, we explore the application of speculative decoding to enhance the inference efficiency of MLLMs, specifically the LLaVA 7B model. We show that a language-only model can serve as a good draft model for speculative decoding with LLaVA 7B, bypassing the need for image tokens and their associated processing components from the draft model. Our experiments across three different tasks show that speculative decoding can achieve a memory-bound speedup of up to 2.37$\\times$ using a 115M parameter language model that we trained from scratch. Additionally, we introduce a compact LLaVA draft model incorporating an image adapter, which shows marginal performance gains in image captioning while maintaining comparable results in other tasks.",
        "authors": "Mukul Gagrani, Raghavv Goel, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott",
        "published": "2024-04-13",
        "link": "http://arxiv.org/abs/2404.08856v1",
        "chinese_summary": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u901f\u5ea6\u8f83\u6162\uff0c\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e8e\u5176\u5e9e\u5927\u7684\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u7f51\u7edc\u5b58\u5728\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff0c\u5e76\u4e14\u4ee5\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u6807\u8bb0\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u5c06\u63a8\u6d4b\u6027\u89e3\u7801\u5e94\u7528\u4e8e\u63d0\u5347MLLMs\u63a8\u7406\u6548\u7387\u7684\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9LLaVA 7B\u6a21\u578b\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4ec5\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u8349\u7a3f\u6a21\u578b\uff0c\u53ef\u4ee5\u6709\u6548\u7ed5\u8fc7\u5bf9\u56fe\u50cf\u6807\u8bb0\u53ca\u5176\u76f8\u5173\u5904\u7406\u7ec4\u4ef6\u7684\u9700\u6c42\u3002\u901a\u8fc7\u5728\u4e09\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u63a8\u6d4b\u6027\u89e3\u7801\u80fd\u591f\u5229\u7528\u6211\u4eec\u4ece\u5934\u8bad\u7ec3\u7684115M\u53c2\u6570\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u8fbe2.37\u500d\u7684\u5185\u5b58\u53d7\u9650\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u5305\u542b\u56fe\u50cf\u9002\u914d\u5668\u7684\u7d27\u51d1LLaVA\u8349\u7a3f\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f7b\u5fae\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u6210\u679c\u3002",
        "tldr_en": "This paper demonstrates that speculative decoding with a language-only model can significantly speed up multimodal large language model inference, achieving up to 2.37$\\times$ memory-bound speedup, and introduces a compact LLaVA draft model with an image adapter for marginal performance gains in image captioning.",
        "tldr_zh": "\u672c\u6587\u901a\u8fc7\u63a2\u7d22\u63a8\u6d4b\u89e3\u7801\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578bLLaVA 7B\u7684\u63a8\u7406\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.37\u500d\u7684\u5185\u5b58\u5e26\u5bbd\u52a0\u901f\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u56fe\u50cf\u9002\u914d\u5668\u7684\u7d27\u51d1\u8349\u7a3f\u6a21\u578b\uff0c\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7565\u6709\u6027\u80fd\u63d0\u5347\u3002"
    },
    {
        "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference",
        "summary": "Given the increasing demand for tree-structured interactions with LLMs, we introduce DeFT (Decoding with Flash Tree-Attention), an IO-aware tree attention algorithm tailored for tree-structured inference. Unlike traditional sequence-based decoding, tree-structured decoding better accommodates modern task requirements, including self-consistency, few-shot prompting, multi-step reasoning, and multi-model/head coordination. However, existing sequence-based inference systems are ill-suited for tree-structured decoding, resulting in redundancy in computation, memory footprints, and memory access, thereby undermining inference efficiency. To address this challenge, DeFT maintains memory-efficient attention calculation with low memory footprints through two key stages: (1) QKV Preparation: We propose a KV-Guided Grouping Strategy with Tree Split to intelligently group QKV, optimizing GPU resource utilization while minimizing memory reads/writes for KV cache between GPU global memory and on-chip shared memory; (2)Attention Calculation: We compute partial attention of each QKV group in a fused kernel and employ a Tree-topology-aware Global Reduction strategy to obtain final attention. By reducing 73-99% KV cache IO and nearly 100% IO for partial results during attention calculation (e.g., Softmax), DeFT achieves up to 2.52/3.82x speedup in the end-to-end/attention latency across three practical tree-based workloads: namely, few-shot prompting, multi-step reasoning, and speculative decoding, over state-of-the-art attention algorithms.",
        "authors": "Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin",
        "published": "2024-03-30",
        "link": "http://arxiv.org/abs/2404.00242v2",
        "chinese_summary": "\u9274\u4e8e\u5bf9\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u6811\u72b6\u4ea4\u4e92\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u6211\u4eec\u5f15\u5165\u4e86DeFT\uff08Decoding with Flash Tree-Attention\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a\u6811\u72b6\u7ed3\u6784\u63a8\u7406\u8bbe\u8ba1\u7684IO\u611f\u77e5\u6811\u6ce8\u610f\u529b\u7b97\u6cd5\u3002\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u5e8f\u5217\u7684\u89e3\u7801\u4e0d\u540c\uff0c\u6811\u72b6\u7ed3\u6784\u89e3\u7801\u66f4\u597d\u5730\u9002\u5e94\u4e86\u73b0\u4ee3\u4efb\u52a1\u9700\u6c42\uff0c\u5305\u62ec\u81ea\u4e00\u81f4\u6027\u3001\u5c11\u6837\u672c\u63d0\u793a\u3001\u591a\u6b65\u63a8\u7406\u4ee5\u53ca\u591a\u6a21\u578b/\u591a\u5934\u534f\u8c03\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u5e8f\u5217\u7684\u63a8\u7406\u7cfb\u7edf\u5e76\u4e0d\u9002\u5408\u6811\u72b6\u7ed3\u6784\u89e3\u7801\uff0c\u5bfc\u81f4\u8ba1\u7b97\u3001\u5185\u5b58\u5360\u7528\u548c\u5185\u5b58\u8bbf\u95ee\u7684\u5197\u4f59\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u63a8\u7406\u6548\u7387\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0cDeFT\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u5185\u5b58\u5360\u7528\uff1a\uff081\uff09QKV\u51c6\u5907\uff1a\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cdKV\u5f15\u5bfc\u7684\u5206\u7ec4\u7b56\u7565\u4e0e\u6811\u5206\u5272\u65b9\u6cd5\uff0c\u667a\u80fd\u5730\u5bf9QKV\u8fdb\u884c\u5206\u7ec4\uff0c\u4f18\u5316GPU\u8d44\u6e90\u5229\u7528\u7387\uff0c\u540c\u65f6\u6700\u5c0f\u5316GPU\u5168\u5c40\u5185\u5b58\u4e0e\u7247\u4e0a\u5171\u4eab\u5185\u5b58\u4e4b\u95f4KV\u7f13\u5b58\u7684\u8bfb\u5199\u64cd\u4f5c\uff1b\uff082\uff09\u6ce8\u610f\u529b\u8ba1\u7b97\uff1a\u6211\u4eec\u5728\u878d\u5408\u5185\u6838\u4e2d\u8ba1\u7b97\u6bcf\u4e2aQKV\u7ec4\u7684\u5c40\u90e8\u6ce8\u610f\u529b\uff0c\u5e76\u91c7\u7528\u4e00\u79cd\u6811\u72b6\u62d3\u6251\u611f\u77e5\u7684\u5168\u5c40\u5f52\u7ea6\u7b56\u7565\u6765\u83b7\u5f97\u6700\u7ec8\u7684\u6ce8\u610f\u529b\u3002\u901a\u8fc7\u51cf\u5c1173-99%\u7684KV\u7f13\u5b58IO\u548c\u63a5\u8fd1100%\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u5c40\u90e8\u7ed3\u679c\uff08\u5982Softmax\uff09\u7684IO\uff0cDeFT\u5728\u4e09\u79cd\u5b9e\u9645\u7684\u6811\u72b6\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5373\u5c11\u6837\u672c\u63d0\u793a\u3001\u591a\u6b65\u63a8\u7406\u548c\u63a8\u6d4b\u6027\u89e3\u7801\uff09\u4e0a\uff0c\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u6ce8\u610f\u529b\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.52/3.82\u500d\u7684\u7aef\u5230\u7aef/\u6ce8\u610f\u529b\u5ef6\u8fdf\u52a0\u901f\u3002",
        "tldr_en": "DeFT introduces an IO-aware tree attention algorithm for efficient tree-structured inference, significantly reducing KV cache IO and achieving up to 3.82x speedup in attention latency.",
        "tldr_zh": "DeFT\u662f\u4e00\u79cd\u9762\u5411\u6811\u7ed3\u6784\u63a8\u7406\u7684IO\u611f\u77e5\u6811\u6ce8\u610f\u529b\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316QKV\u5206\u7ec4\u548c\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u6811\u7ed3\u6784\u89e3\u7801\u6548\u7387\uff0c\u51cf\u5c11KV\u7f13\u5b58IO\u548c\u90e8\u5206\u7ed3\u679cIO\uff0c\u5b9e\u73b0\u9ad8\u8fbe2.52/3.82\u500d\u7684\u7aef\u5230\u7aef/\u6ce8\u610f\u529b\u5ef6\u8fdf\u52a0\u901f\u3002"
    },
    {
        "title": "SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens",
        "summary": "We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the \"two-step-draft-then-verify\" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.",
        "authors": "Chengbo Liu, Yong Zhu",
        "published": "2024-03-27",
        "link": "http://arxiv.org/abs/2403.18647v2",
        "chinese_summary": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8bed\u4e49\u81ea\u9002\u5e94\u6807\u8bb0\uff08SDSAT\uff09\u8fdb\u884c\u63a8\u6d4b\u89e3\u7801\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u52a0\u901f\u65b9\u6848\u3002\u8be5\u8bbe\u8ba1\u7684\u4e3b\u8981\u76ee\u6807\u662f\u63d0\u9ad8LLM\u6a21\u578b\u751f\u6210\u8349\u7a3f\u6807\u8bb0\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u6a21\u578b\u7684\u7cbe\u5ea6\u3002\u6838\u5fc3\u7b56\u7565\u5305\u62ec\uff1a1\uff09\u901a\u8fc7\u5f15\u5165\u5177\u6709\u7075\u6d3b\u89e3\u7801\u80fd\u529b\u7684\u8bed\u4e49\u81ea\u9002\u5e94\u6807\u8bb0\u6765\u5fae\u8c03\u6a21\u578b\uff0c\u8fd9\u4e9b\u6807\u8bb0\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8349\u7a3f\u6807\u8bb0\u30022\uff09\u91c7\u7528\u4e0d\u5f71\u54cd\u6807\u51c6\u6807\u8bb0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u5728\u5176\u539f\u6709\u6846\u67b6\u4e0a\u83b7\u5f97\u5e76\u884c\u89e3\u7801\u80fd\u529b\uff0c\u4e14\u8bad\u7ec3\u5f00\u9500\u6700\u5c0f\u5316\u30023\uff09\u6211\u4eec\u8bbe\u8ba1\u4e86\u201c\u4e24\u6b65\u8349\u7a3f-\u7136\u540e\u9a8c\u8bc1\u201d\u751f\u6210\u7b56\u7565\uff0c\u7ed3\u5408\u8d2a\u5fc3\u641c\u7d22\u548c\u6838\u91c7\u6837\u3002\u5728CodeLlama-13B\u548c7B\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u5206\u522b\u5b9e\u73b0\u4e86\u8d85\u8fc73.5\u500d\u548c3.0\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002\u8bf7\u53c2\u9605https://github.com/hasuoshenyun/SDSAT\u3002",
        "tldr_en": "We propose Speculative Decoding with Semantic Adaptive Tokens (SDSAT) to accelerate large language models, achieving up to 3.5X speed-up without accuracy loss.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8bed\u4e49\u81ea\u9002\u5e94\u4ee4\u724c\uff08SDSAT\uff09\u8fdb\u884c\u63a8\u6d4b\u89e3\u7801\u7684\u52a0\u901f\u65b9\u6848\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u8349\u7a3f\u4ee4\u724c\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728CodeLlama-13B\u548c7B\u6a21\u578b\u4e0a\u5206\u522b\u5b9e\u73b0\u4e863.5\u500d\u548c3\u500d\u7684\u52a0\u901f\u3002"
    },
    {
        "title": "Block Verification Accelerates Speculative Decoding",
        "summary": "Speculative decoding is an effective method for lossless acceleration of large language models during inference. It uses a fast model to draft a block of tokens which are then verified in parallel by the target model, and provides a guarantee that the output is distributed identically to a sample from the target model. In prior works, draft verification is performed independently token-by-token. Surprisingly, we show that this approach is not optimal. We propose Block Verification, a simple draft verification algorithm that verifies the entire block jointly and provides additional wall-clock speedup. We prove that the proposed mechanism is optimal in the expected number of tokens produced each iteration and specifically is never worse than the standard token-level verification. Empirically, block verification provides modest but consistent wall-clock speedups over the standard token verification algorithm of 5%-8% in a range of tasks and datasets. Given that block verification does not increase code complexity, maintains the strong lossless guarantee of the standard speculative decoding verification algorithm, cannot deteriorate performance, and, in fact, consistently improves it, it can be used as a good default in speculative decoding implementations.",
        "authors": "Ziteng Sun, Uri Mendlovic, Yaniv Leviathan, Asaf Aharoni, Ahmad Beirami, Jae Hun Ro, Ananda Theertha Suresh",
        "published": "2024-03-15",
        "link": "http://arxiv.org/abs/2403.10444v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e00\u79cd\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u635f\u52a0\u901f\u7684\u6709\u6548\u65b9\u6cd5\u3002\u5b83\u5229\u7528\u4e00\u4e2a\u5feb\u901f\u6a21\u578b\u6765\u8349\u62df\u4e00\u7ec4\u6807\u8bb0\uff0c\u7136\u540e\u7531\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\u8fd9\u4e9b\u6807\u8bb0\uff0c\u5e76\u4fdd\u8bc1\u8f93\u51fa\u4e0e\u4ece\u76ee\u6807\u6a21\u578b\u4e2d\u91c7\u6837\u7684\u5206\u5e03\u5b8c\u5168\u76f8\u540c\u3002\u5728\u5148\u524d\u7684\u7814\u7a76\u4e2d\uff0c\u8349\u7a3f\u9a8c\u8bc1\u662f\u9010\u4e2a\u6807\u8bb0\u72ec\u7acb\u8fdb\u884c\u7684\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u79cd\u65b9\u6cd5\u5e76\u975e\u6700\u4f18\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5757\u9a8c\u8bc1\u201d\u7684\u7b80\u5355\u8349\u7a3f\u9a8c\u8bc1\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u8054\u5408\u9a8c\u8bc1\u6574\u4e2a\u5757\uff0c\u5e76\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u65f6\u949f\u52a0\u901f\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u673a\u5236\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u751f\u6210\u7684\u9884\u671f\u6807\u8bb0\u6570\u91cf\u65b9\u9762\u662f\u6700\u4f18\u7684\uff0c\u5e76\u4e14\u660e\u786e\u6307\u51fa\u5b83\u6c38\u8fdc\u4e0d\u4f1a\u6bd4\u6807\u51c6\u7684\u9010\u6807\u8bb0\u9a8c\u8bc1\u66f4\u5dee\u3002\u4ece\u7ecf\u9a8c\u4e0a\u770b\uff0c\u5757\u9a8c\u8bc1\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6\u7684\u9010\u6807\u8bb0\u9a8c\u8bc1\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e865%-8%\u7684\u9002\u5ea6\u4f46\u6301\u7eed\u7684\u65f6\u949f\u52a0\u901f\u3002\u9274\u4e8e\u5757\u9a8c\u8bc1\u4e0d\u4f1a\u589e\u52a0\u4ee3\u7801\u590d\u6742\u6027\uff0c\u4fdd\u6301\u4e86\u6807\u51c6\u63a8\u6d4b\u6027\u89e3\u7801\u9a8c\u8bc1\u7b97\u6cd5\u7684\u5f3a\u65e0\u635f\u4fdd\u8bc1\uff0c\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u5e76\u4e14\u5b9e\u9645\u4e0a\u59cb\u7ec8\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u56e0\u6b64\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u63a8\u6d4b\u6027\u89e3\u7801\u5b9e\u73b0\u4e2d\u7684\u4e00\u4e2a\u826f\u597d\u9ed8\u8ba4\u9009\u9879\u3002",
        "tldr_en": "Block Verification optimizes speculative decoding by jointly verifying entire token blocks, achieving consistent 5%-8% wall-clock speedups with no performance degradation.",
        "tldr_zh": "\u6295\u673a\u89e3\u7801\u901a\u8fc7\u5757\u9a8c\u8bc1\u5b9e\u73b0\u65e0\u635f\u52a0\u901f\uff0c\u6bd4\u6807\u51c6\u9010\u8bcd\u9a8c\u8bc1\u63d0\u4f9b5%-8%\u7684\u6301\u7eed\u63d0\u901f\uff0c\u4e14\u4e0d\u589e\u52a0\u4ee3\u7801\u590d\u6742\u6027\uff0c\u53ef\u4f5c\u4e3a\u9ed8\u8ba4\u65b9\u6cd5\u3002"
    },
    {
        "title": "Recurrent Drafter for Fast Speculative Decoding in Large Language Models",
        "summary": "In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attention structure only for inference in Medusa. We empirically demonstrate the effectiveness of the proposed method on several popular open source language models, along with a comprehensive analysis of the trade-offs involved in adopting this approach.",
        "authors": "Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng",
        "published": "2024-03-14",
        "link": "http://arxiv.org/abs/2403.09919v3",
        "chinese_summary": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u6548\u7387\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e24\u79cd\u6210\u719f\u6280\u672f\u7684\u4f18\u52bf\uff1a\u7ecf\u5178\u7684\u4e8c\u6a21\u578b\u63a8\u6d4b\u6027\u89e3\u7801\u65b9\u6cd5\u548c\u8fd1\u671f\u51fa\u73b0\u7684\u5355\u6a21\u578b\u65b9\u6cd5\u2014\u2014Medusa\u3002\u53d7Medusa\u542f\u53d1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u5355\u6a21\u578b\u7b56\u7565\u8fdb\u884c\u63a8\u6d4b\u6027\u89e3\u7801\u3002\u7136\u800c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8349\u7a3f\u5934\uff0c\u5e76\u91c7\u7528\u5faa\u73af\u4f9d\u8d56\u8bbe\u8ba1\uff0c\u4e0e\u7ecf\u5178\u63a8\u6d4b\u6027\u89e3\u7801\u4e2d\u4f7f\u7528\u7684\u5c0f\u578b\u8349\u7a3f\u6a21\u578b\u5728\u672c\u8d28\u4e0a\u76f8\u4f3c\uff0c\u4f46\u907f\u514d\u4e86\u5b8c\u6574Transformer\u67b6\u6784\u7684\u590d\u6742\u6027\u3002\u7531\u4e8e\u5faa\u73af\u4f9d\u8d56\uff0c\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u8349\u7a3f\u5934\u5feb\u901f\u4f7f\u7528\u675f\u641c\u7d22\u8fc7\u6ee4\u6389\u4e0d\u7406\u60f3\u7684\u5019\u9009\u8005\u3002\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86\u5355\u6a21\u578b\u8bbe\u8ba1\u7684\u7b80\u6d01\u6027\uff0c\u5e76\u907f\u514d\u4e86Medusa\u4e2d\u4ec5\u7528\u4e8e\u63a8\u7406\u7684\u6570\u636e\u4f9d\u8d56\u6811\u6ce8\u610f\u529b\u7ed3\u6784\u7684\u521b\u5efa\u3002\u6211\u4eec\u901a\u8fc7\u5728\u51e0\u4e2a\u6d41\u884c\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5168\u9762\u5206\u6790\u4e86\u91c7\u7528\u8fd9\u79cd\u65b9\u6cd5\u6240\u6d89\u53ca\u7684\u6743\u8861\u3002",
        "tldr_en": "We present an enhanced speculative decoding method using a single, lightweight draft head with recurrent dependencies, combining simplicity and efficiency without the complexities of full transformer architectures or data-dependent tree structures.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ecf\u5178\u53cc\u6a21\u578b\u548c\u8fd1\u671f\u5355\u6a21\u578bMedusa\u4f18\u52bf\u7684\u6539\u8fdb\u578b\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5355\u4e00\u8f7b\u91cf\u7ea7\u8349\u7a3f\u5934\u548c\u5faa\u73af\u4f9d\u8d56\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u6570\u636e\u4f9d\u8d56\u6811\u6ce8\u610f\u529b\u7ed3\u6784\u3002"
    },
    {
        "title": "Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs",
        "summary": "Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional alignment procedure. For the finetuning step, we use instruction-response pairs generated by target model for distillation in plausible data distribution, and propose a new Total Variation Distance++ (TVD++) loss that incorporates variance reduction techniques inspired from the policy gradient method in reinforcement learning. Our empirical results show that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3 block efficiency and 2.4$\\times$ speed-up relative to autoregressive decoding on various tasks with no further task-specific fine-tuning.",
        "authors": "Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott",
        "published": "2024-02-29",
        "link": "http://arxiv.org/abs/2403.00858v4",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6587\u672c\u751f\u6210\u7531\u4e8e\u5176\u81ea\u56de\u5f52\u7279\u6027\u3001\u5e9e\u5927\u7684\u53c2\u6570\u6570\u91cf\u4ee5\u53ca\u6709\u9650\u7684\u5185\u5b58\u5e26\u5bbd\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u8f83\u4f4e\u7684token\u751f\u6210\u901f\u7387\uff0c\u56e0\u6b64\u88ab\u8ba4\u4e3a\u662f\u5185\u5b58\u53d7\u9650\u7684\u3002\u63a8\u6d4b\u6027\u89e3\u7801\u5df2\u88ab\u63d0\u51fa\u4f5c\u4e3a\u52a0\u901fLLM\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7531\u4e8e\u73b0\u4ee3\u5f00\u6e90LLM\u5bb6\u65cf\uff08\u5982Llama 2 7B\uff09\u4e2d\u901a\u5e38\u6ca1\u6709\u73b0\u6210\u7684\u8349\u7a3f\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u8349\u7a3f\u6a21\u578b\u4ee5\u901a\u8fc7\u63a8\u6d4b\u6027\u89e3\u7801\u5b9e\u73b0\u63a8\u7406\u52a0\u901f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u8349\u7a3f\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u76f4\u63a5\u5bf9\u9f50\u4e8e\u5177\u5907\u804a\u5929\u80fd\u529b\u7684\u9776\u6a21\u578b\u3002\u5229\u7528\u8be5\u6846\u67b6\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86Llama 2 Chat Drafter 115M\uff0c\u8fd9\u662f\u4e00\u4e2a\u9002\u7528\u4e8eLlama 2 Chat 7B\u6216\u66f4\u5927\u6a21\u578b\u7684\u8349\u7a3f\u6a21\u578b\uff0c\u5176\u5927\u5c0f\u4ec5\u4e3a\u539f\u59cb\u6a21\u578b\u76841.64%\u3002\u6211\u4eec\u7684\u8bad\u7ec3\u6846\u67b6\u4ec5\u5305\u62ec\u9884\u8bad\u7ec3\u3001\u84b8\u998f\u6570\u636e\u96c6\u751f\u6210\u4ee5\u53ca\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u7684\u5fae\u8c03\uff0c\u65e0\u9700\u989d\u5916\u7684\u5bf9\u9f50\u8fc7\u7a0b\u3002\u5728\u5fae\u8c03\u6b65\u9aa4\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u9776\u6a21\u578b\u751f\u6210\u7684\u6307\u4ee4-\u54cd\u5e94\u5bf9\u8fdb\u884c\u84b8\u998f\uff0c\u8fd9\u4e9b\u6570\u636e\u5177\u6709\u5408\u7406\u7684\u6570\u636e\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u603b\u53d8\u5dee\u8ddd\u79bb++\uff08TVD++\uff09\u635f\u5931\uff0c\u8be5\u635f\u5931\u7ed3\u5408\u4e86\u4ece\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4e2d\u542f\u53d1\u800c\u6765\u7684\u65b9\u5dee\u51cf\u5c11\u6280\u672f\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\uff0cLlama 2 Chat Drafter 115M\u7ed3\u5408\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u76f8\u5bf9\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.3\u500d\u7684\u5757\u6548\u7387\u548c2.4\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u8fdb\u4e00\u6b65\u7684\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002",
        "tldr_en": "This paper introduces a draft model training framework for accelerating LLM inference via speculative decoding, achieving significant speed-up and efficiency without additional alignment procedures.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u8349\u7a3f\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5bf9\u9f50\u804a\u5929\u80fd\u529b\u76ee\u6807\u6a21\u578b\uff0c\u8bad\u7ec3\u51fa\u4ec5\u5360\u539f\u6a21\u578b1.64%\u5927\u5c0f\u7684Llama 2 Chat Drafter 115M\uff0c\u7ed3\u5408\u63a8\u6d4b\u89e3\u7801\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.4\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u65e0\u9700\u8fdb\u4e00\u6b65\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002"
    },
    {
        "title": "Minions: Accelerating Large Language Model Inference with Adaptive and Collective Speculative Decoding",
        "summary": "Large language models (LLM) have recently attracted surging interest due to their outstanding capabilities across various domains. However, enabling efficient LLM inference is challenging due to its autoregressive decoding that generates tokens only one at a time. Although research works apply pruning or quantization to speed up LLM inference, they typically require fine-tuning the LLM, incurring significant time and economic costs. Meanwhile, speculative decoding has been proposed to use small speculative models (SSMs) to accelerate the inference of LLM. However, the low acceptance rate of SSM and the high verification cost of LLM prohibit further performance improvement of inference. In this paper, we propose Minions, an LLM inference system that accelerates LLM inference with a collective and adaptive speculative generation. Specifically, Minions proposes a majority-voted mechanism to leverage multiple SSMs to jointly speculate the outputs of LLM, which improves the inference performance without introducing prohibitive computation costs for LLM. To better trade off the number of tokens speculated from SSM and the verification cost of LLM, Minions proposes an adaptive mechanism to dynamically determine the optimal speculation length of SSM, which can achieve better inference performance across different models, datasets, and hyper-parameters. In addition, Minions decouples the SSM decoding and LLM verification efficiently and adopts a pipelined execution mechanism to further improve the inference performance of LLM. By comparing with the state-of-the-art LLM inference systems, we demonstrate that Minions can achieve higher inference throughput and lower inference time.",
        "authors": "Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng, Xin You, Yongjun Bao, Yi Liu, Zhongzhi Luan, Depei Qian",
        "published": "2024-02-24",
        "link": "http://arxiv.org/abs/2402.15678v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u56e0\u5176\u8de8\u591a\u4e2a\u9886\u57df\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u8fd1\u671f\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5176\u81ea\u56de\u5f52\u89e3\u7801\u65b9\u5f0f\uff0c\u5373\u4e00\u6b21\u53ea\u80fd\u751f\u6210\u4e00\u4e2a\u6807\u8bb0\uff0c\u4f7f\u5f97\u9ad8\u6548\u7684LLM\u63a8\u7406\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u901a\u8fc7\u526a\u679d\u6216\u91cf\u5316\u6765\u52a0\u901fLLM\u63a8\u7406\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u5e26\u6765\u663e\u8457\u7684\u65f6\u95f4\u548c\u7ecf\u6d4e\u6210\u672c\u3002\u540c\u65f6\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u5df2\u88ab\u63d0\u51fa\uff0c\u901a\u8fc7\u4f7f\u7528\u5c0f\u578b\u63a8\u6d4b\u6a21\u578b\uff08SSM\uff09\u6765\u52a0\u901fLLM\u7684\u63a8\u7406\u3002\u7136\u800c\uff0cSSM\u7684\u4f4e\u63a5\u53d7\u7387\u548cLLM\u7684\u9ad8\u9a8c\u8bc1\u6210\u672c\u9650\u5236\u4e86\u63a8\u7406\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Minions\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u96c6\u4f53\u548c\u81ea\u9002\u5e94\u63a8\u6d4b\u751f\u6210\u6765\u52a0\u901fLLM\u63a8\u7406\u7684\u7cfb\u7edf\u3002\u5177\u4f53\u800c\u8a00\uff0cMinions\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u5229\u7528\u591a\u4e2aSSM\u5171\u540c\u63a8\u6d4bLLM\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u5728\u4e0d\u5f15\u5165LLM\u7684\u8fc7\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u6743\u8861\u4eceSSM\u63a8\u6d4b\u7684\u6807\u8bb0\u6570\u91cf\u4e0eLLM\u7684\u9a8c\u8bc1\u6210\u672c\uff0cMinions\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u673a\u5236\uff0c\u52a8\u6001\u786e\u5b9aSSM\u7684\u6700\u4f73\u63a8\u6d4b\u957f\u5ea6\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u8d85\u53c2\u6570\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u63a8\u7406\u6027\u80fd\u3002\u6b64\u5916\uff0cMinions\u9ad8\u6548\u5730\u89e3\u8026\u4e86SSM\u89e3\u7801\u4e0eLLM\u9a8c\u8bc1\uff0c\u5e76\u91c7\u7528\u6d41\u6c34\u7ebf\u6267\u884c\u673a\u5236\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8LLM\u7684\u63a8\u7406\u6027\u80fd\u3002\u901a\u8fc7\u4e0e\u6700\u5148\u8fdb\u7684LLM\u63a8\u7406\u7cfb\u7edf\u8fdb\u884c\u6bd4\u8f83\uff0c\u6211\u4eec\u5c55\u793a\u4e86Minions\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u7684\u63a8\u7406\u541e\u5410\u91cf\u548c\u66f4\u4f4e\u7684\u63a8\u7406\u65f6\u95f4\u3002",
        "tldr_en": "Minions is an LLM inference system that accelerates decoding through collective, adaptive speculative generation using multiple small models, optimizing performance without excessive verification costs.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMinions\u7684LLM\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u4f53\u81ea\u9002\u5e94\u7684\u63a8\u6d4b\u751f\u6210\u673a\u5236\uff0c\u5229\u7528\u591a\u6570\u6295\u7968\u673a\u5236\u8054\u5408\u591a\u4e2a\u5c0f\u578b\u63a8\u6d4b\u6a21\u578b\uff08SSM\uff09\u6765\u52a0\u901fLLM\u63a8\u7406\uff0c\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u957f\u5ea6\u4ee5\u5e73\u8861\u63a8\u6d4b\u4e0e\u9a8c\u8bc1\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u6d41\u6c34\u7ebf\u6267\u884c\u673a\u5236\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u7406\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u541e\u5410\u91cf\u548c\u964d\u4f4e\u63a8\u7406\u65f6\u95f4\u3002"
    },
    {
        "title": "Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement",
        "summary": "Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the diversity of the tree. During RSD's drafting, the tree is built by either Gumbel-Top-$k$ trick that draws tokens without replacement in parallel or Stochastic Beam Search that samples sequences without replacement while early-truncating unlikely draft sequences and reducing the computational cost of LLM. We empirically evaluate RSD with Llama 2 and OPT models, showing that RSD outperforms the baseline methods, consistently for fixed draft sequence length and in most cases for fixed computational budgets at LLM.",
        "authors": "Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher Lott",
        "published": "2024-02-21",
        "link": "http://arxiv.org/abs/2402.14160v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6cd5\uff0c\u5176\u4e2d\u4e00\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e00\u4e2a\u8349\u7a3f\u6807\u8bb0\u5e8f\u5217\uff0c\u8be5\u5e8f\u5217\u968f\u540e\u7531\u76ee\u6807LLM\u5e76\u884c\u9a8c\u8bc1\u3002\u6700\u8fd1\u7684\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u8349\u7a3f\u6807\u8bb0\u6811\uff0c\u5c06\u8fd9\u4e00\u65b9\u6cd5\u63a8\u8fdb\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5355\u4e00\u5e8f\u5217\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5de5\u4f5c\u5728\u6811\u7684\u6bcf\u4e00\u5c42\u72ec\u7acb\u751f\u6210\u6807\u8bb0\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6811\u7684\u6574\u4f53\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u7684\u5b9e\u8bc1\u4f18\u8d8a\u6027\u4ec5\u5728\u56fa\u5b9a\u957f\u5ea6\u7684\u5e8f\u5217\u4e0a\u5f97\u5230\u5c55\u793a\uff0c\u9690\u542b\u5730\u4e3a\u57fa\u4e8e\u6811\u7684\u65b9\u6cd5\u8d4b\u4e88\u4e86\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90\u7ed9LLM\u3002\u5c3d\u7ba1\u56fa\u5b9a\u76ee\u6807\u8ba1\u7b97\u9884\u7b97\u5bf9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u91cd\u8981\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e2d\u6ca1\u6709\u4efb\u4f55\u4e00\u9879\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u9012\u5f52\u63a8\u6d4b\u6027\u89e3\u7801\uff08RSD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6811\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u653e\u56de\u62bd\u6837\u8349\u7a3f\u6807\u8bb0\uff0c\u6700\u5927\u5316\u6811\u7684\u591a\u6837\u6027\u3002\u5728RSD\u7684\u8349\u7a3f\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u6811\u7684\u6784\u5efa\u901a\u8fc7Gumbel-Top-$k$\u6280\u5de7\u5e76\u884c\u65e0\u653e\u56de\u5730\u62bd\u53d6\u6807\u8bb0\uff0c\u6216\u901a\u8fc7\u968f\u673a\u675f\u641c\u7d22\u65e0\u653e\u56de\u5730\u62bd\u6837\u5e8f\u5217\uff0c\u540c\u65f6\u65e9\u671f\u622a\u65ad\u4e0d\u592a\u53ef\u80fd\u7684\u8349\u7a3f\u5e8f\u5217\uff0c\u51cf\u5c11LLM\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6211\u4eec\u901a\u8fc7Llama 2\u548cOPT\u6a21\u578b\u5bf9RSD\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aRSD\u5728\u56fa\u5b9a\u8349\u7a3f\u5e8f\u5217\u957f\u5ea6\u548c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5728\u56fa\u5b9aLLM\u8ba1\u7b97\u9884\u7b97\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002",
        "tldr_en": "Recursive Speculative Decoding (RSD) enhances tree-based speculative decoding by maximizing tree diversity and efficiency, outperforming baselines under both fixed sequence length and computational budgets.",
        "tldr_zh": "\u9012\u5f52\u63a8\u6d4b\u89e3\u7801\uff08RSD\uff09\u901a\u8fc7\u65e0\u653e\u56de\u62bd\u6837\u548c\u6700\u5927\u5316\u6811\u591a\u6837\u6027\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u52a0\u901f\u6548\u679c\u3002"
    },
    {
        "title": "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding",
        "summary": "Speculative decoding is a widely used method that accelerates the generation process of large language models (LLMs) with no compromise in model performance. It achieves this goal by using an existing smaller model for drafting and then employing the target LLM to verify the draft in a low-cost parallel manner. Under such a drafting-verification framework, drafting efficiency has become a bottleneck in the final speedup of speculative decoding. Therefore, generating longer drafts at less cost can lead to better decoding speedup. To achieve this, we introduce Ouroboros, which can generate draft phrases to parallelize the drafting process and meanwhile lengthen drafts in a training-free manner. The experimental results on various typical text generation tasks show that Ouroboros can achieve speedups of up to $2.4\\times$ over speculative decoding and $3.9\\times$ over vanilla decoding, without fine-tuning draft and target models.",
        "authors": "Weilin Zhao, Yuxiang Huang, Xu Han, Wang Xu, Chaojun Xiao, Xinrong Zhang, Yewei Fang, Kaihuo Zhang, Zhiyuan Liu, Maosong Sun",
        "published": "2024-02-21",
        "link": "http://arxiv.org/abs/2402.13720v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u5728\u4e0d\u727a\u7272\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u52a0\u901f\u5176\u751f\u6210\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u73b0\u6709\u7684\u8f83\u5c0f\u6a21\u578b\u8fdb\u884c\u8349\u7a3f\u62df\u5b9a\uff0c\u7136\u540e\u4ee5\u4f4e\u6210\u672c\u7684\u5e76\u884c\u65b9\u5f0f\u4f7f\u7528\u76ee\u6807LLM\u6765\u9a8c\u8bc1\u8349\u7a3f\uff0c\u4ece\u800c\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u5728\u8fd9\u79cd\u8349\u7a3f-\u9a8c\u8bc1\u6846\u67b6\u4e0b\uff0c\u8349\u7a3f\u62df\u5b9a\u7684\u6548\u7387\u5df2\u6210\u4e3a\u6700\u7ec8\u52a0\u901f\u6548\u679c\u7684\u74f6\u9888\u3002\u56e0\u6b64\uff0c\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u751f\u6210\u66f4\u957f\u7684\u8349\u7a3f\u80fd\u591f\u5e26\u6765\u66f4\u597d\u7684\u89e3\u7801\u52a0\u901f\u6548\u679c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86Ouroboros\uff0c\u5b83\u80fd\u591f\u751f\u6210\u8349\u7a3f\u77ed\u8bed\u4ee5\u5e76\u884c\u5316\u8349\u7a3f\u62df\u5b9a\u8fc7\u7a0b\uff0c\u540c\u65f6\u4ee5\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u5f0f\u5ef6\u957f\u8349\u7a3f\u957f\u5ea6\u3002\u5728\u5404\u79cd\u5178\u578b\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOuroboros\u76f8\u8f83\u4e8e\u63a8\u6d4b\u6027\u89e3\u7801\u53ef\u5b9e\u73b0\u9ad8\u8fbe2.4\u500d\u7684\u52a0\u901f\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u89e3\u7801\u5219\u53ef\u8fbe\u52303.9\u500d\u7684\u52a0\u901f\uff0c\u4e14\u65e0\u9700\u5bf9\u8349\u7a3f\u548c\u76ee\u6807\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002",
        "tldr_en": "Ouroboros enhances speculative decoding by generating longer drafts without training, achieving up to 2.4x speedup over speculative and 3.9x over vanilla decoding.",
        "tldr_zh": "\u6295\u673a\u89e3\u7801\u901a\u8fc7\u4f7f\u7528\u8f83\u5c0f\u6a21\u578b\u8d77\u8349\u5e76\u7531\u76ee\u6807\u5927\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\uff0c\u5728\u4e0d\u964d\u4f4e\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\uff0c\u800cOuroboros\u901a\u8fc7\u751f\u6210\u66f4\u957f\u7684\u8349\u7a3f\u4ee5\u65e0\u8bad\u7ec3\u65b9\u5f0f\u63d0\u5347\u89e3\u7801\u6548\u7387\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u76f8\u6bd4\u6295\u673a\u89e3\u7801\u548c\u666e\u901a\u89e3\u7801\u5206\u522b\u63d0\u901f2.4\u500d\u548c3.9\u500d\u3002"
    },
    {
        "title": "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding",
        "summary": "As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.73\\times$, and $2.27\\times$. For offloading setting on L40, Sequoia achieves as low as 0.56 s/token for exact Llama2-70B inference latency, which is $9.96\\times$ on our optimized offloading system (5.6 s/token), $9.7\\times$ than DeepSpeed-Zero-Inference, $19.5\\times$ than Huggingface Accelerate.",
        "authors": "Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, Beidi Chen",
        "published": "2024-02-19",
        "link": "http://arxiv.org/abs/2402.12374v2",
        "chinese_summary": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4f7f\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u5982\u4f55\u9ad8\u6548\u5730\u8fdb\u884c\u8fd9\u4e9b\u6a21\u578b\u7684\u63a8\u7406\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5c3d\u7ba1\u63a8\u6d4b\u6027\u89e3\u7801\u6700\u8fd1\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u52a0\u901f\u63a8\u7406\u65b9\u5411\u51fa\u73b0\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u5c55\u5230\u66f4\u5927\u7684\u63a8\u6d4b\u9884\u7b97\u3001\u9002\u5e94\u4e0d\u540c\u7684\u8d85\u53c2\u6570\u548c\u786c\u4ef6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86Sequoia\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7a33\u5065\u4e14\u786c\u4ef6\u611f\u77e5\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u7b97\u6cd5\u3002\u4e3a\u4e86\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0cSequoia\u5f15\u5165\u4e86\u4e00\u79cd\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u6765\u5bfb\u627e\u63a8\u6d4b\u6027\u6807\u8bb0\u7684\u6700\u4f73\u6811\u7ed3\u6784\u3002\u4e3a\u4e86\u5b9e\u73b0\u7a33\u5065\u7684\u63a8\u6d4b\u6027\u80fd\uff0cSequoia\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91c7\u6837\u548c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u89e3\u7801\u6e29\u5ea6\u4e0b\u5747\u4f18\u4e8e\u5148\u524d\u7684\u5de5\u4f5c\u3002\u6700\u540e\uff0cSequoia\u5f15\u5165\u4e86\u4e00\u4e2a\u786c\u4ef6\u611f\u77e5\u7684\u6811\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u81ea\u52a8\u9009\u62e9\u7ed9\u5b9a\u786c\u4ef6\u5e73\u53f0\u7684\u6807\u8bb0\u6811\u5927\u5c0f\u548c\u6df1\u5ea6\uff0c\u6700\u5927\u5316\u63a8\u6d4b\u6027\u80fd\u3002\u8bc4\u4f30\u663e\u793a\uff0cSequoia\u5728A100\u4e0a\u5c06Llama2-7B\u3001Llama2-13B\u548cVicuna-33B\u7684\u89e3\u7801\u901f\u5ea6\u5206\u522b\u63d0\u9ad8\u4e86\u6700\u591a4.04\u500d\u30013.73\u500d\u548c2.27\u500d\u3002\u5728L40\u7684\u5378\u8f7d\u8bbe\u7f6e\u4e0b\uff0cSequoia\u5b9e\u73b0\u4e86\u4f4e\u81f30.56\u79d2/\u6807\u8bb0\u7684\u7cbe\u786eLlama2-70B\u63a8\u7406\u5ef6\u8fdf\uff0c\u8fd9\u662f\u6211\u4eec\u4f18\u5316\u540e\u7684\u5378\u8f7d\u7cfb\u7edf\uff085.6\u79d2/\u6807\u8bb0\uff09\u76849.96\u500d\uff0c\u6bd4DeepSpeed-Zero-Inference\u5feb9.7\u500d\uff0c\u6bd4Huggingface Accelerate\u5feb19.5\u500d\u3002",
        "tldr_en": "Sequoia introduces a scalable, robust, and hardware-aware speculative decoding algorithm that significantly boosts inference speed across various models and hardware platforms.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u4e14\u786c\u4ef6\u611f\u77e5\u7684\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5Sequoia\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u4f18\u5316\u63a8\u6d4b\u6811\u7ed3\u6784\uff0c\u91c7\u7528\u65b0\u578b\u91c7\u6837\u4e0e\u9a8c\u8bc1\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u786c\u4ef6\u611f\u77e5\u6811\u4f18\u5316\u5668\u81ea\u52a8\u9009\u62e9\u6811\u5927\u5c0f\u548c\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86Llama2\u548cVicuna\u6a21\u578b\u5728A100\u548cL40\u786c\u4ef6\u4e0a\u7684\u89e3\u7801\u901f\u5ea6\u3002"
    },
    {
        "title": "Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding",
        "summary": "This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel \\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality.",
        "authors": "Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, Rong Xiao",
        "published": "2024-02-19",
        "link": "http://arxiv.org/abs/2402.11809v3",
        "chinese_summary": "\u672c\u7814\u7a76\u65e8\u5728\u52a0\u901f\u62e5\u6709\u6570\u5341\u4ebf\u53c2\u6570\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u901f\u5ea6\u3002\u6211\u4eec\u63d0\u51fa\u4e86**S**mart **P**arallel **A**uto-**C**orrect **E**ncoding\uff08SPACE\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0LLMs\u7684\u65e0\u635f\u52a0\u901f\u3002\u901a\u8fc7\u6574\u5408\u534a\u81ea\u56de\u5f52\u63a8\u7406\u548c\u63a8\u6d4b\u6027\u89e3\u7801\u80fd\u529b\uff0cSPACE\u72ec\u7279\u5730\u4f7f\u81ea\u56de\u5f52LLMs\u80fd\u591f\u5e76\u884c\u5316\u4ee4\u724c\u751f\u6210\u548c\u9a8c\u8bc1\u3002\u8fd9\u4e00\u8fc7\u7a0b\u901a\u8fc7\u4e13\u95e8\u7684\u534a\u81ea\u56de\u5f52\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\uff0c\u4f7f\u73b0\u6709LLMs\u5177\u5907\u540c\u65f6\u9884\u6d4b\u591a\u4e2a\u4ee4\u724c\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u4e00\u79cd\u81ea\u52a8\u6821\u6b63\u89e3\u7801\u7b97\u6cd5\u4fc3\u8fdb\u4e86\u5728\u5355\u6b21\u6a21\u578b\u8c03\u7528\u4e2d\u540c\u65f6\u751f\u6210\u548c\u9a8c\u8bc1\u4ee4\u724c\u5e8f\u5217\u3002\u901a\u8fc7\u5bf9\u4e00\u7cfb\u5217LLMs\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cSPACE\u5728HumanEval-X\u4e0a\u5c55\u793a\u4e862.7x-4.0x\u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f93\u51fa\u8d28\u91cf\u3002",
        "tldr_en": "SPACE accelerates large language models by integrating semi-autoregressive inference and speculative decoding, achieving up to 4.0x speedup with no loss in quality.",
        "tldr_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u667a\u80fd\u5e76\u884c\u81ea\u52a8\u6821\u6b63\u89e3\u7801\uff08SPACE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u81ea\u56de\u5f52\u63a8\u7406\u548c\u63a8\u6d4b\u89e3\u7801\uff0c\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65e0\u635f\u52a0\u901f\uff0c\u5b9e\u9a8c\u663e\u793a\u5728HumanEval-X\u4e0a\u63a8\u7406\u901f\u5ea6\u63d0\u53472.7x-4.0x\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002"
    },
    {
        "title": "Speculative Streaming: Fast LLM Inference without Auxiliary Models",
        "summary": "Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while using ~10000X fewer extra parameters, making it well-suited for resource-constrained devices.",
        "authors": "Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, Mahyar Najibi",
        "published": "2024-02-16",
        "link": "http://arxiv.org/abs/2402.11131v1",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e00\u79cd\u663e\u8457\u7684\u6280\u672f\uff0c\u901a\u8fc7\u8f85\u52a9\u8349\u7a3f\u6a21\u578b\u7684\u9884\u6d4b\u6765\u52a0\u901f\u5927\u578b\u76ee\u6807\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u3002\u5c3d\u7ba1\u6709\u6548\uff0c\u4f46\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u901a\u5e38\u9700\u8981\u5bf9\u8349\u7a3f\u6a21\u578b\u548c\u76ee\u6807\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u5b9e\u73b0\u9ad8\u63a5\u53d7\u7387\u3002\u968f\u7740\u4e0b\u6e38\u4efb\u52a1\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8fd9\u4e9b\u8349\u7a3f\u6a21\u578b\u7ed9\u63a8\u7406\u7cfb\u7edf\u5e26\u6765\u4e86\u663e\u8457\u7684\u590d\u6742\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u63a8\u6d4b\u6027\u6d41\u5f0f\u5904\u7406\uff08Speculative Streaming\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5355\u6a21\u578b\u63a8\u6d4b\u6027\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5fae\u8c03\u76ee\u6807\u4ece\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u6539\u4e3a\u672a\u6765n-gram\u9884\u6d4b\uff0c\u5c06\u8349\u7a3f\u529f\u80fd\u878d\u5408\u5230\u76ee\u6807\u6a21\u578b\u4e2d\u3002\u5728\u6458\u8981\u751f\u6210\u3001\u7ed3\u6784\u5316\u67e5\u8be2\u548c\u610f\u4e49\u8868\u793a\u7b49\u591a\u6837\u5316\u4efb\u52a1\u4e2d\uff0c\u63a8\u6d4b\u6027\u6d41\u5f0f\u5904\u7406\u5c06\u89e3\u7801\u901f\u5ea6\u63d0\u9ad8\u4e861.8\u81f33.1\u500d\uff0c\u4e14\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u63a8\u6d4b\u6027\u6d41\u5f0f\u5904\u7406\u5177\u6709\u53c2\u6570\u6548\u7387\u3002\u5b83\u5728\u901f\u5ea6\u63d0\u5347\u65b9\u9762\u4e0eMedusa\u98ce\u683c\u7684\u67b6\u6784\u76f8\u5f53\u751a\u81f3\u66f4\u9ad8\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u989d\u5916\u53c2\u6570\u51cf\u5c11\u4e86\u7ea610000\u500d\uff0c\u975e\u5e38\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u3002",
        "tldr_en": "Speculative Streaming is a single-model speculative decoding method that accelerates inference by 1.8-3.1X across diverse tasks without quality loss, using significantly fewer parameters than traditional approaches.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u4e00\u6a21\u578b\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u2014\u2014\u63a8\u6d4b\u6d41\uff08Speculative Streaming\uff09\uff0c\u901a\u8fc7\u5c06\u8349\u7a3f\u751f\u6210\u878d\u5165\u76ee\u6807\u6a21\u578b\uff0c\u6539\u53d8\u4e86\u5fae\u8c03\u76ee\u6807\u4ece\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u5230\u672a\u6765n-gram\u9884\u6d4b\uff0c\u4ece\u800c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e861.8-3.1\u500d\u7684\u89e3\u7801\u52a0\u901f\uff0c\u4e14\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u5177\u6709\u6781\u9ad8\u7684\u53c2\u6570\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002"
    },
    {
        "title": "Tandem Transformers for Inference Efficient LLMs",
        "summary": "The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.",
        "authors": "Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli",
        "published": "2024-02-13",
        "link": "http://arxiv.org/abs/2402.08644v3",
        "chinese_summary": "\u4f20\u7edf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u56de\u5f52\u7279\u6027\u672c\u8d28\u4e0a\u9650\u5236\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u56e0\u4e3a\u4ee4\u724c\u662f\u6309\u987a\u5e8f\u751f\u6210\u7684\u3002\u5c3d\u7ba1\u63a8\u6d4b\u6027\u548c\u5e76\u884c\u89e3\u7801\u6280\u672f\u8bd5\u56fe\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u9762\u4e34\u5c40\u9650\u6027\uff1a\u8981\u4e48\u4f9d\u8d56\u751f\u6210\u51c6\u786e\u6027\u8f83\u4f4e\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u8981\u4e48\u65e0\u6cd5\u5145\u5206\u5229\u7528\u57fa\u7840LLM\u7684\u8868\u793a\u80fd\u529b\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\u2014\u2014Tandem transformers\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u8be5\u67b6\u6784\u72ec\u7279\u5730\u7ed3\u5408\u4e86\uff081\uff09\u4e00\u4e2a\u5c0f\u578b\u81ea\u56de\u5f52\u6a21\u578b\u548c\uff082\uff09\u4e00\u4e2a\u4ee5\u5757\u6a21\u5f0f\u8fd0\u884c\u7684\u5927\u578b\u6a21\u578b\uff08\u540c\u65f6\u5904\u7406\u591a\u4e2a\u4ee4\u724c\uff09\u3002\u901a\u8fc7\u8d4b\u4e88\u5c0f\u578b\u6a21\u578b\u5bf9\u5927\u578b\u6a21\u578b\u66f4\u4e30\u5bcc\u8868\u793a\u7684\u5173\u6ce8\uff0c\u5176\u9884\u6d4b\u51c6\u786e\u6027\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\u3002\u5728PaLM2\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\uff0cPaLM2-Bison\u548cPaLM2-Gecko\u7684\u7ec4\u5408\u5728\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u6bd4\u5355\u72ec\u7684PaLM2-Gecko\u63d0\u9ad8\u4e863.3%\uff0c\u4e0e\u4e0b\u6e38\u6027\u80fd\u76f8\u5f53\u7684PaLM2-Otter\u6a21\u578b\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e861.16\u500d\u7684\u52a0\u901f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u8fd9\u79cd\u7ec4\u5408\u6a21\u578b\u6574\u5408\u5230\u63a8\u6d4b\u6027\u89e3\u7801\uff08SPEED\uff09\u6846\u67b6\u4e2d\uff0c\u5176\u4e2d\u5927\u578b\u6a21\u578b\u9a8c\u8bc1\u6765\u81ea\u5c0f\u578b\u6a21\u578b\u7684\u4ee4\u724c\u3002\u8fd9\u786e\u4fdd\u4e86PaLM2-Bison\u548cPaLM2-Gecko\u7684\u7ec4\u5408\u5728\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u4e0d\u53d8\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\uff08\u6bd4\u5728SPEED\u4e2d\u4f7f\u7528\u539f\u59cbPaLM2-Gecko\u5feb\u7ea61.14\u500d\uff09\u3002",
        "tldr_en": "Tandem transformers enhance inference speed and accuracy by combining a small autoregressive model with a large block-mode model, achieving significant improvements in next-token prediction and downstream task performance.",
        "tldr_zh": "Tandem transformers\u67b6\u6784\u901a\u8fc7\u7ed3\u5408\u5c0f\u81ea\u56de\u5f52\u6a21\u578b\u548c\u5927\u5757\u5904\u7406\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u548c\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728PaLM2\u6570\u636e\u96c6\u4e0a\u5b9e\u73b01.16\u500d\u52a0\u901f\u548c3.3%\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002"
    },
    {
        "title": "Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding",
        "summary": "To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design space of Hydra head training objectives and architectures, and propose a carefully-tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by 1.31x and 2.71x compared to Medusa decoding and autoregressive decoding, respectively. Overall, Hydra heads are a simple intervention on standard draft heads that significantly improve the end-to-end speed of draft head based speculative decoding.",
        "authors": "Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, William Brandon",
        "published": "2024-02-07",
        "link": "http://arxiv.org/abs/2402.05109v1",
        "chinese_summary": "\u4e3a\u4e86\u5e94\u5bf9\u81ea\u56de\u5f52\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u5185\u5b58\u5e26\u5bbd\u53d7\u9650\u7684\u7279\u6027\uff0c\u5148\u524d\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u63a8\u6d4b\u6027\u89e3\u7801\u6846\u67b6\u3002\u4e3a\u4e86\u6267\u884c\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u4e00\u4e2a\u5c0f\u578b\u7684\u8349\u7a3f\u6a21\u578b\u4f1a\u63d0\u51fa\u8f93\u5165\u5e8f\u5217\u7684\u5019\u9009\u5ef6\u7eed\uff0c\u7136\u540e\u7531\u57fa\u7840\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\u8fd9\u4e9b\u5019\u9009\u5ef6\u7eed\u3002\u6700\u8fd1\u5728Medusa\u89e3\u7801\u6846\u67b6\u4e2d\u4f7f\u7528\u7684\u4e00\u79cd\u6307\u5b9a\u8349\u7a3f\u6a21\u578b\u7684\u65b9\u6cd5\u662f\uff0c\u5c06\u5176\u4f5c\u4e3a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5934\u90e8\u7684\u96c6\u5408\uff0c\u79f0\u4e3a\u8349\u7a3f\u5934\uff0c\u8fd9\u4e9b\u5934\u90e8\u5728\u57fa\u7840\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u4e0a\u8fdb\u884c\u64cd\u4f5c\u3002\u8fc4\u4eca\u4e3a\u6b62\uff0c\u6240\u6709\u73b0\u6709\u7684\u8349\u7a3f\u5934\u90fd\u662f\u987a\u5e8f\u72ec\u7acb\u7684\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4eec\u5728\u5019\u9009\u5ef6\u7eed\u4e2d\u63a8\u6d4b\u7684\u6807\u8bb0\u4e0e\u5019\u9009\u5ef6\u7eed\u4e2d\u7684\u4efb\u4f55\u5148\u524d\u6807\u8bb0\u65e0\u5173\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Hydra\u5934\uff0c\u8fd9\u662f\u4e00\u79cd\u987a\u5e8f\u4f9d\u8d56\u7684\u3001\u6807\u51c6\u8349\u7a3f\u5934\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u54c1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u6d4b\u7684\u51c6\u786e\u6027\u3002\u4f7f\u7528Hydra\u5934\u8fdb\u884c\u89e3\u7801\u76f8\u6bd4\u4f7f\u7528\u6807\u51c6\u8349\u7a3f\u5934\u7684Medusa\u89e3\u7801\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e86Hydra\u5934\u8bad\u7ec3\u76ee\u6807\u548c\u67b6\u6784\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u5fc3\u8c03\u4f18\u7684Hydra\u5934\u914d\u65b9\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aHydra++\uff0c\u76f8\u6bd4Medusa\u89e3\u7801\u548c\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u5206\u522b\u63d0\u9ad8\u4e861.31\u500d\u548c2.71\u500d\u7684\u89e3\u7801\u541e\u5410\u91cf\u3002\u603b\u7684\u6765\u8bf4\uff0cHydra\u5934\u662f\u5bf9\u6807\u51c6\u8349\u7a3f\u5934\u7684\u7b80\u5355\u5e72\u9884\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8e\u8349\u7a3f\u5934\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u7aef\u5230\u7aef\u901f\u5ea6\u3002",
        "tldr_en": "Hydra heads enhance speculative decoding accuracy and throughput by introducing sequentially dependent draft heads, outperforming standard draft heads and traditional autoregressive decoding.",
        "tldr_zh": "\u63d0\u51fa\u987a\u5e8f\u4f9d\u8d56\u7684Hydra\u5934\u4f5c\u4e3a\u6807\u51c6\u8349\u7a3f\u5934\u7684\u66ff\u4ee3\uff0c\u663e\u8457\u63d0\u5347\u63a8\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7Hydra++\u4f18\u5316\u8bbe\u8ba1\uff0c\u4f7f\u89e3\u7801\u541e\u5410\u91cf\u5206\u522b\u6bd4Medusa\u89e3\u7801\u548c\u81ea\u56de\u5f52\u89e3\u7801\u63d0\u9ad81.31\u500d\u548c2.71\u500d\u3002"
    },
    {
        "title": "Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation",
        "summary": "To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin.",
        "authors": "Luca Beurer-Kellner, Marc Fischer, Martin Vechev",
        "published": "2024-02-07",
        "link": "http://arxiv.org/abs/2403.06988v1",
        "chinese_summary": "\u4e3a\u4e86\u786e\u4fdd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u6587\u672c\u7b26\u5408\u9884\u671f\u683c\u5f0f\uff0c\u53d7\u9650\u89e3\u7801\u63d0\u51fa\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u6267\u884c\u4e25\u683c\u7684\u6b63\u5f0f\u8bed\u8a00\u7ea6\u675f\u3002\u7136\u800c\uff0c\u6b63\u5982\u6211\u4eec\u5728\u672c\u6587\u4e2d\u6240\u5c55\u793a\u7684\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u4ec5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5e26\u6765\u4e86\u6027\u80fd\u5f00\u9500\uff0c\u800c\u4e14\u5728\u672a\u80fd\u6b63\u786e\u5bf9\u9f50\u5e95\u5c42LLM\u5b50\u8bcd\u8bcd\u6c47\u4e0e\u5916\u90e8\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd8\u4f1a\u663e\u8457\u635f\u5bb3\u4efb\u52a1\u51c6\u786e\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u7801\u7b97\u6cd5\u2014\u2014DOMINO\uff0c\u5b83\u80fd\u591f\u5728\u5b8c\u5168\u5b50\u8bcd\u5bf9\u9f50\u7684\u65b9\u5f0f\u4e0b\u5f3a\u5236\u6267\u884c\u7ea6\u675f\uff0c\u540c\u65f6\u5229\u7528\u9884\u8ba1\u7b97\u548c\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u5b9e\u73b0\u51e0\u4e4e\u65e0\u5f00\u9500\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6bd4\u65e0\u7ea6\u675f\u89e3\u7801\u5feb\u8fd12\u500d\u2014\u2014\u4ece\u800c\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002",
        "tldr_en": "DOMINO introduces a novel decoding algorithm that enforces subword-aligned constraints with minimal overhead, significantly outperforming existing methods in both speed and accuracy.",
        "tldr_zh": "DOMINO\u7b97\u6cd5\u901a\u8fc7\u5168\u5b50\u8bcd\u5bf9\u9f50\u7ea6\u675f\u89e3\u7801\uff0c\u5b9e\u73b0\u65e0\u6027\u80fd\u5f00\u9500\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u52a0\u901f\u8fd1\u4e24\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"
    },
    {
        "title": "GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding",
        "summary": "Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.",
        "authors": "Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, Yang You",
        "published": "2024-02-03",
        "link": "http://arxiv.org/abs/2402.02082v1",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e00\u79cd\u76f8\u5bf9\u8f83\u65b0\u7684\u89e3\u7801\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5c0f\u578b\u4e14\u9ad8\u6548\u7684\u8349\u7a3f\u6a21\u578b\u6765\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5ef6\u8fdf\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86GliDe\u548cCaPE\uff0c\u8fd9\u4e24\u4e2a\u5bf9\u4f20\u7edf\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u4f4e\u6210\u672c\u6539\u8fdb\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u51bb\u7ed3LLM\u7684\u89e3\u7801\u901f\u5ea6\u3002\u5177\u4f53\u800c\u8a00\uff0cGliDe\u662f\u4e00\u79cd\u4fee\u6539\u540e\u7684\u8349\u7a3f\u6a21\u578b\u67b6\u6784\uff0c\u5b83\u91cd\u7528\u4e86\u76ee\u6807LLM\u7684\u7f13\u5b58\u952e\u548c\u503c\uff0c\u800cCaPE\u5219\u662f\u4e00\u79cd\u63d0\u6848\u6269\u5c55\u65b9\u6cd5\uff0c\u5229\u7528\u8349\u7a3f\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u5e2e\u52a9\u9009\u62e9\u989d\u5916\u7684\u5019\u9009\u4ee4\u724c\u8fdb\u884c\u9a8c\u8bc1\u3002\u5728\u4e0d\u540c\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684GliDe\u8349\u7a3f\u6a21\u578b\u663e\u8457\u964d\u4f4e\u4e86\u9884\u671f\u7684\u89e3\u7801\u5ef6\u8fdf\u3002\u901a\u8fc7\u4f7f\u7528\u5b9e\u9645\u65f6\u95f4\uff08walltime\uff09\u8fdb\u884c\u7684\u989d\u5916\u8bc4\u4f30\u663e\u793a\uff0cGliDe\u53ef\u4ee5\u5c06Vicuna\u6a21\u578b\u7684\u901f\u5ea6\u63d0\u5347\u81f32.17\u500d\uff0c\u800c\u7ed3\u5408CaPE\u540e\uff0c\u8fd9\u4e00\u6539\u8fdb\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5c55\u81f32.61\u500d\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u6211\u4eec\u7684\u4ee3\u7801\u3001\u6570\u636e\u4ee5\u53ca\u8bad\u7ec3\u597d\u7684\u8349\u7a3f\u6a21\u578b\u3002",
        "tldr_en": "GliDe and CaPE enhance speculative decoding by optimizing draft model architecture and candidate token selection, significantly reducing LLM decoding latency up to 2.61x.",
        "tldr_zh": "\u672c\u7814\u7a76\u63d0\u51faGliDe\u548cCaPE\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\u4e2d\u7684\u8349\u7a3f\u6a21\u578b\u67b6\u6784\u548c\u5019\u9009\u8bcd\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u51bb\u7ed3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u901f\u5ea6\uff0c\u5b9e\u9a8c\u8bc1\u660eGliDe\u53ef\u52a0\u901fVicuna\u6a21\u578b\u8fbe2.17\u500d\uff0c\u7ed3\u5408CaPE\u53ef\u63d0\u5347\u81f32.61\u500d\uff0c\u5e76\u5c06\u516c\u5f00\u4ee3\u7801\u3001\u6570\u636e\u53ca\u8bad\u7ec3\u6a21\u578b\u3002"
    },
    {
        "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
        "summary": "Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding",
        "authors": "Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang",
        "published": "2024-02-03",
        "link": "http://arxiv.org/abs/2402.02057v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u56de\u5f52\u89e3\u7801\u53d7\u9650\u4e8e\u5185\u5b58\u5e26\u5bbd\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u73b0\u4ee3\u52a0\u901f\u5668\u5e76\u884c\u5904\u7406\u80fd\u529b\u7684\u5927\u91cf\u6d6a\u8d39\u3002\u73b0\u6709\u7684\u52a0\u901fLLM\u89e3\u7801\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e00\u4e2a\u8349\u7a3f\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\uff09\uff0c\u8fd9\u4e0d\u5bb9\u6613\u83b7\u5f97\u4e14\u65e0\u6cd5\u6cdb\u5316\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86Lookahead\u89e3\u7801\uff0c\u8fd9\u662f\u4e00\u79cd\u7cbe\u786e\u7684\u3001\u5e76\u884c\u7684\u89e3\u7801\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u8f85\u52a9\u6a21\u578b\u6216\u6570\u636e\u5b58\u50a8\u7684\u60c5\u51b5\u4e0b\u52a0\u901fLLM\u89e3\u7801\u3002\u5b83\u5141\u8bb8\u901a\u8fc7\u51cf\u5c11\u603b\u89e3\u7801\u6b65\u9aa4\u6765\u4ea4\u6362\u6bcf\u4e00\u6b65\u7684\u5bf9\u6570\uff08FLOPs\uff09\uff0c\u5728\u5355\u4e2a\u6216\u591a\u4e2a\u73b0\u4ee3\u52a0\u901f\u5668\u4e0a\u66f4\u5177\u5e76\u884c\u6027\uff0c\u5e76\u4e14\u4e0e\u5e76\u53d1\u5185\u5b58\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\uff08\u4f8b\u5982\uff0cFlashAttention\uff09\u517c\u5bb9\u3002\u6211\u4eec\u7684Lookahead\u89e3\u7801\u5b9e\u73b0\u53ef\u4ee5\u5728MT-bench\u4e0a\u5c06\u81ea\u56de\u5f52\u89e3\u7801\u901f\u5ea6\u63d0\u5347\u81f3\u591a1.8\u500d\uff0c\u5728\u591aGPU\u4e0a\u7684\u4ee3\u7801\u5b8c\u6210\u4efb\u52a1\u4e2d\u901a\u8fc7\u5f3a\u6269\u5c55\u6027\u63d0\u5347\u81f3\u591a4\u500d\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/hao-ai-lab/LookaheadDecoding\u83b7\u53d6\u3002",
        "tldr_en": "Lookahead decoding introduces an exact, parallel algorithm to accelerate large language model decoding without auxiliary models, trading per-step computation for fewer steps and achieving up to 4x speedup on multi-GPU code completion tasks.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u6216\u6570\u636e\u5b58\u50a8\u7684\u7cbe\u786e\u5e76\u884c\u89e3\u7801\u7b97\u6cd5Lookahead decoding\uff0c\u901a\u8fc7\u51cf\u5c11\u89e3\u7801\u6b65\u9aa4\u548c\u63d0\u9ad8\u5e76\u884c\u6027\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u6700\u9ad8\u53ef\u63d0\u901f1.8\u500d\u3002"
    },
    {
        "title": "Decoding Speculative Decoding",
        "summary": "Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using speculative decoding and delineate the factors that affect the performance gain provided by speculative decoding. Our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model's capability in language modeling does not correlate strongly with its performance in speculative decoding. Based on these insights we explore a new design space for draft models and design hardware-efficient draft models for speculative decoding. Our newly designed draft model for LLaMA-65B can provide 111% higher throughput than existing draft models and can generalize further to the LLaMA-2 model family and supervised fine-tuned models.",
        "authors": "Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman",
        "published": "2024-02-02",
        "link": "http://arxiv.org/abs/2402.01528v3",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u52a0\u5feb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u901f\u5ea6\u3002\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u4f7f\u7528\u4e00\u4e2a\u8f83\u5c0f\u7684\u8349\u7a3f\u6a21\u578b\u6765\u751f\u6210\u63a8\u6d4b\u6027\u6807\u8bb0\uff0c\u7136\u540e\u4f7f\u7528\u76ee\u6807LLM\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u8349\u7a3f\u6807\u8bb0\u3002\u63a8\u6d4b\u6027\u89e3\u7801\u63d0\u4f9b\u7684\u52a0\u901f\u6548\u679c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u8349\u7a3f\u6a21\u578b\u7684\u9009\u62e9\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u7814\u7a76\uff0c\u5305\u62ec\u5bf9LLaMA-65B\u548cOPT-66B\u8fdb\u884c\u7684\u8d85\u8fc7350\u6b21\u5b9e\u9a8c\uff0c\u4f7f\u7528\u63a8\u6d4b\u6027\u89e3\u7801\u5e76\u9610\u660e\u4e86\u5f71\u54cd\u63a8\u6d4b\u6027\u89e3\u7801\u6027\u80fd\u589e\u76ca\u7684\u56e0\u7d20\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u8349\u7a3f\u6a21\u578b\u7684\u5ef6\u8fdf\uff0c\u800c\u8349\u7a3f\u6a21\u578b\u5728\u8bed\u8a00\u5efa\u6a21\u65b9\u9762\u7684\u80fd\u529b\u4e0e\u5176\u5728\u63a8\u6d4b\u6027\u89e3\u7801\u4e2d\u7684\u8868\u73b0\u6ca1\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u8349\u7a3f\u6a21\u578b\u7684\u65b0\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9002\u7528\u4e8e\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u786c\u4ef6\u9ad8\u6548\u8349\u7a3f\u6a21\u578b\u3002\u6211\u4eec\u4e3aLLaMA-65B\u65b0\u8bbe\u8ba1\u7684\u8349\u7a3f\u6a21\u578b\u53ef\u4ee5\u6bd4\u73b0\u6709\u8349\u7a3f\u6a21\u578b\u63d0\u4f9b111%\u7684\u66f4\u9ad8\u541e\u5410\u91cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a8\u5e7f\u5230LLaMA-2\u6a21\u578b\u5bb6\u65cf\u548c\u76d1\u7763\u5fae\u8c03\u6a21\u578b\u3002",
        "tldr_en": "Speculative Decoding accelerates Large Language Models by using hardware-efficient draft models, significantly boosting throughput without compromising quality.",
        "tldr_zh": "\u63a8\u6d4b\u6027\u89e3\u7801\u901a\u8fc7\u4f7f\u7528\u5c0f\u578b\u8349\u7a3f\u6a21\u578b\u751f\u6210\u63a8\u6d4b\u6027\u6807\u8bb0\u5e76\u7531\u76ee\u6807\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e14\u4e0d\u964d\u4f4e\u8d28\u91cf\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8349\u7a3f\u6a21\u578b\u7684\u5ef6\u8fdf\u5bf9\u63a8\u6d4b\u6027\u89e3\u7801\u6027\u80fd\u5f71\u54cd\u91cd\u5927\uff0c\u800c\u5176\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u4e0e\u5176\u5728\u63a8\u6d4b\u6027\u89e3\u7801\u4e2d\u7684\u8868\u73b0\u5173\u8054\u4e0d\u5927\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u786c\u4ef6\u9ad8\u6548\u7684\u8349\u7a3f\u6a21\u578b\uff0c\u65b0\u8bbe\u8ba1\u7684LLaMA-65B\u8349\u7a3f\u6a21\u578b\u6bd4\u73b0\u6709\u6a21\u578b\u541e\u5410\u91cf\u63d0\u9ad8111%\uff0c\u5e76\u80fd\u63a8\u5e7f\u81f3LLaMA-2\u7cfb\u5217\u548c\u76d1\u7763\u5fae\u8c03\u6a21\u578b\u3002"
    },
    {
        "title": "MambaByte: Token-free Selective State Space Model",
        "summary": "Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.",
        "authors": "Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush",
        "published": "2024-01-24",
        "link": "http://arxiv.org/abs/2401.13660v3",
        "chinese_summary": "\u65e0\u4ee4\u724c\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u4ece\u539f\u59cb\u5b57\u8282\u4e2d\u5b66\u4e60\uff0c\u5e76\u6d88\u9664\u4e86\u5b50\u8bcd\u5206\u8bcd\u5e26\u6765\u7684\u5f52\u7eb3\u504f\u5dee\u3002\u7136\u800c\uff0c\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\u8fdb\u884c\u64cd\u4f5c\u4f1a\u5bfc\u81f4\u5e8f\u5217\u957f\u5ea6\u663e\u8457\u589e\u52a0\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6807\u51c6\u7684\u81ea\u56de\u5f52Transformer\u5728\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6240\u9700\u7684\u6709\u6548\u5185\u5b58\u4f1a\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u52a0\u800c\u589e\u957f\u3002\u6700\u8fd1\u5f00\u53d1\u7684Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5b83\u5177\u6709\u56fa\u5b9a\u5927\u5c0f\u7684\u5185\u5b58\u72b6\u6001\u548c\u9ad8\u6548\u7684\u89e3\u7801\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86MambaByte\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u4ee4\u724c\u7684Mamba SSM\u81ea\u56de\u5f52\u8bad\u7ec3\u5728\u5b57\u8282\u5e8f\u5217\u4e0a\u7684\u9002\u5e94\u6a21\u578b\u3002\u5728\u6a21\u578b\u6027\u80fd\u65b9\u9762\uff0c\u6211\u4eec\u5c55\u793a\u4e86MambaByte\u5728\u4e0e\u6700\u5148\u8fdb\u7684\u5b50\u8bcdTransformer\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u65f6\u5177\u6709\u7ade\u4e89\u529b\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u65e0\u4ee4\u724c\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5982\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002\u5728\u6548\u7387\u65b9\u9762\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4ee4\u724c\u5316\u8349\u7a3f\u548c\u5b57\u8282\u7ea7\u9a8c\u8bc1\u3002\u8fd9\u4f7f\u5f97\u6807\u51c6MambaByte\u5b9e\u73b0\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e862.6\u500d\uff0c\u663e\u793a\u51fa\u4e0e\u5b50\u8bcdMamba\u76f8\u4f3c\u7684\u89e3\u7801\u6548\u7387\u3002\u8fd9\u4e9b\u53d1\u73b0\u786e\u7acb\u4e86SSM\u5728\u5b9e\u73b0\u65e0\u4ee4\u724c\u8bed\u8a00\u5efa\u6a21\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002",
        "tldr_en": "MambaByte leverages the Mamba state space model for efficient token-free language modeling, outperforming subword Transformers and achieving a 2.6x inference speedup with speculative decoding.",
        "tldr_zh": "MambaByte\u901a\u8fc7\u5f15\u5165\u56fa\u5b9a\u5185\u5b58\u72b6\u6001\u548c\u9ad8\u6548\u89e3\u7801\u7684Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e0\u4ee4\u724c\u8bed\u8a00\u6a21\u578b\u7684\u521b\u65b0\uff0c\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u63a8\u7406\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002"
    },
    {
        "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "summary": "Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.   Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.",
        "authors": "Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao",
        "published": "2024-01-19",
        "link": "http://arxiv.org/abs/2401.10774v3",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u91c7\u7528\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u8fd9\u79cd\u89e3\u7801\u65b9\u5f0f\u9700\u8981\u987a\u5e8f\u8ba1\u7b97\uff0c\u6bcf\u4e00\u6b65\u7684\u8ba1\u7b97\u90fd\u4f9d\u8d56\u4e8e\u4e0a\u4e00\u6b65\u7684\u8f93\u51fa\u3002\u8fd9\u9020\u6210\u4e86\u4e00\u4e2a\u74f6\u9888\uff0c\u56e0\u4e3a\u6bcf\u4e00\u6b65\u90fd\u9700\u8981\u5c06\u5b8c\u6574\u7684\u6a21\u578b\u53c2\u6570\u4ece\u9ad8\u5e26\u5bbd\u5185\u5b58\uff08HBM\uff09\u79fb\u52a8\u5230\u52a0\u901f\u5668\u7684\u7f13\u5b58\u4e2d\u3002\u5c3d\u7ba1\u5df2\u7ecf\u63d0\u51fa\u4e86\u5982\u63a8\u6d4b\u6027\u89e3\u7801\u7b49\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u7684\u5b9e\u65bd\u53d7\u5230\u4e86\u83b7\u53d6\u548c\u7ef4\u62a4\u4e00\u4e2a\u5355\u72ec\u7684\u8349\u7a3f\u6a21\u578b\u7684\u6311\u6218\u7684\u963b\u788d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Medusa\uff0c\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u989d\u5916\u7684\u89e3\u7801\u5934\u6765\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u5e76\u884c\u9884\u6d4b\u591a\u4e2a\u540e\u7eed\u7684token\u3002\u5229\u7528\u57fa\u4e8e\u6811\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0cMedusa\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\u6784\u5efa\u591a\u4e2a\u5019\u9009\u7684\u5ef6\u7eed\uff0c\u5e76\u540c\u65f6\u9a8c\u8bc1\u5b83\u4eec\u3002\u901a\u8fc7\u5229\u7528\u5e76\u884c\u5904\u7406\uff0cMedusa\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u7684\u89e3\u7801\u6b65\u9aa4\u6570\u91cf\u3002\u6211\u4eec\u4e3aMedusa\u63d0\u4f9b\u4e86\u4e24\u79cd\u7ea7\u522b\u7684\u5fae\u8c03\u7a0b\u5e8f\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u4f7f\u7528\u573a\u666f\u7684\u9700\u6c42\uff1aMedusa-1\uff1aMedusa\u76f4\u63a5\u5728\u51bb\u7ed3\u7684\u9aa8\u5e72LLM\u4e4b\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u65e0\u635f\u7684\u63a8\u7406\u52a0\u901f\u3002Medusa-2\uff1aMedusa\u4e0e\u9aa8\u5e72LLM\u4e00\u8d77\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5f97Medusa\u5934\u7684\u9884\u6d4b\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u52a0\u901f\u6548\u679c\u66f4\u597d\uff0c\u4f46\u9700\u8981\u4e00\u79cd\u7279\u6b8a\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u4fdd\u6301\u9aa8\u5e72\u6a21\u578b\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u6269\u5c55\uff0c\u4ee5\u6539\u8fdb\u6216\u6269\u5c55Medusa\u7684\u5b9e\u7528\u6027\uff0c\u5305\u62ec\u4e00\u79cd\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6ca1\u6709\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u60c5\u51b5\uff0c\u4ee5\u53ca\u4e00\u79cd\u5178\u578b\u7684\u63a5\u53d7\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u63a5\u53d7\u7387\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002\u6211\u4eec\u5728\u4e0d\u540c\u5927\u5c0f\u548c\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6a21\u578b\u4e0a\u8bc4\u4f30\u4e86Medusa\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMedusa-1\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8d85\u8fc72.2\u500d\u7684\u52a0\u901f\uff0c\u800cMedusa-2\u8fdb\u4e00\u6b65\u5c06\u52a0\u901f\u6548\u679c\u63d0\u5347\u52302.3-3.6\u500d\u3002",
        "tldr_en": "Medusa enhances LLM inference by adding parallel decoding heads and a tree-based attention mechanism, achieving up to 3.6x speedup without compromising quality.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedusa\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u989d\u5916\u7684\u89e3\u7801\u5934\u5e76\u5229\u7528\u6811\u72b6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5e76\u884c\u9884\u6d4b\u591a\u4e2a\u540e\u7eedtoken\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u7684\u89e3\u7801\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e862.2\u500d\u4ee5\u4e0a\u7684\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5931\u751f\u6210\u8d28\u91cf\u3002"
    },
    {
        "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding",
        "summary": "To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.",
        "authors": "Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui",
        "published": "2024-01-15",
        "link": "http://arxiv.org/abs/2401.07851v3",
        "chinese_summary": "\u4e3a\u4e86\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u81ea\u56de\u5f52\u89e3\u7801\u5e26\u6765\u7684\u9ad8\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684LLM\u63a8\u7406\u89e3\u7801\u8303\u5f0f\u5e94\u8fd0\u800c\u751f\u3002\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\uff0c\u8be5\u65b9\u6cd5\u9996\u5148\u9ad8\u6548\u5730\u8349\u62df\u51fa\u51e0\u4e2a\u672a\u6765\u6807\u8bb0\uff0c\u7136\u540e\u5e76\u884c\u5730\u8fdb\u884c\u9a8c\u8bc1\u3002\u4e0e\u81ea\u56de\u5f52\u89e3\u7801\u4e0d\u540c\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u652f\u6301\u6bcf\u4e00\u6b65\u540c\u65f6\u89e3\u7801\u591a\u4e2a\u6807\u8bb0\uff0c\u4ece\u800c\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\u3002\u672c\u6587\u5bf9\u8fd9\u4e00\u6709\u524d\u666f\u7684\u89e3\u7801\u8303\u5f0f\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6982\u8ff0\u548c\u5206\u6790\u3002\u6211\u4eec\u9996\u5148\u63d0\u4f9b\u4e86\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u6b63\u5f0f\u5b9a\u4e49\u548c\u516c\u5f0f\u5316\u63cf\u8ff0\u3002\u63a5\u7740\uff0c\u6211\u4eec\u6df1\u5165\u63a2\u8ba8\u4e86\u5176\u5173\u952e\u65b9\u9762\uff0c\u5982\u8349\u7a3f\u6a21\u578b\u9009\u62e9\u548c\u9a8c\u8bc1\u7b56\u7565\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5bf9\u7b2c\u4e09\u65b9\u6d4b\u8bd5\u73af\u5883\u4e2d\u9886\u5148\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002\u6211\u4eec\u671f\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u591f\u6210\u4e3a\u63a8\u52a8\u63a8\u6d4b\u6027\u89e3\u7801\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u50ac\u5316\u5242\uff0c\u6700\u7ec8\u4e3a\u66f4\u9ad8\u6548\u7684LLM\u63a8\u7406\u505a\u51fa\u8d21\u732e\u3002",
        "tldr_en": "This paper introduces Speculative Decoding, a novel parallel decoding approach for accelerating Large Language Models by efficiently drafting and verifying multiple tokens per step, and aims to foster further research in this area.",
        "tldr_zh": "\u672c\u6587\u7efc\u8ff0\u5e76\u5206\u6790\u4e86\u65b0\u578b\u89e3\u7801\u8303\u5f0f\u201c\u63a8\u6d4b\u89e3\u7801\u201d\uff0c\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u5e76\u884c\u9a8c\u8bc1\u672a\u6765\u8bcd\u5143\uff0c\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u63a8\u52a8\u66f4\u9ad8\u6548\u7684LLM\u63a8\u7406\u7814\u7a76\u3002"
    },
    {
        "title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding",
        "summary": "The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks.",
        "authors": "Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, Yuxiao Dong",
        "published": "2024-01-12",
        "link": "http://arxiv.org/abs/2401.06761v1",
        "chinese_summary": "\u5927\u89c4\u6a21\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u9ad8\u6548\u7684\u90e8\u7f72\u7b56\u7565\u3002\u7136\u800c\uff0c\u5927\u591a\u6570LLMs\u751f\u6210\u6587\u672c\u6240\u4f9d\u8d56\u7684\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\uff0c\u5bf9\u5b9e\u73b0\u9ad8\u6548\u670d\u52a1\u63d0\u51fa\u4e86\u6311\u6218\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5e76\u884c\u81ea\u56de\u5f52\u751f\u6210\u65b9\u6cd5\u3002\u901a\u8fc7\u5728\u5305\u542b\u5c42\u6b21\u7ed3\u6784\u7684\u4e00\u822c\u9886\u57df\u6570\u636e\u4e0a\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\uff0c\u6211\u4eec\u4f7fLLMs\u80fd\u591f\u72ec\u7acb\u89c4\u5212\u5176\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u6267\u884c\u81ea\u52a8\u5e76\u884c\u81ea\u56de\u5f52\uff08APAR\uff09\u751f\u6210\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u751f\u6210\u6b65\u9aa4\u7684\u6570\u91cf\u3002\u4ec5APAR\u5c31\u80fd\u5b9e\u73b0\u9ad8\u8fbe2\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u800c\u4e0e\u63a8\u6d4b\u6027\u89e3\u7801\u7ed3\u5408\u65f6\uff0c\u901f\u5ea6\u63d0\u5347\u53ef\u8fbe4\u500d\u3002\u6b64\u5916\uff0cAPAR\u8fd8\u51cf\u5c11\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u952e\u503c\u7f13\u5b58\u6d88\u8017\u548c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u90e8\u7f72\u6846\u67b6\u76f8\u6bd4\uff0c\u5728\u9ad8\u541e\u5410\u91cf\u573a\u666f\u4e0b\uff0c\u8fd9\u5e26\u6765\u4e8620-70%\u7684\u541e\u5410\u91cf\u589e\u52a0\u548c20-35%\u7684\u5ef6\u8fdf\u51cf\u5c11\u3002",
        "tldr_en": "We introduce a parallel auto-regressive generation method (APAR) that significantly speeds up LLM text generation, achieving up to 4x speedup with speculative decoding and improving throughput and latency in high-throughput scenarios.",
        "tldr_zh": "\u5f15\u5165\u5e76\u884c\u81ea\u56de\u5f52\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u901f\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"
    },
    {
        "title": "Multi-Candidate Speculative Decoding",
        "summary": "Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding.",
        "authors": "Sen Yang, Shujian Huang, Xinyu Dai, Jiajun Chen",
        "published": "2024-01-12",
        "link": "http://arxiv.org/abs/2401.06706v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u80fd\u529b\uff0c\u4f46\u5176\u81ea\u56de\u5f52\u751f\u6210\u6587\u672c\u7684\u8fc7\u7a0b\u8017\u65f6\u8f83\u957f\u3002\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u5b83\u901a\u8fc7\u4e00\u4e2a\u5feb\u901f\u7684\u8349\u7a3f\u6a21\u578b\u751f\u6210\u5019\u9009\u7247\u6bb5\uff08\u5373\u4e00\u7cfb\u5217\u6807\u8bb0\uff09\uff0c\u7136\u540e\u7531\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\u8fd9\u4e9b\u5019\u9009\u7247\u6bb5\u3002\u7136\u800c\uff0c\u5019\u9009\u6807\u8bb0\u7684\u63a5\u53d7\u7387\u53d7\u5230\u591a\u79cd\u56e0\u7d20\u7684\u9650\u5236\uff0c\u5982\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u89e3\u7801\u8bbe\u7f6e\u3002\u672c\u6587\u63d0\u51fa\u4ece\u8349\u7a3f\u6a21\u578b\u4e2d\u91c7\u6837\u591a\u4e2a\u5019\u9009\u7247\u6bb5\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u6279\u6b21\u8fdb\u884c\u9a8c\u8bc1\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684\u591a\u79cd\u5019\u9009\u9a8c\u8bc1\u7b97\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u6a21\u578b\u7684\u5206\u5e03\u7279\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\uff0c\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u65b9\u6cd5\u3002",
        "tldr_en": "This paper enhances speculative decoding by sampling multiple draft model candidates and batch-verifying them, significantly improving acceptance rates across datasets and models.",
        "tldr_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5019\u9009\u91c7\u6837\u548c\u6279\u91cf\u9a8c\u8bc1\u7684\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u3002"
    },
    {
        "title": "Cascade Speculative Drafting for Even Faster LLM Inference",
        "summary": "Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves up to an 81 percent additional speedup over speculative decoding in our experiments, while maintaining the same output distribution as the target model. Our code is publicly available at https://github.com/lfsszd/CS-Drafting.",
        "authors": "Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Kevin Chen-Chuan Chang, Jie Huang",
        "published": "2023-12-18",
        "link": "http://arxiv.org/abs/2312.11462v4",
        "chinese_summary": "\u4e3a\u4e86\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u6548\u7387\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\uff08Speculative Decoding\uff09\u5f15\u5165\u4e86\u7531\u8f83\u5c0f\u6a21\u578b\u751f\u6210\u8349\u7a3f\u7684\u673a\u5236\u3002\u968f\u540e\uff0c\u4e00\u4e2a\u66f4\u5927\u7684\u76ee\u6807\u6a21\u578b\u4f1a\u5bf9\u8fd9\u4e2a\u8349\u7a3f\u8fdb\u884c\u5ba1\u67e5\uff0c\u4ee5\u786e\u4fdd\u5176\u8f93\u51fa\u4e0e\u76ee\u6807\u6a21\u578b\u4e00\u81f4\u3002\u4e00\u65e6\u76ee\u6807\u6a21\u578b\u63a5\u53d7\u4e86\u8fd9\u4e2a\u8349\u7a3f\uff0c\u5c31\u4f1a\u51cf\u5c11\u76ee\u6807\u6a21\u578b\u7684\u8fd0\u884c\u6b21\u6570\uff0c\u4ece\u800c\u63d0\u5347\u6574\u4f53\u6548\u7387\u3002\u7136\u800c\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u4e2d\u7684\u8349\u7a3f\u751f\u6210\u8fc7\u7a0b\u5305\u542b\u4e86\u7f13\u6162\u7684\u81ea\u56de\u5f52\u751f\u6210\uff0c\u5e76\u4e14\u65e0\u8bba\u751f\u6210\u7684\u6807\u8bb0\u7684\u91cd\u8981\u6027\u5982\u4f55\uff0c\u90fd\u5206\u914d\u4e86\u76f8\u540c\u7684\u65f6\u95f4\u3002\u8fd9\u4e9b\u4f4e\u6548\u56e0\u7d20\u5171\u540c\u5bfc\u81f4\u4e86\u63a8\u6d4b\u6027\u89e3\u7801\u6027\u80fd\u7684\u6b20\u4f73\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316LLM\u63a8\u7406\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7ea7\u8054\u63a8\u6d4b\u8349\u7a3f\uff08Cascade Speculative Drafting\uff0c\u7b80\u79f0CS Drafting\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u4e24\u79cd\u7ea7\u8054\u7c7b\u578b\u7684\u63a8\u6d4b\u6267\u884c\u7b97\u6cd5\u3002\u5782\u76f4\u7ea7\u8054\u6d88\u9664\u4e86\u795e\u7ecf\u6a21\u578b\u4e2d\u7684\u81ea\u56de\u5f52\u751f\u6210\uff0c\u800c\u6c34\u5e73\u7ea7\u8054\u5219\u4f18\u5316\u4e86\u8349\u7a3f\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e24\u79cd\u7ea7\u8054\uff0cCS Drafting\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u63a8\u6d4b\u6027\u89e3\u7801\u9ad8\u8fbe81%\u7684\u989d\u5916\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u76ee\u6807\u6a21\u578b\u76f8\u540c\u7684\u8f93\u51fa\u5206\u5e03\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728https://github.com/lfsszd/CS-Drafting\u516c\u5f00\u53d1\u5e03\u3002",
        "tldr_en": "Cascade Speculative Drafting (CS Drafting) enhances LLM inference efficiency by eliminating autoregressive generation and optimizing token time allocation, achieving up to 81% additional speedup.",
        "tldr_zh": "\u5f15\u5165\u5782\u76f4\u548c\u6c34\u5e73\u7ea7\u8054\u7684Cascade Speculative Drafting\u7b97\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u81ea\u56de\u5f52\u751f\u6210\u548c\u4f18\u5316\u65f6\u95f4\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u5b9e\u9a8c\u4e2d\u6bd4\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u5feb81%\u3002"
    },
    {
        "title": "Speculative Contrastive Decoding",
        "summary": "Large language models~(LLMs) exhibit exceptional performance in language tasks, yet their auto-regressive inference is limited due to high computational requirements and is sub-optimal due to the exposure bias. Inspired by speculative decoding and contrastive decoding, we introduce Speculative Contrastive Decoding~(SCD), a straightforward yet powerful decoding approach that leverages predictions from smaller language models~(LMs) to achieve both decoding acceleration and quality improvement. Extensive evaluations and analyses on four diverse language tasks demonstrate the effectiveness of SCD, showing that decoding efficiency and quality can compatibly benefit from one smaller LM.",
        "authors": "Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou",
        "published": "2023-11-15",
        "link": "http://arxiv.org/abs/2311.08981v2",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u66b4\u9732\u504f\u5dee\uff0c\u5176\u81ea\u56de\u5f52\u63a8\u7406\u53d7\u5230\u9650\u5236\u4e14\u6548\u679c\u6b20\u4f73\u3002\u53d7\u63a8\u6d4b\u6027\u89e3\u7801\u548c\u5bf9\u6bd4\u89e3\u7801\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u63a8\u6d4b\u6027\u5bf9\u6bd4\u89e3\u7801\uff08Speculative Contrastive Decoding\uff0c\u7b80\u79f0SCD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u5229\u7528\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u7684\u9884\u6d4b\u6765\u5b9e\u73b0\u89e3\u7801\u52a0\u901f\u548c\u8d28\u91cf\u63d0\u5347\u3002\u901a\u8fc7\u5bf9\u56db\u79cd\u4e0d\u540c\u8bed\u8a00\u4efb\u52a1\u7684\u5e7f\u6cdb\u8bc4\u4f30\u548c\u5206\u6790\uff0c\u6211\u4eec\u5c55\u793a\u4e86SCD\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u89e3\u7801\u6548\u7387\u548c\u8d28\u91cf\u53ef\u4ee5\u540c\u65f6\u53d7\u76ca\u4e8e\u4e00\u4e2a\u8f83\u5c0f\u7684LM\u3002",
        "tldr_en": "Speculative Contrastive Decoding (SCD) enhances large language model performance by leveraging smaller models for accelerated and improved decoding.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u63a8\u6d4b\u5bf9\u6bd4\u89e3\u7801\u201d\uff08SCD\uff09\u7684\u7b80\u5355\u800c\u5f3a\u5927\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u6765\u52a0\u901f\u89e3\u7801\u5e76\u63d0\u5347\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u9879\u8bed\u8a00\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"
    },
    {
        "title": "REST: Retrieval-Based Speculative Decoding",
        "summary": "We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm designed to speed up language model generation. The key insight driving the development of REST is the observation that the process of text generation often includes certain common phases and patterns. Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens. This method draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context. Its plug-and-play nature allows for seamless integration and acceleration of any language models, all without necessitating additional training. When benchmarked on 7B and 13B language models in a single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on code or text generation. The code of REST is available at https://github.com/FasterDecoding/REST.",
        "authors": "Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, Di He",
        "published": "2023-11-14",
        "link": "http://arxiv.org/abs/2311.08252v2",
        "chinese_summary": "\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u57fa\u4e8e\u68c0\u7d22\u7684\u63a8\u6d4b\u89e3\u7801\uff08Retrieval-Based Speculative Decoding, REST\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u65e8\u5728\u52a0\u901f\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u3002\u63a8\u52a8REST\u5f00\u53d1\u7684\u5173\u952e\u6d1e\u5bdf\u5728\u4e8e\u89c2\u5bdf\u5230\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u901a\u5e38\u5305\u542b\u67d0\u4e9b\u5e38\u89c1\u7684\u9636\u6bb5\u548c\u6a21\u5f0f\u3002\u4e0e\u4ee5\u5f80\u4f9d\u8d56\u8349\u7a3f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u6d4b\u89e3\u7801\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cREST\u5229\u7528\u68c0\u7d22\u7684\u529b\u91cf\u6765\u751f\u6210\u8349\u7a3f\u6807\u8bb0\u3002\u8fd9\u79cd\u65b9\u6cd5\u4ece\u73b0\u6709\u77e5\u8bc6\u5e93\u4e2d\u6c72\u53d6\u4fe1\u606f\uff0c\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u68c0\u7d22\u5e76\u4f7f\u7528\u76f8\u5173\u6807\u8bb0\u3002\u5176\u5373\u63d2\u5373\u7528\u7684\u7279\u6027\u4f7f\u5f97\u4efb\u4f55\u8bed\u8a00\u6a21\u578b\u90fd\u80fd\u65e0\u7f1d\u96c6\u6210\u5e76\u52a0\u901f\uff0c\u4e14\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u3002\u5728\u5355\u6279\u6b21\u8bbe\u7f6e\u4e0b\uff0c\u5bf97B\u548c13B\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u65f6\uff0cREST\u5728\u4ee3\u7801\u6216\u6587\u672c\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e861.62\u500d\u52302.36\u500d\u7684\u663e\u8457\u52a0\u901f\u3002REST\u7684\u4ee3\u7801\u53ef\u5728https://github.com/FasterDecoding/REST\u83b7\u53d6\u3002",
        "tldr_en": "REST accelerates language model generation by leveraging retrieval for draft token creation, achieving up to 2.36X speedup without additional training.",
        "tldr_zh": "REST\u7b97\u6cd5\u901a\u8fc7\u68c0\u7d22\u751f\u6210\u8349\u7a3f\u4ee4\u724c\uff0c\u663e\u8457\u52a0\u901f\u8bed\u8a00\u6a21\u578b\u751f\u6210\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5b9e\u73b01.62X\u81f32.36X\u7684\u52a0\u901f\u3002"
    },
    {
        "title": "Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling",
        "summary": "As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. In this work, we leverage pseudo-labelling to assemble a large-scale open-source dataset which we use to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, we select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51% fewer parameters, while performing to within 1% WER on out-of-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. To facilitate further research in this domain, we make our training code, inference code and models publicly accessible.",
        "authors": "Sanchit Gandhi, Patrick von Platen, Alexander M. Rush",
        "published": "2023-11-01",
        "link": "http://arxiv.org/abs/2311.00430v1",
        "chinese_summary": "\u968f\u7740\u9884\u8bad\u7ec3\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\uff0c\u5728\u4f4e\u5ef6\u8fdf\u6216\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u8fd0\u884c\u8fd9\u4e9b\u5927\u578b\u6a21\u578b\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u4f2a\u6807\u7b7e\u6280\u672f\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5c06Whisper\u6a21\u578b\u84b8\u998f\u6210\u4e00\u4e2a\u66f4\u5c0f\u7684\u53d8\u4f53\uff0c\u79f0\u4e3aDistil-Whisper\u3002\u901a\u8fc7\u7b80\u5355\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6211\u4eec\u4ec5\u9009\u62e9\u6700\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\u3002\u84b8\u998f\u540e\u7684\u6a21\u578b\u5728\u53c2\u6570\u51cf\u5c1151%\u7684\u60c5\u51b5\u4e0b\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e865.8\u500d\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u5206\u5e03\u5916\u6d4b\u8bd5\u6570\u636e\u7684WER\u8868\u73b0\u4ec5\u4e0b\u964d1%\u3002Distil-Whisper\u4fdd\u6301\u4e86Whisper\u6a21\u578b\u5bf9\u56f0\u96be\u58f0\u5b66\u6761\u4ef6\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u957f\u97f3\u9891\u4e0a\u66f4\u4e0d\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u9519\u8bef\u3002Distil-Whisper\u8bbe\u8ba1\u7528\u4e8e\u4e0eWhisper\u6a21\u578b\u914d\u5bf9\u8fdb\u884c\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u5b9e\u73b0\u4e862\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u5728\u6570\u5b66\u4e0a\u786e\u4fdd\u4e0e\u539f\u59cb\u6a21\u578b\u8f93\u51fa\u76f8\u540c\u7684\u7ed3\u679c\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u5c06\u8bad\u7ec3\u4ee3\u7801\u3001\u63a8\u7406\u4ee3\u7801\u548c\u6a21\u578b\u516c\u5f00\u53d1\u5e03\u3002",
        "tldr_en": "We distill Whisper into a smaller, faster model called Distil-Whisper, which is 5.8 times faster with 51% fewer parameters, maintains robustness, and can be paired with Whisper for a 2x speed-up in speculative decoding.",
        "tldr_zh": "\u901a\u8fc7\u4f2a\u6807\u7b7e\u6784\u5efa\u5927\u89c4\u6a21\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5c06Whisper\u6a21\u578b\u84b8\u998f\u4e3a\u66f4\u5c0f\u3001\u66f4\u5feb\u7684Distil-Whisper\u6a21\u578b\uff0c\u6027\u80fd\u63a5\u8fd1\u4e14\u901f\u5ea6\u63d0\u53475.8\u500d\uff0c\u53c2\u6570\u51cf\u5c1151%\uff0c\u9002\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"
    },
    {
        "title": "The Synergy of Speculative Decoding and Batching in Serving Large Language Models",
        "summary": "Large Language Models (LLMs) like GPT are state-of-the-art text generation models that provide significant assistance in daily routines. However, LLM execution is inherently sequential, since they only produce one token at a time, thus incurring low hardware utilization on modern GPUs. Batching and speculative decoding are two techniques to improve GPU hardware utilization in LLM inference. To study their synergy, we implement a prototype implementation and perform an extensive characterization analysis on various LLM models and GPU architectures. We observe that the optimal speculation length depends on the batch size used. We analyze the key observation and build a quantitative model to explain it. Based on our analysis, we propose a new adaptive speculative decoding strategy that chooses the optimal speculation length for different batch sizes. Our evaluations show that our proposed method can achieve equal or better performance than the state-of-the-art speculation decoding schemes with fixed speculation length.",
        "authors": "Qidong Su, Christina Giannoula, Gennady Pekhimenko",
        "published": "2023-10-28",
        "link": "http://arxiv.org/abs/2310.18813v1",
        "chinese_summary": "\u50cfGPT\u8fd9\u6837\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5148\u8fdb\u7684\u6587\u672c\u751f\u6210\u6a21\u578b\uff0c\u4e3a\u65e5\u5e38\u5de5\u4f5c\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u5e2e\u52a9\u3002\u7136\u800c\uff0cLLM\u7684\u6267\u884c\u672c\u8d28\u4e0a\u662f\u6709\u5e8f\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e00\u6b21\u53ea\u80fd\u751f\u6210\u4e00\u4e2a\u6807\u8bb0\uff0c\u56e0\u6b64\u5728\u73b0\u4ee3GPU\u4e0a\u7684\u786c\u4ef6\u5229\u7528\u7387\u8f83\u4f4e\u3002\u6279\u5904\u7406\u548c\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e24\u79cd\u63d0\u9ad8LLM\u63a8\u7406\u4e2dGPU\u786c\u4ef6\u5229\u7528\u7387\u7684\u6280\u672f\u3002\u4e3a\u4e86\u7814\u7a76\u5b83\u4eec\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u539f\u578b\uff0c\u5e76\u5bf9\u5404\u79cdLLM\u6a21\u578b\u548cGPU\u67b6\u6784\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7279\u6027\u5206\u6790\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u6700\u4f73\u7684\u63a8\u6d4b\u957f\u5ea6\u53d6\u51b3\u4e8e\u6240\u4f7f\u7528\u7684\u6279\u6b21\u5927\u5c0f\u3002\u6211\u4eec\u5206\u6790\u4e86\u8fd9\u4e00\u5173\u952e\u89c2\u5bdf\u7ed3\u679c\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9a\u91cf\u6a21\u578b\u6765\u89e3\u91ca\u5b83\u3002\u57fa\u4e8e\u6211\u4eec\u7684\u5206\u6790\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u63a8\u6d4b\u6027\u89e3\u7801\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4e3a\u4e0d\u540c\u7684\u6279\u6b21\u5927\u5c0f\u9009\u62e9\u6700\u4f73\u7684\u63a8\u6d4b\u957f\u5ea6\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5177\u6709\u56fa\u5b9a\u63a8\u6d4b\u957f\u5ea6\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u65b9\u6848\u76f8\u7b49\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002",
        "tldr_en": "We propose an adaptive speculative decoding strategy for LLMs that optimizes GPU utilization by dynamically adjusting speculation length based on batch size, outperforming fixed-length schemes.",
        "tldr_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982GPT\u5728\u6587\u672c\u751f\u6210\u4e2d\u63d0\u4f9b\u663e\u8457\u5e2e\u52a9\uff0c\u4f46\u5176\u987a\u5e8f\u6267\u884c\u5bfc\u81f4GPU\u5229\u7528\u7387\u4f4e\uff0c\u901a\u8fc7\u6279\u5904\u7406\u548c\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u53ef\u63d0\u9ad8\u5229\u7528\u7387\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u7b56\u7565\uff0c\u6839\u636e\u4e0d\u540c\u6279\u6b21\u5927\u5c0f\u9009\u62e9\u6700\u4f73\u63a8\u6d4b\u957f\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u56fa\u5b9a\u63a8\u6d4b\u957f\u5ea6\u7684\u65b9\u6848\u3002"
    },
    {
        "title": "SpecTr: Fast Speculative Decoding via Optimal Transport",
        "summary": "Autoregressive sampling from large language models has led to state-of-the-art results in several natural language tasks. However, autoregressive sampling generates tokens one at a time making it slow, and even prohibitive in certain tasks. One way to speed up sampling is $\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$ (block or sequence of tokens), and then score all tokens in the draft by the large language model in parallel. A subset of the tokens in the draft are accepted (and the rest rejected) based on a statistical method to guarantee that the final output follows the distribution of the large model. In this work, we provide a principled understanding of speculative decoding through the lens of optimal transport (OT) with $\\textit{membership cost}$. This framework can be viewed as an extension of the well-known $\\textit{maximal-coupling}$ problem. This new formulation enables us to generalize the speculative decoding method to allow for a set of $k$ candidates at the token-level, which leads to an improved optimal membership cost. We show that the optimal draft selection algorithm (transport plan) can be computed via linear programming, whose best-known runtime is exponential in $k$. We then propose a valid draft selection algorithm whose acceptance probability is $(1-1/e)$-optimal multiplicatively. Moreover, it can be computed in time almost linear with size of domain of a single token. Using this $new draft selection$ algorithm, we develop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which provides speedup in decoding while ensuring that there is no quality degradation in the decoded output. We experimentally demonstrate that for state-of-the-art large language models, the proposed approach achieves a wall clock speedup of 2.13X, a further 1.37X speedup over speculative decoding on standard benchmarks.",
        "authors": "Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu",
        "published": "2023-10-23",
        "link": "http://arxiv.org/abs/2310.15141v2",
        "chinese_summary": "\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8fdb\u884c\u81ea\u56de\u5f52\u91c7\u6837\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u81ea\u56de\u5f52\u91c7\u6837\u4e00\u6b21\u751f\u6210\u4e00\u4e2a\u6807\u8bb0\uff0c\u8fd9\u4f7f\u5f97\u5b83\u901f\u5ea6\u7f13\u6162\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u53d8\u5f97\u4e0d\u53ef\u884c\u3002\u52a0\u901f\u91c7\u6837\u7684\u4e00\u79cd\u65b9\u6cd5\u662f$\\textit{\u63a8\u6d4b\u6027\u89e3\u7801}$\uff1a\u4f7f\u7528\u4e00\u4e2a\u5c0f\u6a21\u578b\u6765\u751f\u6210\u4e00\u4e2a$\\textit{\u8349\u7a3f}$\uff08\u6807\u8bb0\u5757\u6216\u5e8f\u5217\uff09\uff0c\u7136\u540e\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e76\u884c\u5730\u5bf9\u8349\u7a3f\u4e2d\u7684\u6240\u6709\u6807\u8bb0\u8fdb\u884c\u8bc4\u5206\u3002\u6839\u636e\u4e00\u79cd\u7edf\u8ba1\u65b9\u6cd5\uff0c\u8349\u7a3f\u4e2d\u7684\u4e00\u90e8\u5206\u6807\u8bb0\u88ab\u63a5\u53d7\uff08\u5176\u4f59\u88ab\u62d2\u7edd\uff09\uff0c\u4ee5\u4fdd\u8bc1\u6700\u7ec8\u8f93\u51fa\u9075\u5faa\u5927\u578b\u6a21\u578b\u7684\u5206\u5e03\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u4e0e$\\textit{\u6210\u5458\u6210\u672c}$\u7684\u89c6\u89d2\uff0c\u4e3a\u63a8\u6d4b\u6027\u89e3\u7801\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u7406\u89e3\u3002\u8fd9\u4e2a\u6846\u67b6\u53ef\u4ee5\u88ab\u89c6\u4e3a\u8457\u540d\u7684$\\textit{\u6700\u5927\u8026\u5408}$\u95ee\u9898\u7684\u6269\u5c55\u3002\u8fd9\u79cd\u65b0\u7684\u8868\u8ff0\u4f7f\u6211\u4eec\u80fd\u591f\u5c06\u63a8\u6d4b\u6027\u89e3\u7801\u65b9\u6cd5\u63a8\u5e7f\u5230\u5141\u8bb8\u5728\u6807\u8bb0\u7ea7\u522b\u4e0a\u6709\u4e00\u7ec4$k$\u4e2a\u5019\u9009\u8005\uff0c\u4ece\u800c\u5bfc\u81f4\u6539\u8fdb\u7684\u6700\u4f18\u6210\u5458\u6210\u672c\u3002\u6211\u4eec\u8868\u660e\uff0c\u6700\u4f18\u8349\u7a3f\u9009\u62e9\u7b97\u6cd5\uff08\u4f20\u8f93\u8ba1\u5212\uff09\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u8ba1\u7b97\uff0c\u5176\u5df2\u77e5\u7684\u6700\u4f18\u8fd0\u884c\u65f6\u95f4\u662f$k$\u7684\u6307\u6570\u7ea7\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8349\u7a3f\u9009\u62e9\u7b97\u6cd5\uff0c\u5176\u63a5\u53d7\u6982\u7387\u662f$(1-1/e)$\u500d\u7684\u6700\u4f18\u3002\u6b64\u5916\uff0c\u5b83\u53ef\u4ee5\u5728\u4e0e\u5355\u4e2a\u6807\u8bb0\u7684\u57df\u5927\u5c0f\u51e0\u4e4e\u6210\u7ebf\u6027\u7684\u65f6\u95f4\u5185\u8ba1\u7b97\u51fa\u6765\u3002\u4f7f\u7528\u8fd9\u79cd$\\textit{\u65b0\u7684\u8349\u7a3f\u9009\u62e9}$\u7b97\u6cd5\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u56de\u5f52\u91c7\u6837\u7b97\u6cd5\uff0c\u79f0\u4e3a$\\textit{SpecTr}$\uff0c\u5b83\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u52a0\u901f\uff0c\u540c\u65f6\u786e\u4fdd\u89e3\u7801\u8f93\u51fa\u7684\u8d28\u91cf\u6ca1\u6709\u4e0b\u964d\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e862.13\u500d\u7684\u6302\u949f\u52a0\u901f\uff0c\u6bd4\u63a8\u6d4b\u6027\u89e3\u7801\u8fdb\u4e00\u6b65\u52a0\u901f\u4e861.37\u500d\u3002",
        "tldr_en": "This work introduces SpecTr, an autoregressive sampling algorithm using speculative decoding with optimal transport, achieving significant speedup without quality degradation in large language models.",
        "tldr_zh": "\u901a\u8fc7\u6700\u4f18\u8fd0\u8f93\u7406\u8bba\u4f18\u5316\u63a8\u6d4b\u89e3\u7801\uff0c\u63d0\u51faSpecTr\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u56de\u5f52\u91c7\u6837\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002"
    },
    {
        "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation",
        "summary": "Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.",
        "authors": "Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran\u00e7ois Kagy, Rishabh Agarwal",
        "published": "2023-10-12",
        "link": "http://arxiv.org/abs/2310.08461v2",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\uff08Speculative Decoding, SD\uff09\u901a\u8fc7\u4f7f\u7528\u66f4\u5feb\u7684\u8349\u7a3f\u6a21\u578b\u751f\u6210\u591a\u4e2a\u6807\u8bb0\uff0c\u7136\u540e\u7531\u66f4\u5927\u7684\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\u8fd9\u4e9b\u6807\u8bb0\uff0c\u4ece\u800c\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u6700\u7ec8\u751f\u6210\u7b26\u5408\u76ee\u6807\u6a21\u578b\u5206\u5e03\u7684\u6587\u672c\u3002\u7136\u800c\uff0c\u8bc6\u522b\u4e00\u4e2a\u4e0e\u76ee\u6807\u6a21\u578b\u9ad8\u5ea6\u4e00\u81f4\u7684\u7d27\u51d1\u8349\u7a3f\u6a21\u578b\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DistillSpec\uff0c\u8be5\u65b9\u6cd5\u5728\u5e94\u7528SD\u4e4b\u524d\uff0c\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u66f4\u597d\u5730\u5c06\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u5bf9\u9f50\u3002DistillSpec\u505a\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u6211\u4eec\u901a\u8fc7\u7cfb\u7edf\u7684\u7814\u7a76\u8bc1\u660e\uff0c\u8fd9\u4e24\u4e2a\u9009\u62e9\u5bf9\u4e8e\u63d0\u9ad8\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u7684\u5bf9\u9f50\u5ea6\u81f3\u5173\u91cd\u8981\uff1a\u5229\u7528\u8349\u7a3f\u6a21\u578b\u751f\u6210\u7684\u7b56\u7565\u6570\u636e\uff0c\u4ee5\u53ca\u6839\u636e\u4efb\u52a1\u548c\u89e3\u7801\u7b56\u7565\u5b9a\u5236\u5dee\u5f02\u51fd\u6570\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cDistillSpec\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u8bba\u662f\u91c7\u7528\u8d2a\u5a6a\u91c7\u6837\u8fd8\u662f\u975e\u8d2a\u5a6a\u91c7\u6837\uff0c\u90fd\u6bd4\u6807\u51c6\u7684SD\u5b9e\u73b0\u4e86\u663e\u8457\u768410-45%\u7684\u901f\u5ea6\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06DistillSpec\u4e0e\u6709\u635fSD\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5ef6\u8fdf\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u6743\u8861\u7684\u7cbe\u7ec6\u63a7\u5236\u3002\u6700\u540e\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u9762\u5bf9\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b\uff0c\u9996\u5148\u901a\u8fc7\u84b8\u998f\u63d0\u5347\u76ee\u6807\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7136\u540e\u5e94\u7528DistillSpec\u8bad\u7ec3\u4e00\u4e2a\u9ad8\u5ea6\u5bf9\u9f50\u7684\u8349\u7a3f\u6a21\u578b\uff0c\u76f8\u6bd4\u4e0d\u4f7f\u7528\u84b8\u998f\u7684\u6807\u51c6\u89e3\u7801\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e6-10\u500d\uff0c\u540c\u65f6\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002",
        "tldr_en": "DistillSpec enhances speculative decoding by aligning draft models with target models through knowledge distillation, achieving significant speedups and fine-grained control over latency-performance trade-offs.",
        "tldr_zh": "DistillSpec \u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4f18\u5316\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u7684\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\u901f\u5ea6\uff0c\u5b9e\u73b010-45%\u7684\u52a0\u901f\uff0c\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u4e0b\u5b9e\u73b06-10\u500d\u7684\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e\u3002"
    },
    {
        "title": "MatFormer: Nested Transformer for Elastic Inference",
        "summary": "Transformer models are deployed in a wide range of settings, from multi-accelerator clusters to standalone mobile phones. The diverse inference constraints in these scenarios necessitate practitioners to train foundation models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes. Due to significant training costs, only a select few model sizes are trained and supported, limiting more fine-grained control over relevant tradeoffs, including latency, cost, and accuracy. This work introduces MatFormer, a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks. This training procedure allows for the Mix'n'Match of model granularities across layers -- i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models, which were never explicitly optimized. We empirically demonstrate MatFormer's effectiveness across different model classes (decoders & encoders), modalities (language & vision), and scales (up to 2.6B parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM) allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting comparable validation loss and one-shot downstream evaluations to their independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and consistent submodels extracted from MatFormer can further reduce inference latency.",
        "authors": "Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain",
        "published": "2023-10-11",
        "link": "http://arxiv.org/abs/2310.07707v1",
        "chinese_summary": "Transformer\u6a21\u578b\u88ab\u90e8\u7f72\u5728\u5404\u79cd\u73af\u5883\u4e2d\uff0c\u4ece\u591a\u52a0\u901f\u5668\u96c6\u7fa4\u5230\u72ec\u7acb\u7684\u79fb\u52a8\u8bbe\u5907\u3002\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u591a\u6837\u5316\u63a8\u7406\u7ea6\u675f\u8981\u6c42\u4ece\u4e1a\u8005\u8bad\u7ec3\u4e00\u7cfb\u5217\u4e0d\u540c\u5927\u5c0f\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5982PaLM 2\u3001Llama\u548cViTs\u3002\u7531\u4e8e\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u53ea\u6709\u5c11\u6570\u51e0\u4e2a\u6a21\u578b\u5927\u5c0f\u88ab\u8bad\u7ec3\u548c\u7ef4\u62a4\uff0c\u9650\u5236\u4e86\u5bf9\u5305\u62ec\u5ef6\u8fdf\u3001\u6210\u672c\u548c\u51c6\u786e\u6027\u5728\u5185\u7684\u76f8\u5173\u6743\u8861\u7684\u66f4\u7cbe\u7ec6\u63a7\u5236\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86MatFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u5d4c\u5957\u7684Transformer\u67b6\u6784\uff0c\u65e8\u5728\u5728\u5404\u79cd\u90e8\u7f72\u7ea6\u675f\u4e0b\u63d0\u4f9b\u5f39\u6027\u3002MatFormer\u6a21\u578b\u7684\u6bcf\u4e2a\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u5757\u90fd\u4e0e\u51e0\u4e2a\u5d4c\u5957\u7684\u8f83\u5c0fFFN\u5757\u5171\u540c\u4f18\u5316\u3002\u8fd9\u79cd\u8bad\u7ec3\u8fc7\u7a0b\u5141\u8bb8\u5728\u4e0d\u540c\u5c42\u4e4b\u95f4\u6df7\u5408\u5339\u914d\u6a21\u578b\u7c92\u5ea6\u2014\u2014\u5373\uff0c\u4e00\u4e2a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u901a\u7528MatFormer\u6a21\u578b\u80fd\u591f\u63d0\u53d6\u51fa\u6570\u767e\u4e2a\u4ece\u672a\u660e\u786e\u4f18\u5316\u8fc7\u7684\u8f83\u5c0f\u4f46\u51c6\u786e\u5ea6\u9ad8\u7684\u6a21\u578b\u3002\u6211\u4eec\u901a\u8fc7\u5b9e\u8bc1\u8bc1\u660e\u4e86MatFormer\u5728\u4e0d\u540c\u6a21\u578b\u7c7b\u522b\uff08\u89e3\u7801\u5668\u548c\u7f16\u7801\u5668\uff09\u3001\u6a21\u6001\uff08\u8bed\u8a00\u548c\u89c6\u89c9\uff09\u4ee5\u53ca\u89c4\u6a21\uff08\u9ad8\u8fbe26\u4ebf\u53c2\u6570\uff09\u4e0a\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e00\u4e2a26\u4ebf\u53c2\u6570\u7684\u4ec5\u89e3\u7801\u5668MatFormer\u8bed\u8a00\u6a21\u578b\uff08MatLM\uff09\u4f7f\u6211\u4eec\u80fd\u591f\u63d0\u53d6\u4ece15\u4ebf\u523026\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u5728\u9a8c\u8bc1\u635f\u5931\u548c\u4e00\u6b21\u6027\u4e0b\u6e38\u8bc4\u4f30\u4e2d\u90fd\u4e0e\u5176\u72ec\u7acb\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u4ece\u901a\u7528MatFormer\u57fa\u7840\u7684ViT\uff08MatViT\uff09\u7f16\u7801\u5668\u4e2d\u63d0\u53d6\u7684\u5c0f\u578b\u7f16\u7801\u5668\u4fdd\u7559\u4e86\u9002\u5e94\u5927\u89c4\u6a21\u68c0\u7d22\u7684\u5ea6\u91cf\u7a7a\u95f4\u7ed3\u6784\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4f7f\u7528\u4eceMatFormer\u4e2d\u63d0\u53d6\u7684\u51c6\u786e\u4e14\u4e00\u81f4\u7684\u5b50\u6a21\u578b\u8fdb\u884c\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002",
        "tldr_en": "MatFormer introduces a nested Transformer architecture enabling the extraction of hundreds of accurate smaller models from a single trained universal model, offering flexibility across various deployment constraints and reducing inference latency.",
        "tldr_zh": "MatFormer\u662f\u4e00\u79cd\u5d4c\u5957Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e0d\u540c\u5927\u5c0f\u7684\u524d\u9988\u7f51\u7edc\u5757\uff0c\u5b9e\u73b0\u6a21\u578b\u7c92\u5ea6\u7684\u7075\u6d3b\u7ec4\u5408\uff0c\u4ece\u800c\u5728\u591a\u79cd\u90e8\u7f72\u7ea6\u675f\u4e0b\u63d0\u4f9b\u5f39\u6027\uff0c\u5e76\u80fd\u63d0\u53d6\u51fa\u5927\u91cf\u672a\u5355\u72ec\u4f18\u5316\u7684\u8f83\u5c0f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002"
    },
    {
        "title": "Online Speculative Decoding",
        "summary": "Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding to address this challenge. The main idea is to continuously update the (multiple) draft model(s) on observed user query data. Adapting to query distribution mitigates the shifts between the training distribution of the draft model and the query distribution, enabling the draft model to more accurately predict the target model's outputs. We develop a prototype of online speculative decoding based on knowledge distillation and evaluate it using both synthetic and real query data. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, bringing 1.42x to 2.17x latency reduction. Our code is available at https://github.com/LiuXiaoxuanPKU/OSD.",
        "authors": "Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang",
        "published": "2023-10-11",
        "link": "http://arxiv.org/abs/2310.07177v4",
        "chinese_summary": "\u63a8\u6d4b\u6027\u89e3\u7801\u662f\u4e00\u79cd\u5173\u952e\u6280\u672f\uff0c\u901a\u8fc7\u4f7f\u7528\u8f83\u5c0f\u7684\u8349\u7a3f\u6a21\u578b\u6765\u9884\u6d4b\u76ee\u6807\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8349\u7a3f\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u8f83\u4f4e\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u591a\u6837\u5316\u7684\u6587\u672c\u8f93\u5165\u4ee5\u53ca\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u65f6\uff0c\u5176\u6548\u679c\u53ef\u80fd\u53d7\u5230\u9650\u5236\u3002\u6211\u4eec\u5f15\u5165\u4e86\u5728\u7ebf\u63a8\u6d4b\u6027\u89e3\u7801\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002\u5176\u4e3b\u8981\u601d\u60f3\u662f\u6301\u7eed\u5728\u89c2\u5bdf\u5230\u7684\u7528\u6237\u67e5\u8be2\u6570\u636e\u4e0a\u66f4\u65b0\uff08\u591a\u4e2a\uff09\u8349\u7a3f\u6a21\u578b\u3002\u9002\u5e94\u67e5\u8be2\u5206\u5e03\u53ef\u4ee5\u7f13\u89e3\u8349\u7a3f\u6a21\u578b\u7684\u8bad\u7ec3\u5206\u5e03\u4e0e\u67e5\u8be2\u5206\u5e03\u4e4b\u95f4\u7684\u504f\u79fb\uff0c\u4f7f\u8349\u7a3f\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u76ee\u6807\u6a21\u578b\u7684\u8f93\u51fa\u3002\u6211\u4eec\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u5f00\u53d1\u4e86\u5728\u7ebf\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u539f\u578b\uff0c\u5e76\u4f7f\u7528\u5408\u6210\u548c\u771f\u5b9e\u7684\u67e5\u8be2\u6570\u636e\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4ee4\u724c\u63a5\u53d7\u7387\u663e\u8457\u63d0\u9ad8\u4e860.1\u81f30.65\uff0c\u5e26\u6765\u4e861.42\u500d\u81f32.17\u500d\u7684\u5ef6\u8fdf\u51cf\u5c11\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/LiuXiaoxuanPKU/OSD\u83b7\u53d6\u3002",
        "tldr_en": "Online speculative decoding enhances speculative decoding by continuously updating draft models with user query data, significantly improving predictive accuracy and reducing latency by up to 2.17x.",
        "tldr_zh": "\u5728\u7ebf\u63a8\u6d4b\u89e3\u7801\u901a\u8fc7\u6301\u7eed\u66f4\u65b0\u8349\u7a3f\u6a21\u578b\u4ee5\u9002\u5e94\u7528\u6237\u67e5\u8be2\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"
    },
    {
        "title": "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding",
        "summary": "We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting. Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM. Moreover, the proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its variants demonstrated a speedup up to 1.99$\\times$.",
        "authors": "Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad Mehrotra",
        "published": "2023-09-15",
        "link": "http://arxiv.org/abs/2309.08168v2",
        "chinese_summary": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u65b9\u6848\u2014\u2014\u81ea\u63a8\u6d4b\u89e3\u7801\uff0c\u7528\u4e8e\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u800c\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u7684\u7279\u70b9\u662f\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u8349\u7a3f\u751f\u6210\u548c\u9a8c\u8bc1\u3002\u8349\u7a3f\u751f\u6210\u9636\u6bb5\u4ee5\u7a0d\u4f4e\u4f46\u66f4\u5feb\u7684\u8d28\u91cf\u751f\u6210\u8349\u7a3f\u6807\u8bb0\uff0c\u8fd9\u662f\u901a\u8fc7\u5728\u8349\u7a3f\u751f\u6210\u8fc7\u7a0b\u4e2d\u6709\u9009\u62e9\u5730\u8df3\u8fc7\u67d0\u4e9b\u4e2d\u95f4\u5c42\u6765\u5b9e\u73b0\u7684\u3002\u968f\u540e\uff0c\u9a8c\u8bc1\u9636\u6bb5\u4f7f\u7528\u539f\u59cbLLM\u5728\u4e00\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u9a8c\u8bc1\u8fd9\u4e9b\u8349\u7a3f\u8f93\u51fa\u6807\u8bb0\u3002\u8fd9\u4e00\u8fc7\u7a0b\u786e\u4fdd\u6700\u7ec8\u8f93\u51fa\u4e0e\u672a\u7ecf\u4fee\u6539\u7684LLM\u751f\u6210\u7684\u8f93\u51fa\u5b8c\u5168\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u9700\u8981\u989d\u5916\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u4e5f\u4e0d\u589e\u52a0\u989d\u5916\u7684\u5185\u5b58\u5360\u7528\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u63a8\u7406\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002\u5728LLaMA-2\u53ca\u5176\u53d8\u4f53\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u53ef\u8fbe1.99\u500d\u3002",
        "tldr_en": "We introduce self-speculative decoding, a two-stage inference acceleration method for LLMs that achieves up to 1.99$\\times$ speedup without auxiliary models, additional training, or memory overhead.",
        "tldr_zh": "\u63d0\u51fa\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\uff0c\u901a\u8fc7\u8349\u7a3f\u548c\u9a8c\u8bc1\u4e24\u9636\u6bb5\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u8fbe1.99\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u548c\u5185\u5b58\u5f00\u9500\u3002"
    },
    {
        "title": "Accelerating LLM Inference with Staged Speculative Decoding",
        "summary": "Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of speculative decoding. Taken together, we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L model while perfectly preserving output quality.",
        "authors": "Benjamin Spector, Chris Re",
        "published": "2023-08-08",
        "link": "http://arxiv.org/abs/2308.04623v1",
        "chinese_summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u5c55\u793a\u4e86\u5176\u591a\u6837\u5316\u7684\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b97\u6cd5\u2014\u2014\u5206\u9636\u6bb5\u63a8\u6d4b\u6027\u89e3\u7801\uff0c\u65e8\u5728\u52a0\u901fLLM\u5728\u5c0f\u6279\u91cf\u3001\u8bbe\u5907\u7aef\u573a\u666f\u4e2d\u7684\u63a8\u7406\u3002\u6211\u4eec\u901a\u8fc7\u6539\u8fdb\u5148\u524d\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u5de5\u4f5c\uff0c\u89e3\u51b3\u4e86\u5c0f\u6279\u91cf\u63a8\u7406\u7684\u4f4e\u7b97\u672f\u5f3a\u5ea6\u95ee\u9898\u3002\u9996\u5148\uff0c\u6211\u4eec\u5c06\u63a8\u6d4b\u6027\u6279\u91cf\u91cd\u6784\u4e3a\u6811\u7ed3\u6784\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u751f\u6210\u6210\u672c\u5e76\u589e\u52a0\u4e86\u6bcf\u6279\u6b21\u7684\u9884\u671ftoken\u6570\u91cf\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e8c\u9636\u6bb5\u7684\u63a8\u6d4b\u6027\u89e3\u7801\u3002\u7efc\u5408\u6765\u770b\uff0c\u6211\u4eec\u5728\u5b8c\u7f8e\u4fdd\u7559\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u4f7f\u7528762M\u53c2\u6570\u7684GPT-2-L\u6a21\u578b\u5c06\u5355\u6279\u6b21\u89e3\u7801\u5ef6\u8fdf\u51cf\u5c11\u4e863.16\u500d\u3002",
        "tldr_en": "We introduce staged speculative decoding to accelerate small-batch, on-device LLM inference, reducing latency by 3.16x with GPT-2-L while maintaining output quality.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u9636\u6bb5\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u63a8\u6d4b\u89e3\u7801\u7ed3\u6784\u548c\u589e\u52a0\u7b2c\u4e8c\u9636\u6bb5\u89e3\u7801\uff0c\u5728\u5c0f\u6279\u91cf\u3001\u8bbe\u5907\u7aef\u573a\u666f\u4e0b\u5c06762M\u53c2\u6570GPT-2-L\u6a21\u578b\u7684\u5355\u6279\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e\u4e863.16\u500d\uff0c\u540c\u65f6\u5b8c\u7f8e\u4fdd\u7559\u8f93\u51fa\u8d28\u91cf\u3002"
    },
    {
        "title": "Speculative Decoding with Big Little Decoder",
        "summary": "The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment and makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially without leveraging token-level parallelization. To address this, we propose Big Little Decoder (BiLD), a framework that can improve inference efficiency and latency for a wide range of text generation applications. The BiLD framework contains two models with different sizes that collaboratively generate text. The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally to refine the small model's inaccurate predictions in a non-autoregressive manner. To coordinate the small and large models, BiLD introduces two simple yet effective policies: (1) the fallback policy that determines when to hand control over to the large model; and (2) the rollback policy that determines when the large model needs to correct the small model's inaccurate predictions. To evaluate our framework across different tasks and models, we apply BiLD to various text generation scenarios encompassing machine translation on IWSLT 2017 De-En and WMT 2014 De-En, and summarization on XSUM and CNN/DailyMail. On an NVIDIA T4 GPU, our framework achieves a speedup of up to 2.12x speedup with minimal generation quality degradation. Furthermore, our framework is fully plug-and-play and can be applied without any modifications in the training process or model architecture. Our code is open-sourced",
        "authors": "Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, Kurt Keutzer",
        "published": "2023-02-15",
        "link": "http://arxiv.org/abs/2302.07863v4",
        "chinese_summary": "\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u51fa\u73b0\uff0c\u6781\u5927\u5730\u63a8\u52a8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u8f83\u957f\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u90e8\u7f72\uff0c\u5e76\u4f7f\u5f97\u5b83\u4eec\u5728\u5404\u79cd\u5b9e\u65f6\u5e94\u7528\u4e2d\u53d8\u5f97\u8fc7\u4e8e\u6602\u8d35\u3002\u7531\u4e8e\u81ea\u56de\u5f52\u751f\u6210\u4efb\u52a1\u9700\u8981\u6a21\u578b\u8fed\u4ee3\u8fd0\u884c\u4ee5\u987a\u5e8f\u751f\u6210\u6807\u8bb0\uff0c\u800c\u65e0\u6cd5\u5229\u7528\u6807\u8bb0\u7ea7\u522b\u7684\u5e76\u884c\u5316\uff0c\u56e0\u6b64\u63a8\u7406\u5ef6\u8fdf\u8fdb\u4e00\u6b65\u52a0\u5267\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Big Little Decoder\uff08BiLD\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u9ad8\u5e7f\u6cdb\u6587\u672c\u751f\u6210\u5e94\u7528\u7684\u63a8\u7406\u6548\u7387\u548c\u5ef6\u8fdf\u3002BiLD\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b\uff0c\u5b83\u4eec\u534f\u540c\u751f\u6210\u6587\u672c\u3002\u5c0f\u6a21\u578b\u4ee5\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u6587\u672c\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u63a8\u7406\u6210\u672c\uff0c\u800c\u5927\u6a21\u578b\u5219\u5076\u5c14\u4ee5\u975e\u81ea\u56de\u5f52\u65b9\u5f0f\u8c03\u7528\uff0c\u4ee5\u4fee\u6b63\u5c0f\u6a21\u578b\u7684\u4e0d\u51c6\u786e\u9884\u6d4b\u3002\u4e3a\u4e86\u534f\u8c03\u5c0f\u6a21\u578b\u548c\u5927\u6a21\u578b\uff0cBiLD\u5f15\u5165\u4e86\u4e24\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u7b56\u7565\uff1a\uff081\uff09\u56de\u9000\u7b56\u7565\uff0c\u51b3\u5b9a\u4f55\u65f6\u5c06\u63a7\u5236\u6743\u4ea4\u7ed9\u5927\u6a21\u578b\uff1b\uff082\uff09\u56de\u6eda\u7b56\u7565\uff0c\u51b3\u5b9a\u5927\u6a21\u578b\u4f55\u65f6\u9700\u8981\u4fee\u6b63\u5c0f\u6a21\u578b\u7684\u4e0d\u51c6\u786e\u9884\u6d4b\u3002\u4e3a\u4e86\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u6846\u67b6\uff0c\u6211\u4eec\u5c06BiLD\u5e94\u7528\u4e8e\u5404\u79cd\u6587\u672c\u751f\u6210\u573a\u666f\uff0c\u5305\u62ecIWSLT 2017 De-En\u548cWMT 2014 De-En\u7684\u673a\u5668\u7ffb\u8bd1\uff0c\u4ee5\u53caXSUM\u548cCNN/DailyMail\u7684\u6458\u8981\u751f\u6210\u3002\u5728NVIDIA T4 GPU\u4e0a\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u9ad82.12\u500d\u7684\u52a0\u901f\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u51e0\u4e4e\u6ca1\u6709\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u662f\u5b8c\u5168\u5373\u63d2\u5373\u7528\u7684\uff0c\u53ef\u4ee5\u5728\u4e0d\u4fee\u6539\u8bad\u7ec3\u8fc7\u7a0b\u6216\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u5e94\u7528\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5f00\u6e90\u3002",
        "tldr_en": "The BiLD framework improves text generation efficiency by using small and large models collaboratively, achieving up to 2.12x speedup with minimal quality loss.",
        "tldr_zh": "\u63d0\u51faBig Little Decoder\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u5c0f\u6a21\u578b\u534f\u4f5c\u751f\u6210\u6587\u672c\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u751f\u6210\u5e94\u7528\u7684\u63a8\u7406\u6548\u7387\u548c\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u6700\u9ad82.12\u500d\u52a0\u901f\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u8bad\u7ec3\u8fc7\u7a0b\u6216\u6a21\u578b\u67b6\u6784\u3002"
    },
    {
        "title": "Fast Inference from Transformers via Speculative Decoding",
        "summary": "Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.",
        "authors": "Yaniv Leviathan, Matan Kalman, Yossi Matias",
        "published": "2022-11-30",
        "link": "http://arxiv.org/abs/2211.17192v2",
        "chinese_summary": "\u50cfTransformer\u8fd9\u6837\u7684\u5927\u578b\u81ea\u56de\u5f52\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u2014\u2014\u89e3\u7801K\u4e2atoken\u9700\u8981\u6a21\u578b\u8fdb\u884cK\u6b21\u4e32\u884c\u8fd0\u884c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u63a8\u6d4b\u6027\u89e3\u7801\u2014\u2014\u4e00\u79cd\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u591a\u4e2atoken\u6765\u52a0\u901f\u81ea\u56de\u5f52\u6a21\u578b\u91c7\u6837\u7684\u7b97\u6cd5\uff0c\u4e14\u65e0\u9700\u5bf9\u8f93\u51fa\u8fdb\u884c\u4efb\u4f55\u66f4\u6539\u3002\u6211\u4eec\u65b9\u6cd5\u7684\u6838\u5fc3\u5728\u4e8e\u4ee5\u4e0b\u89c2\u5bdf\uff1a(1) \u590d\u6742\u7684\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u901a\u5e38\u5305\u542b\u53ef\u4ee5\u901a\u8fc7\u66f4\u9ad8\u6548\u6a21\u578b\u8f83\u597d\u8fd1\u4f3c\u7684\u7b80\u5355\u5b50\u4efb\u52a1\uff1b(2) \u901a\u8fc7\u4f7f\u7528\u63a8\u6d4b\u6267\u884c\u548c\u4e00\u79cd\u65b0\u9896\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u8fd1\u4f3c\u6a21\u578b\u7684\u8f93\u51fa\u4e0a\u5e76\u884c\u8fd0\u884c\u5927\u578b\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u786e\u89e3\u7801\u7684\u52a0\u901f\uff0c\u53ef\u80fd\u540c\u65f6\u751f\u6210\u591a\u4e2atoken\uff0c\u4e14\u4e0d\u6539\u53d8\u5206\u5e03\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u67b6\u6784\u8c03\u6574\u5373\u53ef\u52a0\u901f\u73b0\u6709\u7684\u73b0\u6210\u6a21\u578b\u3002\u6211\u4eec\u5728T5-XXL\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u6807\u51c6\u7684T5X\u5b9e\u73b0\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e862\u500d\u52303\u500d\u7684\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u5b8c\u5168\u76f8\u540c\u3002",
        "tldr_en": "We introduce speculative decoding, a method to accelerate autoregressive model inference by parallelizing token generation using approximation models, achieving 2X-3X speedup without altering outputs.",
        "tldr_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u6d4b\u6027\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u591a\u4e2a\u6807\u8bb0\uff0c\u5728\u4e0d\u6539\u53d8\u8f93\u51fa\u7ed3\u679c\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5927\u578b\u81ea\u56de\u5f52\u6a21\u578b\uff08\u5982Transformer\uff09\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e862-3\u500d\u7684\u52a0\u901f\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3002"
    },
    {
        "title": "Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation",
        "summary": "We propose Speculative Decoding (SpecDec), for the first time ever, to formally study exploiting the idea of speculative execution to accelerate autoregressive (AR) decoding. Speculative Decoding has two innovations: Spec-Drafter -- an independent model specially optimized for efficient and accurate drafting -- and Spec-Verification -- a reliable method for verifying the drafted tokens efficiently in the decoding paradigm. Experimental results on various seq2seq tasks including machine translation and abstractive summarization show our approach can achieve around $5\\times$ speedup for the popular Transformer architectures with comparable generation quality to beam search decoding, refreshing the impression that the draft-then-verify paradigm introduces only $1.4\\times$$\\sim$$2\\times$ speedup. In addition to the remarkable speedup, we also demonstrate 3 additional advantages of SpecDec, revealing its practical value for accelerating generative models in real-world applications. Our models and codes are available at https://github.com/hemingkx/SpecDec.",
        "authors": "Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, Zhifang Sui",
        "published": "2022-03-30",
        "link": "http://arxiv.org/abs/2203.16487v6",
        "chinese_summary": "\u6211\u4eec\u9996\u6b21\u63d0\u51fa\u4e86\u63a8\u6d4b\u6027\u89e3\u7801\uff08Speculative Decoding, SpecDec\uff09\uff0c\u6b63\u5f0f\u7814\u7a76\u5229\u7528\u63a8\u6d4b\u6267\u884c\u7684\u601d\u60f3\u6765\u52a0\u901f\u81ea\u56de\u5f52\uff08AR\uff09\u89e3\u7801\u3002\u63a8\u6d4b\u6027\u89e3\u7801\u5305\u542b\u4e24\u9879\u521b\u65b0\uff1aSpec-Drafter\u2014\u2014\u4e00\u4e2a\u4e13\u95e8\u4f18\u5316\u7528\u4e8e\u9ad8\u6548\u4e14\u51c6\u786e\u8d77\u8349\u7684\u72ec\u7acb\u6a21\u578b\uff0c\u4ee5\u53caSpec-Verification\u2014\u2014\u4e00\u79cd\u5728\u89e3\u7801\u8303\u5f0f\u4e2d\u9ad8\u6548\u9a8c\u8bc1\u8d77\u8349\u4ee4\u724c\u7684\u53ef\u9760\u65b9\u6cd5\u3002\u5728\u5305\u62ec\u673a\u5668\u7ffb\u8bd1\u548c\u62bd\u8c61\u6458\u8981\u5728\u5185\u7684\u5404\u79cd\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u4e0e\u675f\u641c\u7d22\u89e3\u7801\u76f8\u5f53\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u4e3a\u6d41\u884c\u7684Transformer\u67b6\u6784\u5b9e\u73b0\u7ea65\u500d\u7684\u52a0\u901f\uff0c\u5237\u65b0\u4e86\u8d77\u8349-\u9a8c\u8bc1\u8303\u5f0f\u4ec5\u80fd\u5e26\u67651.4\u500d\u81f32\u500d\u52a0\u901f\u7684\u5370\u8c61\u3002\u9664\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86SpecDec\u76843\u4e2a\u989d\u5916\u4f18\u52bf\uff0c\u63ed\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u52a0\u901f\u751f\u6210\u6a21\u578b\u7684\u5b9e\u7528\u4ef7\u503c\u3002\u6211\u4eec\u7684\u6a21\u578b\u548c\u4ee3\u7801\u53ef\u5728https://github.com/hemingkx/SpecDec\u83b7\u53d6\u3002",
        "tldr_en": "We introduce Speculative Decoding (SpecDec) to accelerate autoregressive decoding by up to 5x with comparable quality, featuring innovative drafting and verification methods.",
        "tldr_zh": "\u6211\u4eec\u9996\u6b21\u63d0\u51fa\u5229\u7528\u63a8\u6d4b\u6267\u884c\u52a0\u901f\u81ea\u56de\u5f52\u89e3\u7801\u7684\u63a8\u6d4b\u89e3\u7801\uff08SpecDec\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u63a8\u6d4b\u8d77\u8349\uff08Spec-Drafter\uff09\u548c\u63a8\u6d4b\u9a8c\u8bc1\uff08Spec-Verification\uff09\u65b9\u6cd5\uff0c\u5728\u673a\u5668\u7ffb\u8bd1\u548c\u6458\u8981\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7ea65\u500d\u52a0\u901f\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u4e0e\u675f\u641c\u7d22\u76f8\u5f53\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u52a0\u901f\u751f\u6210\u6a21\u578b\u7684\u5de8\u5927\u6f5c\u529b\u3002"
    }
]