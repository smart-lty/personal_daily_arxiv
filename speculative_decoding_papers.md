| Title | Authors | Published | Link | TL, DR(英文) | TL, DR(中文) |
| --- | --- | --- | --- | --- | --- |
| [Dynamic Depth Decoding: Faster Speculative Decoding for LLMs](http://arxiv.org/abs/2409.00142v1) | Oscar Brown, Zhengjie Wang, Andrea Do, Nikhil Mathew, Cheng Yu | 2024-08-30 | [Link](http://arxiv.org/abs/2409.00142v1) | Dynamic Depth Decoding (DDD) enhances EAGLE-2's speculative decoding, achieving a 44% higher average speedup of 3.16x without accuracy loss. | 动态深度解码（DDD）通过优化EAGLE-2的动态草稿树方法，将平均加速比提升44%，达到3.16倍。 |
| [Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation](http://arxiv.org/abs/2408.15562v1) | Lujun Gui, Bin Xiao, Lei Su, Weipeng Chen | 2024-08-28 | [Link](http://arxiv.org/abs/2408.15562v1) | FSPAD enhances lossless speculative decoding by sampling target LLM features and introducing partial alignment distillation, outperforming state-of-the-art methods across various tasks and models. | FSPAD通过引入特征采样和部分对齐蒸馏，优化了无损推测解码框架，显著提升了目标大语言模型推理性能，并在多项任务中超越了现有最先进方法。 |
| [The Mamba in the Llama: Distilling and Accelerating Hybrid Models](http://arxiv.org/abs/2408.15237v1) | Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao | 2024-08-27 | [Link](http://arxiv.org/abs/2408.15237v1) | We demonstrate the feasibility of distilling large Transformers into efficient linear RNNs, achieving superior performance and faster inference with limited resources, outperforming both open-source models and GPT-4 in benchmarks. | 通过重用注意力层的线性投影权重，我们展示了在学术GPU资源下，将大型Transformer模型蒸馏成线性RNN模型的可行性，生成的混合模型在聊天基准测试中表现与原Transformer相当，并在一般基准测试中超越从头训练的开源混合Mamba模型，同时引入硬件感知的推测解码算法加速推理速度。 |
| [MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding](http://arxiv.org/abs/2408.11049v3) | Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen | 2024-08-20 | [Link](http://arxiv.org/abs/2408.11049v3) | MagicDec demonstrates that speculative decoding can achieve significant speedup for high throughput inference on long sequences, even with large batch sizes, by optimizing bottleneck shifts and leveraging sparse KV cache. | MagicDec 通过智能草稿策略和稀疏 KV 缓存，在长序列和高吞吐量推理中显著提升了推测解码的速度，实现了高达 2 倍的加速。 |
| [Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling](http://arxiv.org/abs/2408.08696v1) | Xianzhen Luo, Yixuan Wang, Qingfu Zhu, Zhiming Zhang, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che | 2024-08-16 | [Link](http://arxiv.org/abs/2408.08696v1) | Token Recycling accelerates large language model inference by reusing candidate tokens in a BFS-like draft tree, requiring <2MB storage and achieving up to 2x speedup, outperforming existing methods. | Token Recycling 是一种无损加速大型语言模型推理的方法，通过重用解码过程中的候选词并构建草稿树，实现了约2倍的速度提升，且无需额外训练或适应现有模型和任务。 |
| [KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning](http://arxiv.org/abs/2408.08146v1) | Kaiqi Zhang, Jing Zhao, Rui Chen | 2024-08-15 | [Link](http://arxiv.org/abs/2408.08146v1) | KOALA, a multi-layer adversarial learning architecture, significantly enhances speculative decoding by improving draft head accuracy and reducing latency by up to 14.09%. | KOALA通过多层优化对抗学习架构显著提升推测解码中草稿头的预测精度，虽增加少量开销，但大幅提升解码效率，实现0.24x-0.41x的延迟加速比提升。 |
| [Coupling without Communication and Drafter-Invariant Speculative Decoding](http://arxiv.org/abs/2408.07978v2) | Majid Daliri, Christopher Musco, Ananda Theertha Suresh | 2024-08-15 | [Link](http://arxiv.org/abs/2408.07978v2) | Alice and Bob can achieve a high probability of matching samples without communication, using public randomness, with a bound of $\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$, and can match optimal coupling with $O(\log(n/\epsilon))$ bits of communication. | 在没有通信的情况下，Alice和Bob利用公共随机性仍能达到$Pr[a=b] \geq \frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$的概率匹配，且存在通信方案以$O(\log(n/\epsilon))$比特实现近似最优耦合。 |
| [Parallel Speculative Decoding with Adaptive Draft Length](http://arxiv.org/abs/2408.11850v2) | Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, Winston Hu | 2024-08-13 | [Link](http://arxiv.org/abs/2408.11850v2) | PEARL introduces adaptive draft length and parallel verification to enhance speculative decoding, significantly reducing mutual waiting and achieving superior speedup in LLM inference. | 我们提出了一种名为PEARL的并行自适应推测解码框架，通过预验证和后验证策略，有效缓解了现有推测解码方法中的相互等待问题，并在文本生成任务中实现了高达3.79倍和1.52倍的速度提升。 |
| [A Decoding Acceleration Framework for Industrial Deployable LLM-based Recommender Systems](http://arxiv.org/abs/2408.05676v1) | Yunjia Xi, Hangyu Wang, Bo Chen, Jianghao Lin, Menghui Zhu, Weiwen Liu, Ruiming Tang, Weinan Zhang, Yong Yu | 2024-08-11 | [Link](http://arxiv.org/abs/2408.05676v1) | This paper introduces DARE, a decoding acceleration framework for LLM-based recommendation systems, which enhances retrieval efficiency and draft token acceptance rate, achieving a 3-5x speedup and successful deployment in online advertising. | 本文提出了一种基于LLM的推荐系统解码加速框架DARE，通过定制检索池和宽松验证机制，实现了3-5倍的加速，并在大规模商业环境中成功部署，保持了下游性能。 |
| [Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion](http://arxiv.org/abs/2408.05636v2) | Jacob K Christopher, Brian R Bartoldson, Bhavya Kailkhura, Ferdinando Fioretto | 2024-08-10 | [Link](http://arxiv.org/abs/2408.05636v2) | This paper introduces Speculative Diffusion Decoding (SpecDiff), a novel adaptation of speculative decoding using discrete diffusion models to parallelize both drafting and verification steps, achieving up to 8.7x speed-up in language generation compared to standard methods and up to 2.5x over existing speculative decoding techniques. | 本文提出了一种基于离散扩散模型的推测解码方法SpecDiff，通过并行化草稿生成和验证步骤，显著提升了大语言模型推理速度，在标准语言生成基准测试中实现了高达8.7倍的标准生成加速和2.5倍的现有推测解码加速。 |
| [CREST: Effectively Compacting a Datastore For Retrieval-Based Speculative Decoding](http://arxiv.org/abs/2408.04678v1) | Sophia Ho, Jinsol Park, Patrick Wang | 2024-08-08 | [Link](http://arxiv.org/abs/2408.04678v1) | CREST is a compacted version of REST that reduces storage by 10.6-13.5x and improves performance by 16.5-17.1% on benchmarks. | CREST通过存储最常见的小型n-gram子集，显著减少了存储空间并提升了性能，相比REST在相同存储空间下提高了16.5-17.1%的接受长度。 |
| [Clover-2: Accurate Inference for Regressive Lightweight Speculative Decoding](http://arxiv.org/abs/2408.00264v1) | Bin Xiao, Lujun Gui, Lei Su, Weipeng Chen | 2024-08-01 | [Link](http://arxiv.org/abs/2408.00264v1) | Clover-2, an advanced RNN-based draft model, significantly improves text generation efficiency and accuracy by leveraging knowledge distillation and enhanced architecture, outperforming existing methods across diverse model architectures. | 大型语言模型（LLMs）因自回归解码需求与现代GPU架构不匹配而效率低下，近期回归轻量推测解码因其显著提升文本生成任务效率而受到关注。Clover-2作为基于RNN的轻量草稿模型的迭代，通过增强架构和知识蒸馏，在保持低计算开销的同时，实现了与注意力解码层模型相当的精度，并在实验中超越了现有方法，展示了其有效性和鲁棒性。 |
| [Graph-Structured Speculative Decoding](http://arxiv.org/abs/2407.16207v1) | Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan | 2024-07-23 | [Link](http://arxiv.org/abs/2407.16207v1) | Graph-structured Speculative Decoding (GSD) accelerates Large Language Models by efficiently managing and merging recurring token sequences using a directed acyclic graph, achieving up to 1.96$\times$ speedup. | 图结构推测解码（GSD）通过利用有向无环图（DAG）高效预测和合并重复的令牌序列，显著提升了大语言模型（LLM）推理速度，相较于标准推测解码实现了1.73倍至1.96倍的加速。 |
| [Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference](http://arxiv.org/abs/2407.09722v1) | Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun | 2024-07-12 | [Link](http://arxiv.org/abs/2407.09722v1) | Multi-token joint speculative decoding (MJSD) accelerates large language model inference with better output perplexity by approximating joint distribution and using verification with joint likelihood. | 多令牌联合推测解码（MJSD）通过近似大模型的联合分布并结合验证步骤，提高了输出困惑度并加速了推理过程，相较于传统的推测解码更具优势。 |
| [Accelerating the inference of string generation-based chemical reaction models for industrial applications](http://arxiv.org/abs/2407.09685v2) | Mikhail Andronov, Natalia Andronova, Michael Wand, Jürgen Schmidhuber, Djork-Arné Clevert | 2024-07-12 | [Link](http://arxiv.org/abs/2407.09685v2) | We accelerate template-free SMILES translation for reaction prediction and retrosynthesis by 3X via speculative decoding, maintaining accuracy. | 通过推测解码加速自回归SMILES生成器推理，实现反应预测和单步逆合成中3倍以上的速度提升，且不损失精度。 |
| [S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models](http://arxiv.org/abs/2407.01955v1) | Parsa Kavehzadeh, Mohammadreza Pourreza, Mojtaba Valipour, Tinashu Zhu, Haoli Bai, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh | 2024-07-02 | [Link](http://arxiv.org/abs/2407.01955v1) | This paper introduces a novel multi-target draft model deployment strategy and an efficient sorted speculative decoding mechanism that outperforms baselines in accelerating large language model inference across diverse target models. | 本文提出了一种多目标场景下更高效的排序推测解码机制，通过实验证明在多个目标模型同时运行时，其草稿模型性能优于基线模型。 |
| [SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding](http://arxiv.org/abs/2406.18200v1) | Zhenglin Wang, Jialong Wu, Yilong Lai, Congzhi Zhang, Deyu Zhou | 2024-06-26 | [Link](http://arxiv.org/abs/2406.18200v1) | SeeD optimizes runtime speed and GPU memory for efficient speculative decoding in large language models, outperforming traditional methods in reasoning tasks. | 大型语言模型在多任务中展现出色能力，但在复杂推理和规划任务上表现不足，基于树搜索的推理方法通过探索中间步骤超越了思维链提示，但引入了显著的推理延迟。本文提出SeeD，一种高效推理框架，通过调度推测执行优化运行速度和GPU内存管理，实验证明在三个推理数据集上显著加速，为无训练推测解码中的批量推理提供了可行路径。 |
| [Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training](http://arxiv.org/abs/2406.17404v1) | Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che | 2024-06-25 | [Link](http://arxiv.org/abs/2406.17404v1) | We propose the Make Some Noise (MSN) training framework and TR-Jacobi decoding strategy to enhance parallel decoding and inference speed of large language models without additional model structure, achieving up to 2.7x faster inference with comparable performance to state-of-the-art models. | 提出Make Some Noise (MSN)训练框架和基于树的检索增强Jacobi (TR-Jacobi)解码策略，显著提升大语言模型并行解码能力，实验表明在不影响性能的情况下，推理速度提升2.3-2.7倍，加速比媲美SOTA模型。 |
| [OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure](http://arxiv.org/abs/2406.17276v2) | Jikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang | 2024-06-25 | [Link](http://arxiv.org/abs/2406.17276v2) | OPT-Tree algorithm constructs adaptive draft trees for speculative decoding, significantly accelerating autoregressive language models by up to 3.2x and enabling multi-token generation per step. | OPT-Tree算法通过构建自适应可扩展的草稿树结构，实现了在推理过程中最大化接受长度的优化，显著提升了自回归语言模型的推理效率，最高加速比达3.2倍。 |
| [Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters](http://arxiv.org/abs/2406.16758v1) | Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun | 2024-06-24 | [Link](http://arxiv.org/abs/2406.16758v1) | This paper introduces a speculative decoding approach with language-specific draft models, significantly reducing inference time in multilingual settings by leveraging targeted pretrain-and-finetune strategies. | 本文通过优化多语言场景下的推测解码训练方法，显著提升了大语言模型在商业应用中的推理速度。 |
| [Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](http://arxiv.org/abs/2406.14066v2) | Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang | 2024-06-20 | [Link](http://arxiv.org/abs/2406.14066v2) | SmartSpec dynamically adjusts speculation length based on system load and accuracy to reduce LLM inference latency up to 3.2x across various models and workloads. | SmartSpec 动态调整推测解码长度，基于系统负载和推测准确性优化大型语言模型推理延迟，相比非推测解码基线最多可降低3.2倍延迟。 |
| [Amphista: Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style](http://arxiv.org/abs/2406.13170v1) | Zeping Li, Xinlong Yang, Ziheng Gao, Ji Liu, Zhuang Liu, Dong Li, Jinzhang Peng, Lu Tian, Emad Barsoum | 2024-06-19 | [Link](http://arxiv.org/abs/2406.13170v1) | Amphista introduces a non-autoregressive speculative decoding algorithm, enhancing inference efficiency through increased parallelism and feature fusion, achieving significant speedups in Vicuna models without compromising generation quality. | 我们提出Amphista算法，通过非自回归解码实现并行推理，显著提升推理效率，实验表明在Vicuna 33B模型上最高可实现2.75倍加速，且保持生成质量无损。 |
| [Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding](http://arxiv.org/abs/2406.12295v1) | Kaiyan Zhang, Jianyu Wang, Ning Ding, Biqing Qi, Ermo Hua, Xingtai Lv, Bowen Zhou | 2024-06-18 | [Link](http://arxiv.org/abs/2406.12295v1) | This paper introduces Fast and Slow Generating (FS-GEN), a unified framework leveraging collaborative decoding between Large Language Models (LLMs) and Small Language Models (SLMs) to address LLMs' drawbacks, revealing that less than 20% of interactions are needed for effective collaboration, primarily in areas of high uncertainty in token prediction. | 大型语言模型（LLMs）在多样应用中表现出色，但面临推理延迟高、训练成本昂贵及生成幻觉等问题。基于双过程认知理论，我们提出“快慢生成”（FS-GEN）框架，整合大模型与小模型（SLMs）的协同解码，探索推测解码、对比解码及模拟器微调等技术，揭示协同交互仅需不到20%，且遵循参数比例的缩放规律，有效提升预测准确性。 |
| [When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models](http://arxiv.org/abs/2406.07368v2) | Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Yingyan Celine Lin | 2024-06-11 | [Link](http://arxiv.org/abs/2406.07368v2) | We comprehensively study linear attention methods for autoregressive LLMs, integrating them with speculative decoding to enhance efficiency, achieving up to 6.67 perplexity reduction and 2× speedup in generation. | 自回归大型语言模型（LLMs）在语言任务中表现出色，但面临注意力模块二次复杂度和生成时顺序处理效率低下的瓶颈。我们首次全面研究现有线性注意力方法对自回归LLMs的有效性，并结合推测解码，提出确保与推测解码兼容的线性注意力增强技术，显著提升LLMs的训练和服务效率，实验验证了其有效性，实现了困惑度降低6.67和生成速度提升2倍。 |
| [Proofread: Fixes All Errors with One Tap](http://arxiv.org/abs/2406.04523v1) | Renjie Liu, Yanxiang Zhang, Yun Zhu, Haicheng Sun, Yuanbo Zhang, Michael Xuelin Huang, Shanqing Cai, Lei Meng, Shumin Zhai | 2024-06-06 | [Link](http://arxiv.org/abs/2406.04523v1) | This paper introduces Proofread, a Gboard feature leveraging a server-side LLM for seamless sentence and paragraph corrections, optimized through a two-stage tuning process and deployed on Pixel 8 devices with high user engagement and reduced latency. | 本文介绍了一种基于大型语言模型（LLM）的新型Gboard功能Proofread，通过单次点击实现句子和段落级别的无缝校正，并详细描述了从数据生成、指标设计到模型调优和部署的完整系统流程。 |
| [Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson Sampling Control Mechanism](http://arxiv.org/abs/2406.03853v1) | Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai | 2024-06-06 | [Link](http://arxiv.org/abs/2406.03853v1) | We propose Early-exiting Speculative Decoding (EESD) with lossless acceleration to address the high inference costs of large language models, leveraging a segment of the LLM to generate draft tokens and a novel sampling mechanism to significantly speed up token generation while maintaining output quality. | 提出了一种名为早期退出推测解码（EESD）的无损加速方法，通过利用大型语言模型（LLM）的部分生成草稿令牌，结合早期退出结构和自蒸馏方法，显著降低部署和训练成本并加速令牌生成速度，同时引入基于汤普森采样的采样机制，确保最终输出文本与传统自回归解码分布一致，实验结果表明该方法在13B和70B模型上显著优于现有方法。 |
| [SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices](http://arxiv.org/abs/2406.02532v2) | Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin | 2024-06-04 | [Link](http://arxiv.org/abs/2406.02532v2) | This work introduces SpecExec, a parallel decoding method enabling efficient inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading, achieving up to 6 tokens per second with 4-bit quantization. | 本研究提出了一种在消费级GPU上高效运行大型语言模型（LLM）的并行解码方法SpecExec，通过推测解码技术，实现了在内存卸载条件下对500亿参数以上模型的推理加速，达到每秒4-6个token的处理速度。 |
| [S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs](http://arxiv.org/abs/2405.20314v2) | Wei Zhong, Manasa Bharadwaj | 2024-05-30 | [Link](http://arxiv.org/abs/2405.20314v2) | We propose Skippy Simultaneous Speculative Decoding (S3D), a cost-effective self-speculative method achieving top performance-memory ratios with minimal changes, enabling faster Phi-3-based SD models that outperform quantized EAGLE models in speed and VRAM usage. | 我们提出了Skippy Simultaneous Speculative Decoding (S3D)，一种基于同时多令牌解码和中层跳过的成本效益高的自推测解码方法，相比现有开源系统，在性能-内存比上表现优异，且仅需最小架构改动和训练数据，基于Phi-3的模型比量化EAGLE模型快1.4至2倍，使用半精度且VRAM消耗更少。 |
| [SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths](http://arxiv.org/abs/2405.19715v2) | Kaixuan Huang, Xudong Guo, Mengdi Wang | 2024-05-30 | [Link](http://arxiv.org/abs/2405.19715v2) | SpecDec++ enhances speculative decoding by adaptively determining candidate length via a Markov Decision Process-based threshold policy, achieving significant speedups across datasets. | 我们提出了一种自适应确定候选长度的增强版推测解码方法SpecDec++，通过理论分析和实验验证，实现了在多个数据集上显著的推理加速效果。 |
| [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](http://arxiv.org/abs/2405.19325v2) | Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin | 2024-05-29 | [Link](http://arxiv.org/abs/2405.19325v2) | NEST introduces a novel semi-parametric approach for faster, more accurate, and attributed language model generations by integrating real-world text spans and enhancing inference speed. | 本文提出了一种名为Nearest Neighbor Speculative Decoding (NEST)的新型半参数语言模型方法，通过在推理步骤中进行令牌级检索，显著提升了生成质量和归属率，并在生成速度上实现了1.8倍的加速。 |
| [Faster Cascades via Speculative Decoding](http://arxiv.org/abs/2405.19261v1) | Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, Sanjiv Kumar | 2024-05-29 | [Link](http://arxiv.org/abs/2405.19261v1) | This paper introduces speculative cascading, a novel technique that combines the benefits of cascades and speculative decoding by using speculative execution for deferral rules, achieving superior cost-quality trade-offs in language model inference. | 本文通过结合级联和推测解码的优点，设计了新的推测级联技术，实现了通过推测执行的延迟规则，实验表明其在语言任务中比传统级联和推测解码方法提供了更好的成本-质量权衡。 |
| [Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference](http://arxiv.org/abs/2405.18628v2) | Hao Mark Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko, Stylianos I. Venieris, Hongxiang Fan | 2024-05-28 | [Link](http://arxiv.org/abs/2405.18628v2) | We introduce a novel parallel prompt decoding (PPD) technique for Large Language Models (LLMs) that significantly reduces training costs and memory overhead, achieving up to 2.49× speedup and 28% higher acceptance rates for long-range predictions, with potential for further enhancements when combined with existing speculative decoding methods. | 我们提出了一种仅需0.0002%可训练参数的新型并行提示解码方法，显著提升了大语言模型在硬件性能上的效率，同时保持了低内存消耗和高接受率。 |
| [Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation](http://arxiv.org/abs/2405.15842v1) | Boyuan Chen, Mingzhi Zhu, Brendan Dolan-Gavitt, Muhammad Shafique, Siddharth Garg | 2024-05-24 | [Link](http://arxiv.org/abs/2405.15842v1) | We propose a novel model cascading strategy for code completion that reduces computational costs and increases accuracy by using test case execution results as cascading thresholds, optimizing the cost-accuracy trade-off for large language models in code generation. | 我们提出了一种模型级联策略，通过让每个模型生成并执行测试用例来评估代码完成质量，从而在降低计算成本的同时提高准确性，优化了大语言模型在代码生成任务中的成本-准确性权衡。 |
| [A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models](http://arxiv.org/abs/2405.13019v2) | Mahsa Khoshnoodi, Vinija Jain, Mingye Gao, Malavika Srikanth, Aman Chadha | 2024-05-15 | [Link](http://arxiv.org/abs/2405.13019v2) | This paper surveys accelerated text generation techniques in autoregressive language models, categorizing them into speculative decoding, early exiting, and non-autoregressive methods to guide future NLP research. | 本文综述了自回归语言模型中加速文本生成技术，涵盖推测解码、早期退出机制和非自回归方法，旨在探讨最新技术及其应用，并为未来研究提供指导。 |
| [EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models](http://arxiv.org/abs/2405.07542v1) | Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, Yunhe Wang | 2024-05-13 | [Link](http://arxiv.org/abs/2405.07542v1) | We introduce a novel speculative decoding method that eliminates padding tokens and maintains consistent speedup without increasing memory or computational overhead. | 我们提出了一种无需增加计算和内存开销即可解决多样本推测解码中接受令牌数量不一致问题的新方法，并通过实验验证了其有效性。 |
| [Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models](http://arxiv.org/abs/2405.04304v4) | Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy Schwartz | 2024-05-07 | [Link](http://arxiv.org/abs/2405.04304v4) | DISCO dynamically optimizes speculation lookahead (SL) for reducing inference latency, achieving a 10% average speedup over static SL baselines with identical text output. | 动态推测前瞻优化（DISCO）通过动态选择推测前瞻值，相比静态前瞻方法平均提速10%，且生成相同文本。 |
| [Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge](http://arxiv.org/abs/2405.00263v1) | Bin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin Cui | 2024-05-01 | [Link](http://arxiv.org/abs/2405.00263v1) | Clover, a new speculative decoding algorithm, integrates sequential knowledge to enhance parallel decoding efficiency, outperforming existing methods on Baichuan models. | 本文提出了一种名为Clover的新型推测解码算法，通过整合顺序知识到并行解码过程中，提高了推测解码的命中率，从而显著提升了生成效率，实验结果显示其在多个模型上均优于现有方法。 |
| [Accelerating Production LLMs with Combined Token/Embedding Speculators](http://arxiv.org/abs/2404.19124v2) | Davis Wertheimer, Joshua Rosenkranz, Thomas Parnell, Sahil Suneja, Pavithra Ranganathan, Raghu Ganti, Mudhakar Srivatsa | 2024-04-29 | [Link](http://arxiv.org/abs/2404.19124v2) | This report details the design and training of speculative decoding models to accelerate large language model inference by up to 2-3x through efficient n-gram prediction and acceptance/rejection by the base model. | 本技术报告介绍了一种新型推测解码草稿模型的设计和训练方法，通过结合上下文向量和采样令牌，有效预测高质量n-gram，使基础模型在生产环境中推理速度提升2-3倍，并探讨了进一步优化的方向。 |
| [Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting](http://arxiv.org/abs/2404.18911v1) | Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, Yunhe Wang | 2024-04-29 | [Link](http://arxiv.org/abs/2404.18911v1) | We propose Kangaroo, a self-speculative decoding framework using a fixed shallow sub-network and an adapter module to accelerate large language model inference, achieving up to 1.68x speedup with 88.7% fewer parameters compared to Medusa-1. | 提出了一种名为Kangaroo的自推测解码框架，通过固定浅层子网络作为自草稿模型，结合轻量适配器模块和早期退出机制，显著提升了大语言模型推理速度，实验表明在Spec-Bench上最高可实现1.68倍加速，且参数效率优于Medusa-1。 |
| [LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding](http://arxiv.org/abs/2404.16710v2) | Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu | 2024-04-25 | [Link](http://arxiv.org/abs/2404.16710v2) | LayerSkip accelerates large language model inference via layer dropout training, self-speculative decoding, and shared compute, achieving up to 2.16x speedup across tasks. | 我们提出了LayerSkip，一种端到端加速大型语言模型推理的解决方案，通过训练时的层丢弃和早期退出损失，以及推理时的自推测解码方法，实现了在不增加额外层或模块的情况下提高早期退出准确性，并在多个任务上实现了显著的加速效果。 |
| [BASS: Batched Attention-optimized Speculative Sampling](http://arxiv.org/abs/2404.15778v2) | Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, Anoop Deoras | 2024-04-24 | [Link](http://arxiv.org/abs/2404.15778v2) | This paper introduces batched speculative decoding, achieving state-of-the-art multi-sequence generation latency with 2.15X speed-up, superior GPU utilization, and higher quality within a time budget. | 本文提出了一种批量推测解码系统，显著提升了多序列生成的延迟性能，实现了高达2.15倍的加速，并在GPU利用率和生成质量方面表现卓越。 |
| [Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models](http://arxiv.org/abs/2404.14897v1) | Chen Zhang, Zhuorui Liu, Dawei Song | 2024-04-23 | [Link](http://arxiv.org/abs/2404.14897v1) | This survey paper provides a comprehensive review and taxonomy of speculative execution techniques in large language models (LLMs) to enhance decoding efficiency, highlighting current challenges and future directions. | 随着因果大语言模型（LLMs）规模不断扩大，推理效率成为核心关注点之一，其中延迟瓶颈尤为重要，源于LLMs的自回归特性。为缓解此瓶颈，引入计算机架构领域的推测执行概念，采用“草拟-验证”方式，通过并行验证加速LLM解码。近年来，相关研究迅速增长，但缺乏系统综述。本文首次全面综述LLMs中的推测执行技术，提出系统分类，并分析当前进展及未来挑战。 |
| [Parallel Decoding via Hidden Transfer for Lossless Large Language Model Acceleration](http://arxiv.org/abs/2404.12022v1) | Pengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, Dongyan Zhao | 2024-04-18 | [Link](http://arxiv.org/abs/2404.12022v1) | We introduce a novel parallel decoding approach called "hidden transfer" that decodes multiple tokens simultaneously in a single forward pass, leveraging pseudo hidden states and a tree attention mechanism to enhance predictive accuracy and generation efficiency, outperforming existing acceleration techniques. | 本文提出了一种名为“隐藏转移”的并行解码方法，通过在单次前向传递中同时解码多个连续令牌，并结合新颖的树注意力机制生成和验证多个候选输出序列，显著提升了大语言模型在推理过程中的效率和准确性。 |
| [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](http://arxiv.org/abs/2404.11912v3) | Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen | 2024-04-18 | [Link](http://arxiv.org/abs/2404.11912v3) | TriForce introduces a scalable hierarchical speculative decoding system for efficient long-sequence generation, achieving up to 2.31$\times$ speedup on Llama2-7B-128K with robust performance across various conditions. | TriForce 是一种分层推测解码系统，通过利用原始模型权重和动态稀疏 KV 缓存，显著提升了长序列生成的效率，实现了高达 2.31 倍的加速，并在处理更长上下文时展示了可扩展性。 |
| [On Speculative Decoding for Multimodal Large Language Models](http://arxiv.org/abs/2404.08856v1) | Mukul Gagrani, Raghavv Goel, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott | 2024-04-13 | [Link](http://arxiv.org/abs/2404.08856v1) | This paper demonstrates that speculative decoding with a language-only model can significantly speed up multimodal large language model inference, achieving up to 2.37$\times$ memory-bound speedup, and introduces a compact LLaVA draft model with an image adapter for marginal performance gains in image captioning. | 本文通过探索推测解码技术，显著提升了多模态大语言模型LLaVA 7B的推理效率，实现了高达2.37倍的内存带宽加速，并引入了一个包含图像适配器的紧凑草稿模型，在图像描述任务中略有性能提升。 |
| [DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference](http://arxiv.org/abs/2404.00242v2) | Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin | 2024-03-30 | [Link](http://arxiv.org/abs/2404.00242v2) | DeFT introduces an IO-aware tree attention algorithm for efficient tree-structured inference, significantly reducing KV cache IO and achieving up to 3.82x speedup in attention latency. | DeFT是一种面向树结构推理的IO感知树注意力算法，通过优化QKV分组和注意力计算，显著提升树结构解码效率，减少KV缓存IO和部分结果IO，实现高达2.52/3.82倍的端到端/注意力延迟加速。 |
| [SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens](http://arxiv.org/abs/2403.18647v2) | Chengbo Liu, Yong Zhu | 2024-03-27 | [Link](http://arxiv.org/abs/2403.18647v2) | We propose Speculative Decoding with Semantic Adaptive Tokens (SDSAT) to accelerate large language models, achieving up to 3.5X speed-up without accuracy loss. | 我们提出了一种通过语义自适应令牌（SDSAT）进行推测解码的加速方案，旨在提高大型语言模型（LLM）生成草稿令牌的准确性，同时保持模型精度，实验结果显示在CodeLlama-13B和7B模型上分别实现了3.5倍和3倍的加速。 |
| [Block Verification Accelerates Speculative Decoding](http://arxiv.org/abs/2403.10444v2) | Ziteng Sun, Uri Mendlovic, Yaniv Leviathan, Asaf Aharoni, Ahmad Beirami, Jae Hun Ro, Ananda Theertha Suresh | 2024-03-15 | [Link](http://arxiv.org/abs/2403.10444v2) | Block Verification optimizes speculative decoding by jointly verifying entire token blocks, achieving consistent 5%-8% wall-clock speedups with no performance degradation. | 投机解码通过块验证实现无损加速，比标准逐词验证提供5%-8%的持续提速，且不增加代码复杂性，可作为默认方法。 |
| [Recurrent Drafter for Fast Speculative Decoding in Large Language Models](http://arxiv.org/abs/2403.09919v3) | Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng | 2024-03-14 | [Link](http://arxiv.org/abs/2403.09919v3) | We present an enhanced speculative decoding method using a single, lightweight draft head with recurrent dependencies, combining simplicity and efficiency without the complexities of full transformer architectures or data-dependent tree structures. | 本文提出了一种结合经典双模型和近期单模型Medusa优势的改进型推测解码方法，通过使用单一轻量级草稿头和循环依赖设计，实现了高效的大语言模型服务，避免了复杂的数据依赖树注意力结构。 |
| [Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs](http://arxiv.org/abs/2403.00858v4) | Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott | 2024-02-29 | [Link](http://arxiv.org/abs/2403.00858v4) | This paper introduces a draft model training framework for accelerating LLM inference via speculative decoding, achieving significant speed-up and efficiency without additional alignment procedures. | 本文提出了一种简单的草稿模型训练框架，通过直接对齐聊天能力目标模型，训练出仅占原模型1.64%大小的Llama 2 Chat Drafter 115M，结合推测解码技术，实现了高达2.4倍的推理加速，且无需进一步任务特定微调。 |
| [Minions: Accelerating Large Language Model Inference with Adaptive and Collective Speculative Decoding](http://arxiv.org/abs/2402.15678v1) | Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng, Xin You, Yongjun Bao, Yi Liu, Zhongzhi Luan, Depei Qian | 2024-02-24 | [Link](http://arxiv.org/abs/2402.15678v1) | Minions is an LLM inference system that accelerates decoding through collective, adaptive speculative generation using multiple small models, optimizing performance without excessive verification costs. | 本文提出了一种名为Minions的LLM推理系统，通过集体自适应的推测生成机制，利用多数投票机制联合多个小型推测模型（SSM）来加速LLM推理，动态调整推测长度以平衡推测与验证成本，并通过流水线执行机制进一步优化推理性能，显著提升推理吞吐量和降低推理时间。 |
| [Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement](http://arxiv.org/abs/2402.14160v2) | Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher Lott | 2024-02-21 | [Link](http://arxiv.org/abs/2402.14160v2) | Recursive Speculative Decoding (RSD) enhances tree-based speculative decoding by maximizing tree diversity and efficiency, outperforming baselines under both fixed sequence length and computational budgets. | 递归推测解码（RSD）通过无放回抽样和最大化树多样性，在固定计算预算下显著提升大型语言模型（LLM）的推理加速效果。 |
| [Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding](http://arxiv.org/abs/2402.13720v2) | Weilin Zhao, Yuxiang Huang, Xu Han, Wang Xu, Chaojun Xiao, Xinrong Zhang, Yewei Fang, Kaihuo Zhang, Zhiyuan Liu, Maosong Sun | 2024-02-21 | [Link](http://arxiv.org/abs/2402.13720v2) | Ouroboros enhances speculative decoding by generating longer drafts without training, achieving up to 2.4x speedup over speculative and 3.9x over vanilla decoding. | 投机解码通过使用较小模型起草并由目标大语言模型验证，在不降低性能的情况下加速生成过程，而Ouroboros通过生成更长的草稿以无训练方式提升解码效率，实验表明其相比投机解码和普通解码分别提速2.4倍和3.9倍。 |
| [Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding](http://arxiv.org/abs/2402.12374v2) | Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, Beidi Chen | 2024-02-19 | [Link](http://arxiv.org/abs/2402.12374v2) | Sequoia introduces a scalable, robust, and hardware-aware speculative decoding algorithm that significantly boosts inference speed across various models and hardware platforms. | 本文提出了一种可扩展、鲁棒且硬件感知的推测解码算法Sequoia，通过动态规划优化推测树结构，采用新型采样与验证方法提升性能，并引入硬件感知树优化器自动选择树大小和深度，显著提升了Llama2和Vicuna模型在A100和L40硬件上的解码速度。 |
| [Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding](http://arxiv.org/abs/2402.11809v3) | Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, Rong Xiao | 2024-02-19 | [Link](http://arxiv.org/abs/2402.11809v3) | SPACE accelerates large language models by integrating semi-autoregressive inference and speculative decoding, achieving up to 4.0x speedup with no loss in quality. | 本研究提出智能并行自动校正解码（SPACE）方法，通过半自回归推理和推测解码，实现大型语言模型（LLMs）的无损加速，实验显示在HumanEval-X上推理速度提升2.7x-4.0x，同时保持输出质量。 |
| [Speculative Streaming: Fast LLM Inference without Auxiliary Models](http://arxiv.org/abs/2402.11131v1) | Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, Mahyar Najibi | 2024-02-16 | [Link](http://arxiv.org/abs/2402.11131v1) | Speculative Streaming is a single-model speculative decoding method that accelerates inference by 1.8-3.1X across diverse tasks without quality loss, using significantly fewer parameters than traditional approaches. | 我们提出了一种单一模型的推测解码方法——推测流（Speculative Streaming），通过将草稿生成融入目标模型，改变了微调目标从下一个词预测到未来n-gram预测，从而在多种任务中实现了1.8-3.1倍的解码加速，且不牺牲生成质量，同时具有极高的参数效率，适用于资源受限设备。 |
| [Tandem Transformers for Inference Efficient LLMs](http://arxiv.org/abs/2402.08644v3) | Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli | 2024-02-13 | [Link](http://arxiv.org/abs/2402.08644v3) | Tandem transformers enhance inference speed and accuracy by combining a small autoregressive model with a large block-mode model, achieving significant improvements in next-token prediction and downstream task performance. | Tandem transformers架构通过结合小自回归模型和大块处理模型，显著提升推理速度和预测准确性，并在PaLM2数据集上实现1.16倍加速和3.3%的准确性提升。 |
| [Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding](http://arxiv.org/abs/2402.05109v1) | Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, William Brandon | 2024-02-07 | [Link](http://arxiv.org/abs/2402.05109v1) | Hydra heads enhance speculative decoding accuracy and throughput by introducing sequentially dependent draft heads, outperforming standard draft heads and traditional autoregressive decoding. | 提出顺序依赖的Hydra头作为标准草稿头的替代，显著提升推测准确性，并通过Hydra++优化设计，使解码吞吐量分别比Medusa解码和自回归解码提高1.31倍和2.71倍。 |
| [Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation](http://arxiv.org/abs/2403.06988v1) | Luca Beurer-Kellner, Marc Fischer, Martin Vechev | 2024-02-07 | [Link](http://arxiv.org/abs/2403.06988v1) | DOMINO introduces a novel decoding algorithm that enforces subword-aligned constraints with minimal overhead, significantly outperforming existing methods in both speed and accuracy. | DOMINO算法通过全子词对齐约束解码，实现无性能开销且在某些情况下加速近两倍，显著优于现有方法。 |
| [GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding](http://arxiv.org/abs/2402.02082v1) | Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, Yang You | 2024-02-03 | [Link](http://arxiv.org/abs/2402.02082v1) | GliDe and CaPE enhance speculative decoding by optimizing draft model architecture and candidate token selection, significantly reducing LLM decoding latency up to 2.61x. | 本研究提出GliDe和CaPE两种改进方法，通过优化推测解码框架中的草稿模型架构和候选词选择，显著提升冻结大型语言模型的解码速度，实验证明GliDe可加速Vicuna模型达2.17倍，结合CaPE可提升至2.61倍，并将公开代码、数据及训练模型。 |
| [Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](http://arxiv.org/abs/2402.02057v1) | Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang | 2024-02-03 | [Link](http://arxiv.org/abs/2402.02057v1) | Lookahead decoding introduces an exact, parallel algorithm to accelerate large language model decoding without auxiliary models, trading per-step computation for fewer steps and achieving up to 4x speedup on multi-GPU code completion tasks. | 本文提出了一种无需辅助模型或数据存储的精确并行解码算法Lookahead decoding，通过减少解码步骤和提高并行性，显著加速了大语言模型的自回归解码，最高可提速1.8倍。 |
| [Decoding Speculative Decoding](http://arxiv.org/abs/2402.01528v3) | Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman | 2024-02-02 | [Link](http://arxiv.org/abs/2402.01528v3) | Speculative Decoding accelerates Large Language Models by using hardware-efficient draft models, significantly boosting throughput without compromising quality. | 推测性解码通过使用小型草稿模型生成推测性标记并由目标大型语言模型验证，显著加速了大型语言模型的推理过程，且不降低质量。研究发现，草稿模型的延迟对推测性解码性能影响重大，而其语言建模能力与其在推测性解码中的表现关联不大。基于此，我们设计了硬件高效的草稿模型，新设计的LLaMA-65B草稿模型比现有模型吞吐量提高111%，并能推广至LLaMA-2系列和监督微调模型。 |
| [MambaByte: Token-free Selective State Space Model](http://arxiv.org/abs/2401.13660v3) | Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush | 2024-01-24 | [Link](http://arxiv.org/abs/2401.13660v3) | MambaByte leverages the Mamba state space model for efficient token-free language modeling, outperforming subword Transformers and achieving a 2.6x inference speedup with speculative decoding. | MambaByte通过引入固定内存状态和高效解码的Mamba状态空间模型，实现了无令牌语言模型的创新，在语言建模任务中表现优异且推理速度显著提升。 |
| [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](http://arxiv.org/abs/2401.10774v3) | Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao | 2024-01-19 | [Link](http://arxiv.org/abs/2401.10774v3) | Medusa enhances LLM inference by adding parallel decoding heads and a tree-based attention mechanism, achieving up to 3.6x speedup without compromising quality. | 本文提出了一种名为Medusa的高效方法，通过增加额外的解码头并利用树状注意力机制，在并行预测多个后续token的同时，显著减少了大型语言模型（LLM）推理中的解码步骤，实现了2.2倍以上的加速，且不损失生成质量。 |
| [Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding](http://arxiv.org/abs/2401.07851v3) | Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui | 2024-01-15 | [Link](http://arxiv.org/abs/2401.07851v3) | This paper introduces Speculative Decoding, a novel parallel decoding approach for accelerating Large Language Models by efficiently drafting and verifying multiple tokens per step, and aims to foster further research in this area. | 本文综述并分析了新型解码范式“推测解码”，旨在通过高效并行验证未来词元，加速大型语言模型推理，推动更高效的LLM推理研究。 |
| [APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding](http://arxiv.org/abs/2401.06761v1) | Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, Yuxiao Dong | 2024-01-12 | [Link](http://arxiv.org/abs/2401.06761v1) | We introduce a parallel auto-regressive generation method (APAR) that significantly speeds up LLM text generation, achieving up to 4x speedup with speculative decoding and improving throughput and latency in high-throughput scenarios. | 引入并行自回归生成方法，通过指令微调实现高效的大语言模型部署，显著提升生成速度和资源利用率。 |
| [Multi-Candidate Speculative Decoding](http://arxiv.org/abs/2401.06706v1) | Sen Yang, Shujian Huang, Xinyu Dai, Jiajun Chen | 2024-01-12 | [Link](http://arxiv.org/abs/2401.06706v1) | This paper enhances speculative decoding by sampling multiple draft model candidates and batch-verifying them, significantly improving acceptance rates across datasets and models. | 本文提出了一种多候选采样和批量验证的推测解码加速方法，显著提高了接受率，优于传统推测解码。 |
| [Cascade Speculative Drafting for Even Faster LLM Inference](http://arxiv.org/abs/2312.11462v4) | Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Kevin Chen-Chuan Chang, Jie Huang | 2023-12-18 | [Link](http://arxiv.org/abs/2312.11462v4) | Cascade Speculative Drafting (CS Drafting) enhances LLM inference efficiency by eliminating autoregressive generation and optimizing token time allocation, achieving up to 81% additional speedup. | 引入垂直和水平级联的Cascade Speculative Drafting算法，通过消除自回归生成和优化时间分配，显著提升大语言模型推理效率，实验中比传统推测解码快81%。 |
| [Speculative Contrastive Decoding](http://arxiv.org/abs/2311.08981v2) | Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou | 2023-11-15 | [Link](http://arxiv.org/abs/2311.08981v2) | Speculative Contrastive Decoding (SCD) enhances large language model performance by leveraging smaller models for accelerated and improved decoding. | 我们提出了一种名为“推测对比解码”（SCD）的简单而强大的解码方法，通过利用较小语言模型的预测来加速解码并提升质量，并在多项语言任务中验证了其有效性。 |
| [REST: Retrieval-Based Speculative Decoding](http://arxiv.org/abs/2311.08252v2) | Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, Di He | 2023-11-14 | [Link](http://arxiv.org/abs/2311.08252v2) | REST accelerates language model generation by leveraging retrieval for draft token creation, achieving up to 2.36X speedup without additional training. | REST算法通过检索生成草稿令牌，显著加速语言模型生成，无需额外训练，实现1.62X至2.36X的加速。 |
| [Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling](http://arxiv.org/abs/2311.00430v1) | Sanchit Gandhi, Patrick von Platen, Alexander M. Rush | 2023-11-01 | [Link](http://arxiv.org/abs/2311.00430v1) | We distill Whisper into a smaller, faster model called Distil-Whisper, which is 5.8 times faster with 51% fewer parameters, maintains robustness, and can be paired with Whisper for a 2x speed-up in speculative decoding. | 通过伪标签构建大规模开源数据集，将Whisper模型蒸馏为更小、更快的Distil-Whisper模型，性能接近且速度提升5.8倍，参数减少51%，适用于低延迟和资源受限环境。 |
| [The Synergy of Speculative Decoding and Batching in Serving Large Language Models](http://arxiv.org/abs/2310.18813v1) | Qidong Su, Christina Giannoula, Gennady Pekhimenko | 2023-10-28 | [Link](http://arxiv.org/abs/2310.18813v1) | We propose an adaptive speculative decoding strategy for LLMs that optimizes GPU utilization by dynamically adjusting speculation length based on batch size, outperforming fixed-length schemes. | 大型语言模型（LLMs）如GPT在文本生成中提供显著帮助，但其顺序执行导致GPU利用率低，通过批处理和推测解码技术可提高利用率，我们提出了一种自适应推测解码策略，根据不同批次大小选择最佳推测长度，性能优于现有固定推测长度的方案。 |
| [SpecTr: Fast Speculative Decoding via Optimal Transport](http://arxiv.org/abs/2310.15141v2) | Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu | 2023-10-23 | [Link](http://arxiv.org/abs/2310.15141v2) | This work introduces SpecTr, an autoregressive sampling algorithm using speculative decoding with optimal transport, achieving significant speedup without quality degradation in large language models. | 通过最优运输理论优化推测解码，提出SpecTr算法，显著提升大语言模型自回归采样速度，同时保持输出质量。 |
| [DistillSpec: Improving Speculative Decoding via Knowledge Distillation](http://arxiv.org/abs/2310.08461v2) | Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, Rishabh Agarwal | 2023-10-12 | [Link](http://arxiv.org/abs/2310.08461v2) | DistillSpec enhances speculative decoding by aligning draft models with target models through knowledge distillation, achieving significant speedups and fine-grained control over latency-performance trade-offs. | DistillSpec 通过知识蒸馏优化草稿模型与目标模型的对齐，显著提升推测解码（SD）速度，实现10-45%的加速，并在不同模型大小下实现6-10倍的解码延迟降低。 |
| [MatFormer: Nested Transformer for Elastic Inference](http://arxiv.org/abs/2310.07707v1) | Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain | 2023-10-11 | [Link](http://arxiv.org/abs/2310.07707v1) | MatFormer introduces a nested Transformer architecture enabling the extraction of hundreds of accurate smaller models from a single trained universal model, offering flexibility across various deployment constraints and reducing inference latency. | MatFormer是一种嵌套Transformer架构，通过联合优化不同大小的前馈网络块，实现模型粒度的灵活组合，从而在多种部署约束下提供弹性，并能提取出大量未单独优化的较小模型，显著提升效率和性能。 |
| [Online Speculative Decoding](http://arxiv.org/abs/2310.07177v4) | Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang | 2023-10-11 | [Link](http://arxiv.org/abs/2310.07177v4) | Online speculative decoding enhances speculative decoding by continuously updating draft models with user query data, significantly improving predictive accuracy and reducing latency by up to 2.17x. | 在线推测解码通过持续更新草稿模型以适应用户查询分布，显著提升了大语言模型推理的准确性和效率。 |
| [Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding](http://arxiv.org/abs/2309.08168v2) | Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad Mehrotra | 2023-09-15 | [Link](http://arxiv.org/abs/2309.08168v2) | We introduce self-speculative decoding, a two-stage inference acceleration method for LLMs that achieves up to 1.99$\times$ speedup without auxiliary models, additional training, or memory overhead. | 提出无需辅助模型的自推测解码方案，通过草稿和验证两阶段加速大语言模型，实现高达1.99倍的推理加速，且无需额外训练和内存开销。 |
| [Accelerating LLM Inference with Staged Speculative Decoding](http://arxiv.org/abs/2308.04623v1) | Benjamin Spector, Chris Re | 2023-08-08 | [Link](http://arxiv.org/abs/2308.04623v1) | We introduce staged speculative decoding to accelerate small-batch, on-device LLM inference, reducing latency by 3.16x with GPT-2-L while maintaining output quality. | 我们提出了一种新颖的分阶段推测解码算法，通过改进推测解码结构和增加第二阶段解码，在小批量、设备端场景下将762M参数GPT-2-L模型的单批解码延迟降低了3.16倍，同时完美保留输出质量。 |
| [Speculative Decoding with Big Little Decoder](http://arxiv.org/abs/2302.07863v4) | Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, Kurt Keutzer | 2023-02-15 | [Link](http://arxiv.org/abs/2302.07863v4) | The BiLD framework improves text generation efficiency by using small and large models collaboratively, achieving up to 2.12x speedup with minimal quality loss. | 提出Big Little Decoder框架，通过大小模型协作生成文本，显著提升文本生成应用的推理效率和延迟，实现最高2.12倍加速，且无需修改训练过程或模型架构。 |
| [Fast Inference from Transformers via Speculative Decoding](http://arxiv.org/abs/2211.17192v2) | Yaniv Leviathan, Matan Kalman, Yossi Matias | 2022-11-30 | [Link](http://arxiv.org/abs/2211.17192v2) | We introduce speculative decoding, a method to accelerate autoregressive model inference by parallelizing token generation using approximation models, achieving 2X-3X speedup without altering outputs. | 我们提出了一种推测性解码算法，通过并行计算多个标记，在不改变输出结果的前提下，显著加速了大型自回归模型（如Transformer）的推理过程，实现了2-3倍的加速，且无需重新训练或修改模型架构。 |
| [Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation](http://arxiv.org/abs/2203.16487v6) | Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, Zhifang Sui | 2022-03-30 | [Link](http://arxiv.org/abs/2203.16487v6) | We introduce Speculative Decoding (SpecDec) to accelerate autoregressive decoding by up to 5x with comparable quality, featuring innovative drafting and verification methods. | 我们首次提出利用推测执行加速自回归解码的推测解码（SpecDec），通过创新的推测起草（Spec-Drafter）和推测验证（Spec-Verification）方法，在机器翻译和摘要生成等任务中实现约5倍加速，且生成质量与束搜索相当，展示了其在实际应用中加速生成模型的巨大潜力。 |


### Dynamic Depth Decoding: Faster Speculative Decoding for LLMs
- **Authors**: Oscar Brown, Zhengjie Wang, Andrea Do, Nikhil Mathew, Cheng Yu
- **Published**: 2024-08-30
- **Link**: [http://arxiv.org/abs/2409.00142v1](http://arxiv.org/abs/2409.00142v1)
- **Summary**: The acceleration of Large Language Models (LLMs) with speculative decoding provides a significant runtime improvement without any loss of accuracy. Currently, EAGLE-2 is the state-of-the-art speculative decoding method, improving on EAGLE with a dynamic draft tree. We introduce Dynamic Depth Decoding (DDD), which optimises EAGLE-2's tree drafting method using a dynamic depth. This extends the average speedup that EAGLE-2 achieves over EAGLE by $44\%$, giving DDD an average speedup of $3.16$x.

- **中文摘要**: 使用推测解码加速大型语言模型（LLMs）在保持准确性的同时显著提升了运行时性能。目前，EAGLE-2是推测解码领域的最新技术，它在EAGLE的基础上引入了动态草稿树。我们提出了动态深度解码（Dynamic Depth Decoding, DDD），该方法通过动态深度优化了EAGLE-2的树草稿方法。这使得DDD在EAGLE-2基础上实现了平均44%的加速，总体平均加速达到了3.16倍。

### Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation
- **Authors**: Lujun Gui, Bin Xiao, Lei Su, Weipeng Chen
- **Published**: 2024-08-28
- **Link**: [http://arxiv.org/abs/2408.15562v1](http://arxiv.org/abs/2408.15562v1)
- **Summary**: Lossless speculative decoding accelerates target large language model (LLM) inference by employing a lightweight draft model for generating tree-structured candidates, which are subsequently verified in parallel by the target LLM. Currently, effective approaches leverage feature-level rather than token-level autoregression within the draft model to facilitate more straightforward predictions and enhanced knowledge distillation. In this paper, we reassess these approaches and propose FSPAD (Feature Sampling and Partial Alignment Distillation for Lossless Speculative Decoding), which introduces two straightforward and effective components within the existing framework to boost lossless speculative decoding. Firstly, FSPAD utilizes token embeddings to sample features of the target LLM in high-dimensional space before feeding them into the draft model, due to the inherent uncertainty of the features preventing the draft model from obtaining the specific token output by the target LLM. Secondly, FSPAD introduces partial alignment distillation to weaken the draft model's connection between features and logits, aiming to reduce the conflict between feature alignment and logit confidence during training. Our experiments include both greedy and non-greedy decoding on the largest and smallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in multi-turn conversation, translation, summarization, question answering, mathematical reasoning, and retrieval-augmented generation. The results show that FSPAD outperforms the state-of-the-art method across all the aforementioned tasks and target LLMs.

- **中文摘要**: 无损推测解码通过使用轻量级草稿模型生成树结构候选，并由目标大语言模型（LLM）并行验证，从而加速目标LLM的推理。当前有效的方法利用草稿模型中的特征级而非令牌级自回归，以促进更直接的预测和增强的知识蒸馏。本文重新审视了这些方法，并提出了FSPAD（无损推测解码的特征采样与部分对齐蒸馏），在现有框架中引入了两个简单而有效的组件，以提升无损推测解码的效果。首先，FSPAD利用令牌嵌入在高维空间中采样目标LLM的特征，然后将其输入草稿模型，因为特征的固有不确定性使得草稿模型无法通过这些特征获得目标LLM的具体令牌输出。其次，FSPAD引入了部分对齐蒸馏，以削弱草稿模型中特征与对数之间的联系，旨在减少训练过程中特征对齐与对数置信度之间的冲突。我们的实验包括在Vicuna和LLaMA3-Instruct系列的最大和最小模型上进行贪婪和非贪婪解码，以及多轮对话、翻译、摘要、问答、数学推理和检索增强生成等任务。结果显示，FSPAD在所有上述任务和目标LLM中均优于最先进的方法。

### The Mamba in the Llama: Distilling and Accelerating Hybrid Models
- **Authors**: Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao
- **Published**: 2024-08-27
- **Link**: [http://arxiv.org/abs/2408.15237v1](http://arxiv.org/abs/2408.15237v1)
- **Summary**: Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.

- **中文摘要**: 线性RNN架构，如Mamba，在语言建模方面可以与Transformer模型竞争，同时具有优势的部署特性。鉴于当前对训练大规模Transformer模型的关注，我们考虑了将这些预训练模型转换为部署模型的挑战。我们证明了，通过重用注意力层的线性投影权重，利用学术GPU资源将大型Transformer模型蒸馏为线性RNN是可行的。由此产生的混合模型，包含了四分之一的注意力层，在聊天基准测试中达到了与原始Transformer相当的性能，并且在聊天基准测试和通用基准测试中均优于从头开始训练的开放源代码混合Mamba模型，这些模型训练了数万亿个token。此外，我们引入了一种硬件感知的推测解码算法，加速了Mamba和混合模型的推理速度。总的来说，我们展示了如何在有限的计算资源下，移除许多原始的注意力层，并更高效地从生成的模型中进行推理。我们从Llama3-8B-Instruct蒸馏出的顶级模型，在AlpacaEval 2上以29.61的长度控制胜率对抗GPT-4，并在MT-Bench上达到7.35，超过了最佳指令调优的线性RNN模型。

### MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding
- **Authors**: Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen
- **Published**: 2024-08-20
- **Link**: [http://arxiv.org/abs/2408.11049v3](http://arxiv.org/abs/2408.11049v3)
- **Summary**: Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft models with sparse KV cache to address the KV bottleneck that scales with both sequence length and batch size. This finding underscores the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2x speedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available at https://github.com/Infini-AI-Lab/MagicDec/.

- **中文摘要**: 大型语言模型（LLMs）在长上下文应用中变得越来越普遍，如交互式聊天机器人、文档分析和代理工作流程，但要在低延迟和高吞吐量的情况下处理长上下文请求仍然具有挑战性。推测性解码（SD）是一种广泛使用的技术，可以在不牺牲性能的情况下减少延迟，但传统观点认为其有效性仅限于小批量大小。在MagicDec中，我们惊讶地发现，即使在适中至长序列的高吞吐量推理场景中，SD也能实现加速。更有趣的是，根据我们严谨的分析，智能草稿策略可以在增加批量大小的情况下实现更好的加速。MagicDec首先识别出随着批量大小和序列长度的增加，瓶颈会发生变化，并利用这些洞察更有效地部署推测性解码以实现高吞吐量推理。然后，它利用具有稀疏KV缓存的草稿模型来解决随着序列长度和批量大小扩展的KV瓶颈。这一发现强调了推测性解码在长上下文服务中的广泛适用性，因为它可以在不损害准确性的前提下提高吞吐量并减少延迟。对于适中至长的序列，我们在8个NVIDIA A100 GPU上处理批量大小从32到256的请求时，展示了LLaMA-2-7B-32K最高2倍的加速和LLaMA-3.1-8B最高1.84倍的加速。代码可在https://github.com/Infini-AI-Lab/MagicDec/获取。

### Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling
- **Authors**: Xianzhen Luo, Yixuan Wang, Qingfu Zhu, Zhiming Zhang, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che
- **Published**: 2024-08-16
- **Link**: [http://arxiv.org/abs/2408.08696v1](http://arxiv.org/abs/2408.08696v1)
- **Summary**: The rapid growth in the parameters of large language models (LLMs) has made inference latency a fundamental bottleneck, limiting broader application of LLMs. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm, leveraging the parallel capabilities of modern hardware. Some speculative decoding methods rely on additional structures to guess draft tokens, such as small models or parameter-efficient architectures, which need extra training before use. Alternatively, retrieval-based train-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. This approach stores candidate tokens in an adjacency matrix and employs a breadth-first search (BFS)-like algorithm on the matrix to construct a draft tree. The tree is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\% and even a training method by 25\%. It can be directly applied to any existing LLMs and tasks without the need for adaptation.

- **中文摘要**: 大型语言模型（LLMs）参数的快速增长使得推理延迟成为一个根本性的瓶颈，限制了LLMs的更广泛应用。推测性解码通过猜测-验证范式，利用现代硬件的并行能力，提供了一种无损的推理加速方法。一些推测性解码方法依赖于额外的结构来猜测草稿标记，如小型模型或参数高效架构，这些方法在使用前需要额外的训练。另一种选择是基于检索的无训练技术，它们通过从现有语料库中构建库或通过n-gram生成来实现。然而，这些方法面临存储需求大、检索耗时和适应性有限等挑战。观察到解码过程中生成的候选标记很可能在未来的序列中再次出现，我们提出了标记回收（Token Recycling）。该方法将候选标记存储在一个邻接矩阵中，并采用类似广度优先搜索（BFS）的算法在矩阵上构建草稿树。然后通过树注意力对树进行验证。解码过程中生成的新候选标记随后用于更新矩阵。标记回收仅需不到2MB的额外存储，并在所有规模的LLMs上实现了约2倍的加速。它显著优于现有的无训练方法30%，甚至优于一种训练方法25%。它可以直接应用于任何现有的LLMs和任务，无需适应。

### KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning
- **Authors**: Kaiqi Zhang, Jing Zhao, Rui Chen
- **Published**: 2024-08-15
- **Link**: [http://arxiv.org/abs/2408.08146v1](http://arxiv.org/abs/2408.08146v1)
- **Summary**: Large Language Models (LLMs) exhibit high inference latency due to their autoregressive decoding nature. While the draft head in speculative decoding mitigates this issue, its full potential remains unexplored. In this paper, we introduce KOALA (K-layer Optimized Adversarial Learning Architecture), an orthogonal approach to the draft head. By transforming the conventional single-layer draft head into a multi-layer architecture and incorporating adversarial learning into the traditional supervised training, KOALA significantly improves the accuracy of the draft head in predicting subsequent tokens, thus more closely mirroring the functionality of LLMs. Although this improvement comes at the cost of slightly increased drafting overhead, KOALA substantially unlocks the draft head's potential, greatly enhancing speculative decoding. We conducted comprehensive evaluations of KOALA, including both autoregressive and non-autoregressive draft heads across various tasks, demonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is 10.57%-14.09% faster than the original draft heads.

- **中文摘要**: 大型语言模型（LLMs）由于其自回归解码的特性，表现出较高的推理延迟。尽管推测解码中的草稿头缓解了这一问题，但其全部潜力尚未被充分发掘。在本文中，我们提出了KOALA（K层优化对抗学习架构），这是一种与草稿头正交的方法。通过将传统的单层草稿头转变为多层架构，并将对抗学习融入传统的监督训练中，KOALA显著提高了草稿头预测后续标记的准确性，从而更接近LLMs的功能。尽管这一改进带来了略微增加的草稿开销，但KOALA实质上释放了草稿头的潜力，极大地增强了推测解码。我们对KOALA进行了全面的评估，包括在各种任务中对自回归和非自回归草稿头的评估，结果显示延迟加速比提高了0.24x-0.41x，比原始草稿头快10.57%-14.09%。

### Coupling without Communication and Drafter-Invariant Speculative Decoding
- **Authors**: Majid Daliri, Christopher Musco, Ananda Theertha Suresh
- **Published**: 2024-08-15
- **Link**: [http://arxiv.org/abs/2408.07978v2](http://arxiv.org/abs/2408.07978v2)
- **Summary**: Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice wants to generate a sample $a\sim P$ and Bob a sample $b \sim Q$ such that $a = b$ with has as high of probability as possible. It is well-known that, by sampling from an optimal coupling between the distributions, Alice and Bob can achieve $Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation distance. What if Alice and Bob must solve this same problem without communicating at all? Perhaps surprisingly, with access to public randomness, they can still achieve $Pr[a=b] \geq \frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)} \geq 1-2D_{TV}(P,Q)$. In fact, this bound can be obtained using a simple protocol based on the Weighted MinHash algorithm. In this work, we explore the communication-free coupling problem in greater depth. First, we show that an equally simple protocol based on Gumbel sampling matches the worst-case guarantees of the Weighted MinHash approach, but tends to perform better in practice. Conversely, we prove that both approaches are actually sharp: no communication-free protocol can achieve $Pr[a=b]>\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$ in the worst-case. Finally, we prove that, for distributions over $n$ items, there exists a scheme that uses just $O(\log(n/\epsilon))$ bits of communication to achieve $Pr[a = b] = 1 - D_{TV}(P,Q) - \epsilon$, i.e. to essentially match optimal coupling. Beyond our theoretical results, we demonstrate an application of communication-free coupling to speculative decoding, a recent method for accelerating autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We show that communication-free protocols yield a variant of speculative decoding that we call Drafter-Invariant Speculative Decoding, which has the desirable property that the output of the method is fixed given a fixed random seed, regardless of what drafter is used for speculation.

- **中文摘要**: 假设Alice有一个分布$P$，Bob有一个分布$Q$。Alice希望生成一个样本$a \sim P$，而Bob生成一个样本$b \sim Q$，使得$a = b$的概率尽可能高。众所周知，通过从分布之间的最优耦合中采样，Alice和Bob可以实现$Pr[a = b] = 1 - D_{TV}(P,Q)$，其中$D_{TV}(P,Q)$是总变差距离。如果Alice和Bob必须在不进行任何通信的情况下解决相同的问题，情况会如何呢？或许令人惊讶的是，通过使用公共随机性，他们仍然可以实现$Pr[a=b] \geq \frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)} \geq 1-2D_{TV}(P,Q)$。事实上，这个界限可以通过基于加权MinHash算法的简单协议获得。在这项工作中，我们更深入地探讨了无通信耦合问题。首先，我们展示了一个基于Gumbel采样的同样简单的协议，它与加权MinHash方法的最坏情况保证相匹配，但在实践中往往表现更好。相反，我们证明了这两种方法实际上都是尖锐的：没有任何无通信协议可以在最坏情况下实现$Pr[a=b]>\frac{1-D_{TV}(P,Q)}{1+D_{TV}(P,Q)}$。最后，我们证明，对于$n$个项目的分布，存在一个方案，只需使用$O(\log(n/\epsilon))$比特的通信，就能实现$Pr[a = b] = 1 - D_{TV}(P,Q) - \epsilon$，即基本上匹配最优耦合。除了我们的理论结果，我们还展示了无通信耦合在推测解码中的应用，这是一种最近用于加速自回归大型语言模型的方法[Leviathan, Kalman, Matias, ICML 2023]。我们展示了无通信协议产生了一种我们称之为Drafter-Invariant Speculative Decoding的推测解码变体，该方法具有一个理想的特性，即无论使用何种推测者，给定固定的随机种子，方法的输出是固定的。

### Parallel Speculative Decoding with Adaptive Draft Length
- **Authors**: Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, Winston Hu
- **Published**: 2024-08-13
- **Link**: [http://arxiv.org/abs/2408.11850v2](http://arxiv.org/abs/2408.11850v2)
- **Summary**: Speculative decoding (SD), where an extra draft model is employed to provide multiple \textit{draft} tokens first and then the original target model verifies these tokens in parallel, has shown great power for LLM inference acceleration. However, existing SD methods suffer from the mutual waiting problem, i.e., the target model gets stuck when the draft model is \textit{guessing} tokens, and vice versa. This problem is directly incurred by the asynchronous execution of the draft model and the target model, and is exacerbated due to the fixed draft length in speculative decoding. To address these challenges, we propose a conceptually simple, flexible, and general framework to boost speculative decoding, namely \textbf{P}arallel sp\textbf{E}culative decoding with \textbf{A}daptive d\textbf{R}aft \textbf{L}ength (PEARL). Specifically, PEARL proposes \textit{pre-verify} to verify the first draft token in advance during the drafting phase, and \textit{post-verify} to generate more draft tokens during the verification phase. PEARL parallels the drafting phase and the verification phase via applying the two strategies, and achieves adaptive draft length for different scenarios, which effectively alleviates the mutual waiting problem. Moreover, we theoretically demonstrate that the mean accepted tokens of PEARL is more than existing \textit{draft-then-verify} works. Experiments on various text generation benchmarks demonstrate the effectiveness of our \name, leading to a superior speedup performance up to \textbf{3.79$\times$} and \textbf{1.52$\times$}, compared to auto-regressive decoding and vanilla speculative decoding, respectively.

- **中文摘要**: 推测性解码（Speculative Decoding, SD）是一种利用额外的草稿模型首先生成多个\textit{草稿}标记，然后原始目标模型并行验证这些标记的方法，已显示出对大型语言模型（LLM）推理加速的巨大潜力。然而，现有的SD方法存在相互等待的问题，即当草稿模型在\textit{猜测}标记时，目标模型会陷入停滞，反之亦然。这一问题直接源于草稿模型和目标模型执行的异步性，并且由于推测性解码中固定的草稿长度而加剧。为了应对这些挑战，我们提出了一种概念简单、灵活且通用的框架来提升推测性解码，即\textbf{P}并行\textbf{E}推测性解码与\textbf{A}适应性\textbf{R}草稿\textbf{L}长度（PEARL）。具体而言，PEARL提出了\textit{预验证}，在草稿阶段提前验证第一个草稿标记，以及\textit{后验证}，在验证阶段生成更多草稿标记。PEARL通过应用这两种策略并行化草稿阶段和验证阶段，并实现了不同场景下的自适应草稿长度，从而有效缓解了相互等待问题。此外，我们从理论上证明了PEARL的平均接受标记数高于现有的\textit{草稿-验证}工作。在各种文本生成基准上的实验验证了我们方法的有效性，与自回归解码和普通推测性解码相比，分别实现了高达\textbf{3.79$\times$}和\textbf{1.52$\times$}的加速性能。

### A Decoding Acceleration Framework for Industrial Deployable LLM-based Recommender Systems
- **Authors**: Yunjia Xi, Hangyu Wang, Bo Chen, Jianghao Lin, Menghui Zhu, Weiwen Liu, Ruiming Tang, Weinan Zhang, Yong Yu
- **Published**: 2024-08-11
- **Link**: [http://arxiv.org/abs/2408.05676v1](http://arxiv.org/abs/2408.05676v1)
- **Summary**: Recently, increasing attention has been paid to LLM-based recommender systems, but their deployment is still under exploration in the industry. Most deployments utilize LLMs as feature enhancers, generating augmentation knowledge in the offline stage. However, in recommendation scenarios, involving numerous users and items, even offline generation with LLMs consumes considerable time and resources. This generation inefficiency stems from the autoregressive nature of LLMs, and a promising direction for acceleration is speculative decoding, a Draft-then-Verify paradigm that increases the number of generated tokens per decoding step. In this paper, we first identify that recommendation knowledge generation is suitable for retrieval-based speculative decoding. Then, we discern two characteristics: (1) extensive items and users in RSs bring retrieval inefficiency, and (2) RSs exhibit high diversity tolerance for text generated by LLMs. Based on the above insights, we propose a Decoding Acceleration Framework for LLM-based Recommendation (dubbed DARE), with Customized Retrieval Pool to improve retrieval efficiency and Relaxed Verification to increase the acceptance rate of draft tokens, respectively. Extensive experiments demonstrate that DARE achieves a 3-5x speedup and is compatible with various frameworks and backbone LLMs. DARE has also been deployed to online advertising scenarios within a large-scale commercial environment, achieving a 3.45x speedup while maintaining the downstream performance.

- **中文摘要**: 近年来，基于大语言模型（LLM）的推荐系统受到越来越多的关注，但其在工业界的部署仍处于探索阶段。大多数部署方案利用LLM作为特征增强器，在离线阶段生成增广知识。然而，在推荐场景中，涉及大量用户和物品，即使离线生成也消耗大量时间和资源。这种生成效率低下的根源在于LLM的自回归特性，而加速的一个有前景的方向是推测性解码，这是一种“先草稿后验证”的范式，可以增加每个解码步骤生成的token数量。在本文中，我们首先确定推荐知识生成适合基于检索的推测性解码。然后，我们识别出两个特点：（1）推荐系统中广泛的用户和物品带来了检索效率低下的问题，（2）推荐系统对LLM生成的文本具有较高的多样性容忍度。基于以上洞察，我们提出了一个基于LLM的推荐解码加速框架（简称DARE），分别通过定制检索池提高检索效率和宽松验证提高草稿token的接受率。大量实验表明，DARE实现了3-5倍的加速，并且兼容各种框架和骨干LLM。DARE也已部署到大规模商业环境中的在线广告场景，实现了3.45倍的加速，同时保持了下游性能。

### Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion
- **Authors**: Jacob K Christopher, Brian R Bartoldson, Bhavya Kailkhura, Ferdinando Fioretto
- **Published**: 2024-08-10
- **Link**: [http://arxiv.org/abs/2408.05636v2](http://arxiv.org/abs/2408.05636v2)
- **Summary**: Speculative decoding has emerged as a widely adopted method to accelerate large language model inference without sacrificing the quality of the model outputs. While this technique has facilitated notable speed improvements by enabling parallel sequence verification, its efficiency remains inherently limited by the reliance on incremental token generation in existing draft models. To overcome this limitation, this paper proposes an adaptation of speculative decoding which uses discrete diffusion models to generate draft sequences. This allows parallelization of both the drafting and verification steps, providing significant speed-ups to the inference process. Our proposed approach, Speculative Diffusion Decoding (SpecDiff), is validated on standard language generation benchmarks and empirically demonstrated to provide a up to 8.7x speed-up over standard generation processes and up to 2.5x speed-up over existing speculative decoding approaches.

- **中文摘要**: 推测性解码作为一种广泛采用的方法，已经出现，它能够在不牺牲模型输出质量的情况下加速大型语言模型的推理。尽管这项技术通过实现并行序列验证显著提升了速度，但其效率仍然受到现有草稿模型中依赖增量令牌生成的固有限制。为了克服这一限制，本文提出了一种推测性解码的适应方法，该方法使用离散扩散模型生成草稿序列。这使得草稿和验证步骤都能并行化，从而显著加快了推理过程。我们提出的方法，即推测性扩散解码（SpecDiff），在标准语言生成基准上得到了验证，并实证表明，与标准生成过程相比，速度提升了高达8.7倍，与现有的推测性解码方法相比，速度提升了高达2.5倍。

### CREST: Effectively Compacting a Datastore For Retrieval-Based Speculative Decoding
- **Authors**: Sophia Ho, Jinsol Park, Patrick Wang
- **Published**: 2024-08-08
- **Link**: [http://arxiv.org/abs/2408.04678v1](http://arxiv.org/abs/2408.04678v1)
- **Summary**: We present CREST (Compact Retrieval-Based Speculative Decoding), a redesign of REST that allows it to be effectively "compacted". REST is a drafting technique for speculative decoding based on retrieving exact n-gram matches of the most recent n tokens generated by the target LLM from a datastore. The key idea of CREST is to only store a subset of the smallest and most common n-grams in the datastore with the hope of achieving comparable performance with less storage space. We found that storing a subset of n-grams both reduces storage space and improves performance. CREST matches REST's accepted token length with 10.6-13.5x less storage space and achieves a 16.5-17.1% higher acceptance length than REST using the same storage space on the HumanEval and MT Bench benchmarks.

- **中文摘要**: 我们提出了CREST（基于检索的紧凑推测解码），这是对REST的重新设计，使其能够被有效地“压缩”。REST是一种基于从数据存储中检索目标大型语言模型（LLM）生成的最近n个标记的精确n-gram匹配的推测解码起草技术。CREST的关键思想是仅在数据存储中存储最小且最常见的n-gram子集，以期在减少存储空间的同时实现相当的性能。我们发现，存储n-gram的子集不仅减少了存储空间，还提高了性能。在HumanEval和MT Bench基准测试中，CREST以10.6至13.5倍的存储空间实现了与REST相同的接受标记长度，并且在使用相同存储空间的情况下，比REST的接受长度提高了16.5%至17.1%。

### Clover-2: Accurate Inference for Regressive Lightweight Speculative Decoding
- **Authors**: Bin Xiao, Lujun Gui, Lei Su, Weipeng Chen
- **Published**: 2024-08-01
- **Link**: [http://arxiv.org/abs/2408.00264v1](http://arxiv.org/abs/2408.00264v1)
- **Summary**: Large Language Models (LLMs) frequently suffer from inefficiencies, largely attributable to the discord between the requirements of auto-regressive decoding and the architecture of contemporary GPUs. Recently, regressive lightweight speculative decoding has garnered attention for its notable efficiency improvements in text generation tasks. This approach utilizes a lightweight regressive draft model, like a Recurrent Neural Network (RNN) or a single transformer decoder layer, leveraging sequential information to iteratively predict potential tokens. Specifically, RNN draft models are computationally economical but tend to deliver lower accuracy, while attention decoder layer models exhibit the opposite traits. This paper presents Clover-2, an advanced iteration of Clover, an RNN-based draft model designed to achieve comparable accuracy to that of attention decoder layer models while maintaining minimal computational overhead. Clover-2 enhances the model architecture and incorporates knowledge distillation to increase Clover's accuracy and improve overall efficiency. We conducted experiments using the open-source Vicuna 7B and LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses existing methods across various model architectures, showcasing its efficacy and robustness.

- **中文摘要**: 大型语言模型（LLMs）在效率方面经常遇到问题，这主要归因于自回归解码的需求与当代GPU架构之间的不协调。最近，回归轻量级推测解码因其显著提升文本生成任务效率而受到关注。该方法利用轻量级回归草稿模型，如循环神经网络（RNN）或单个变换器解码层，利用序列信息迭代预测潜在的标记。具体而言，RNN草稿模型计算成本低，但准确性较低，而注意力解码层模型则表现出相反的特征。本文介绍了Clover-2，它是Clover的进阶版本，Clover是一种基于RNN的草稿模型，旨在在保持最小计算开销的同时，实现与注意力解码层模型相当的准确性。Clover-2通过增强模型架构并结合知识蒸馏，提高了Clover的准确性并提升了整体效率。我们使用开源的Vicuna 7B和LLaMA3-Instruct 8B模型进行了实验。结果表明，Clover-2在各种模型架构中均优于现有方法，展示了其有效性和鲁棒性。

### Graph-Structured Speculative Decoding
- **Authors**: Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan
- **Published**: 2024-07-23
- **Link**: [http://arxiv.org/abs/2407.16207v1](http://arxiv.org/abs/2407.16207v1)
- **Summary**: Speculative decoding has emerged as a promising technique to accelerate the inference of Large Language Models (LLMs) by employing a small language model to draft a hypothesis sequence, which is then validated by the LLM. The effectiveness of this approach heavily relies on the balance between performance and efficiency of the draft model. In our research, we focus on enhancing the proportion of draft tokens that are accepted to the final output by generating multiple hypotheses instead of just one. This allows the LLM more options to choose from and select the longest sequence that meets its standards. Our analysis reveals that hypotheses produced by the draft model share many common token sequences, suggesting a potential for optimizing computation. Leveraging this observation, we introduce an innovative approach utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This structure enables us to efficiently predict and merge recurring token sequences, vastly reducing the computational demands of the draft model. We term this approach Graph-structured Speculative Decoding (GSD). We apply GSD across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and observe a remarkable speedup of 1.73$\times$ to 1.96$\times$, significantly surpassing standard speculative decoding.

- **中文摘要**: 推测性解码作为一种有前景的技术，通过使用一个小型语言模型生成假设序列，然后由大型语言模型（LLM）进行验证，从而加速了大型语言模型的推理过程。这种方法的有效性在很大程度上依赖于草稿模型的性能与效率之间的平衡。在我们的研究中，我们专注于通过生成多个假设而不是单一假设来提高草稿标记被最终输出接受的比例。这使得LLM有更多的选择，并可以选择符合其标准的最长序列。我们的分析表明，草稿模型生成的假设共享许多常见的标记序列，这暗示了优化计算的潜力。利用这一观察结果，我们引入了一种创新的方法，使用有向无环图（DAG）来管理草稿假设。这种结构使我们能够高效地预测和合并重复的标记序列，极大地减少了草稿模型的计算需求。我们将这种方法称为图结构推测性解码（GSD）。我们在一系列LLM上应用了GSD，包括一个700亿参数的LLaMA-2模型，并观察到显著的加速效果，从1.73倍到1.96倍，明显超过了标准的推测性解码。

### Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference
- **Authors**: Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun
- **Published**: 2024-07-12
- **Link**: [http://arxiv.org/abs/2407.09722v1](http://arxiv.org/abs/2407.09722v1)
- **Summary**: Transformer-based Large language models (LLMs) have demonstrated their power in various tasks, but their inference incurs significant time and energy costs. To accelerate LLM inference, speculative decoding uses a smaller model to propose one sequence of tokens, which are subsequently validated in batch by the target large model. Compared with autoregressive decoding, speculative decoding generates the same number of tokens with fewer runs of the large model, hence accelerating the overall inference by $1$-$2\times$. However, greedy decoding is not the optimal decoding algorithm in terms of output perplexity, which is a direct measurement of the effectiveness of a decoding algorithm. An algorithm that has better output perplexity and even better efficiency than speculative decoding can be more useful in practice. To achieve this seemingly contradictory goal, we first introduce multi-token joint greedy decoding (MJGD), which greedily generates multiple tokens at each step based on their joint perplexity. We show that it leads to better perplexity for the whole output. But the computation cost of MJGD is infeasible in practice. So we further propose multi-token joint speculative decoding (MJSD), which approximates and accelerates the MJGD from two aspects: it approximates the joint distribution of the large model with that of a small model, and uses a verification step to guarantee the accuracy of approximation; then it uses beam decoding to accelerate the sequence generation from the joint distribution. Compared with vanilla speculative decoding, MJSD has two advantages: (1) it is an approximation of MJGD, thus achieving better output perplexity; (2) verification with joint likelihood allows it to accept the longest prefix sub-sequence of the draft tokens with valid perplexity, leading to better efficiency...

- **中文摘要**: 基于Transformer的大语言模型（LLMs）在各种任务中展示了其强大的能力，但其推理过程却伴随着显著的时间和能源消耗。为了加速LLM的推理，推测性解码（speculative decoding）采用了一个较小的模型来提出一串候选词元，然后由目标大模型批量验证这些词元。与自回归解码相比，推测性解码通过减少大模型的运行次数，以相同的词元数量实现了1-2倍的推理加速。然而，贪婪解码在输出困惑度（perplexity）方面并非最优解码算法，而困惑度是衡量解码算法效果的直接指标。在实际应用中，一种在输出困惑度上表现更优且效率甚至超过推测性解码的算法将更具实用价值。为了实现这一看似矛盾的目标，我们首先引入了多词元联合贪婪解码（MJGD），该方法在每一步贪婪地生成多个词元，基于它们的联合困惑度进行选择。我们证明，这种方法能够改善整体输出的困惑度。但MJGD的计算成本在实践中是不可行的。因此，我们进一步提出了多词元联合推测性解码（MJSD），该方法从两个方面近似并加速了MJGD：它通过一个小模型近似大模型的联合分布，并使用验证步骤来保证近似的准确性；随后，它利用束解码（beam decoding）来加速从联合分布中生成序列。与传统的推测性解码相比，MJSD具有两大优势：（1）作为MJGD的近似，它能够实现更好的输出困惑度；（2）通过联合似然性验证，它能够接受具有有效困惑度的草稿词元中最长的前缀子序列，从而提高效率...

### Accelerating the inference of string generation-based chemical reaction models for industrial applications
- **Authors**: Mikhail Andronov, Natalia Andronova, Michael Wand, Jürgen Schmidhuber, Djork-Arné Clevert
- **Published**: 2024-07-12
- **Link**: [http://arxiv.org/abs/2407.09685v2](http://arxiv.org/abs/2407.09685v2)
- **Summary**: Template-free SMILES-to-SMILES translation models for reaction prediction and single-step retrosynthesis are of interest for industrial applications in computer-aided synthesis planning systems due to their state-of-the-art accuracy. However, they suffer from slow inference speed. We present a method to accelerate inference in autoregressive SMILES generators through speculative decoding by copying query string subsequences into target strings in the right places. We apply our method to the molecular transformer implemented in Pytorch Lightning and achieve over 3X faster inference in reaction prediction and single-step retrosynthesis, with no loss in accuracy.

- **中文摘要**: 无模板SMILES到SMILES的翻译模型在反应预测和单步逆合成中因其卓越的准确性而在计算机辅助合成规划系统的工业应用中备受关注。然而，这些模型在推理速度上存在不足。我们提出了一种通过推测性解码来加速自回归SMILES生成器推理的方法，即在适当的位置将查询字符串的子序列复制到目标字符串中。我们将此方法应用于基于Pytorch Lightning实现的分子变换器，并在反应预测和单步逆合成中实现了超过3倍的推理速度提升，且未损失任何准确性。

### S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models
- **Authors**: Parsa Kavehzadeh, Mohammadreza Pourreza, Mojtaba Valipour, Tinashu Zhu, Haoli Bai, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh
- **Published**: 2024-07-02
- **Link**: [http://arxiv.org/abs/2407.01955v1](http://arxiv.org/abs/2407.01955v1)
- **Summary**: Deployment of autoregressive large language models (LLMs) is costly, and as these models increase in size, the associated costs will become even more considerable. Consequently, different methods have been proposed to accelerate the token generation process and reduce costs. Speculative decoding (SD) is among the most promising approaches to speed up the LLM decoding process by verifying multiple tokens in parallel and using an auxiliary smaller draft model to generate the possible tokens. In SD, usually, one draft model is used to serve a specific target model; however, in practice, LLMs are diverse, and we might need to deal with many target models or more than one target model simultaneously. In this scenario, it is not clear which draft model should be used for which target model, and searching among different draft models or training customized draft models can further increase deployment costs. In this paper, we first introduce a novel multi-target scenario for the deployment of draft models for faster inference. Then, we present a novel, more efficient sorted speculative decoding mechanism that outperforms regular baselines in multi-target settings. We evaluated our method on Spec-Bench in different settings, including base models such as Vicuna 7B, 13B, and LLama Chat 70B. Our results suggest that our draft models perform better than baselines for multiple target models at the same time.

- **中文摘要**: 自回归大型语言模型（LLM）的部署成本高昂，并且随着这些模型规模的增大，相关成本将变得更加可观。因此，人们提出了各种方法来加速令牌生成过程并降低成本。推测性解码（SD）是最有前景的方法之一，它通过并行验证多个令牌并使用辅助的较小草稿模型来生成可能的令牌，从而加速LLM的解码过程。在SD中，通常使用一个草稿模型来服务于特定的目标模型；然而，在实践中，LLM是多样化的，我们可能需要处理多个目标模型或同时处理多个目标模型。在这种情况下，不清楚应该为哪个目标模型使用哪个草稿模型，而在不同的草稿模型中搜索或训练定制的草稿模型可能会进一步增加部署成本。在本文中，我们首先介绍了一种新颖的多目标场景，用于部署草稿模型以实现更快的推理。然后，我们提出了一种新颖的、更高效的排序推测性解码机制，该机制在多目标设置中优于常规基线。我们在不同的设置下评估了我们的方法，包括Vicuna 7B、13B和LLama Chat 70B等基础模型。我们的结果表明，我们的草稿模型在同时处理多个目标模型时表现优于基线。

### SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding
- **Authors**: Zhenglin Wang, Jialong Wu, Yilong Lai, Congzhi Zhang, Deyu Zhou
- **Published**: 2024-06-26
- **Link**: [http://arxiv.org/abs/2406.18200v1](http://arxiv.org/abs/2406.18200v1)
- **Summary**: Large Language Models (LLMs) demonstrate remarkable emergent abilities across various tasks, yet fall short of complex reasoning and planning tasks. The tree-search-based reasoning methods address this by surpassing the capabilities of chain-of-thought prompting, encouraging exploration of intermediate steps. However, such methods introduce significant inference latency due to the systematic exploration and evaluation of multiple thought paths. This paper introduces SeeD, a novel and efficient inference framework to optimize runtime speed and GPU memory management concurrently. By employing a scheduled speculative execution, SeeD efficiently handles multiple iterations for the thought generation and the state evaluation, leveraging a rounds-scheduled strategy to manage draft model dispatching. Extensive experimental evaluations on three reasoning datasets demonstrate superior speedup performance of SeeD, providing a viable path for batched inference in training-free speculative decoding.

- **中文摘要**: 大型语言模型（LLMs）在各种任务中展现出显著的涌现能力，但在复杂推理和规划任务上仍显不足。基于树搜索的推理方法通过超越思维链提示的能力，鼓励探索中间步骤，从而解决了这一问题。然而，这些方法由于系统性地探索和评估多个思维路径，引入了显著的推理延迟。本文介绍了SeeD，一种新颖且高效的推理框架，旨在同时优化运行时速度和GPU内存管理。通过采用计划性的推测执行，SeeD能够高效处理思维生成和状态评估的多次迭代，利用轮次调度策略来管理草稿模型的调度。在三个推理数据集上的广泛实验评估表明，SeeD具有优越的加速性能，为无训练的推测解码中的批量推理提供了一条可行路径。

### Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training
- **Authors**: Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che
- **Published**: 2024-06-25
- **Link**: [http://arxiv.org/abs/2406.17404v1](http://arxiv.org/abs/2406.17404v1)
- **Summary**: Existing speculative decoding methods typically require additional model structure and training processes to assist the model for draft token generation. This makes the migration of acceleration methods to the new model more costly and more demanding on device memory. To address this problem, we propose the Make Some Noise (MSN) training framework as a replacement for the supervised fine-tuning stage of the large language model. The training method simply introduces some noise at the input for the model to learn the denoising task. It significantly enhances the parallel decoding capability of the model without affecting the original task capability. In addition, we propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further improve the inference speed of MSN models. Experiments in both the general and code domains have shown that MSN can improve inference speed by 2.3-2.7x times without compromising model performance. The MSN model also achieves comparable acceleration ratios to the SOTA model with additional model structure on Spec-Bench.

- **中文摘要**: 现有的投机解码方法通常需要额外的模型结构和训练过程来辅助模型进行草稿令牌生成。这使得加速方法向新模型的迁移成本更高，对设备内存的要求也更为严格。为了解决这一问题，我们提出了Make Some Noise（MSN）训练框架，作为大型语言模型监督微调阶段的替代方案。该训练方法仅在输入中引入一些噪声，使模型学习去噪任务。它显著增强了模型的并行解码能力，同时不影响原有任务能力。此外，我们还提出了一种基于树的检索增强雅可比（TR-Jacobi）解码策略，以进一步提高MSN模型的推理速度。在通用和代码领域的实验均表明，MSN在不损害模型性能的前提下，可将推理速度提升2.3-2.7倍。MSN模型在Spec-Bench上的加速比也与具有额外模型结构的SOTA模型相当。

### OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure
- **Authors**: Jikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang
- **Published**: 2024-06-25
- **Link**: [http://arxiv.org/abs/2406.17276v2](http://arxiv.org/abs/2406.17276v2)
- **Summary**: Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a "draft and then verify" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which fail to adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft trees. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree.

- **中文摘要**: 自回归语言模型在多种场景下展现了卓越的性能。然而，其一步一生成的推理模式限制了效率，随着模型规模的不断扩大，这一问题变得愈发紧迫。推测性解码采用“先草拟后验证”的机制，允许一步生成多个词元，实现无损加速。现有方法主要采用固定的启发式草拟结构，无法适应不同情况以最大化验证阶段的接受长度。为缓解这一困境，我们提出了OPT-Tree算法，用于构建自适应且可扩展的草拟树。该算法在每个解码步骤中搜索能够最大化接受长度数学期望的最优树结构。实验结果表明，OPT-Tree优于现有的草拟结构，与自回归解码相比，最高可实现3.2倍的加速比。若草拟模型足够强大且节点预算充足，单步可生成超过十个词元。我们的代码已公开，详见https://github.com/Jikai0Wang/OPT-Tree。

### Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters
- **Authors**: Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun
- **Published**: 2024-06-24
- **Link**: [http://arxiv.org/abs/2406.16758v1](http://arxiv.org/abs/2406.16758v1)
- **Summary**: Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which are leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup of inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.

- **中文摘要**: 大型语言模型（LLMs）已经彻底改变了自然语言处理领域，并扩展了其在各种商业应用中的适用性。然而，这些模型在多语言环境中的部署受到高推理时间的限制。为了应对这一挑战，本文探讨了一种在推测性解码中训练辅助模型的方法，该模型用于起草，然后由目标LLM验证其未来标记。我们展示了通过针对性的预训练和微调策略优化的特定语言草稿模型，与之前的方法相比，显著加快了推理时间。我们在多种语言的推理时间、领域外加速以及GPT-4o评估中验证了这些模型。

### Optimizing Speculative Decoding for Serving Large Language Models Using Goodput
- **Authors**: Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang
- **Published**: 2024-06-20
- **Link**: [http://arxiv.org/abs/2406.14066v2](http://arxiv.org/abs/2406.14066v2)
- **Summary**: Reducing the inference latency of large language models (LLMs) is crucial, and speculative decoding (SD) stands out as one of the most effective techniques. Rather than letting the LLM generate all tokens directly, speculative decoding employs effective proxies to predict potential outputs, which are then verified by the LLM without compromising the generation quality. Yet, deploying SD in real online LLM serving systems (with continuous batching) does not always yield improvement -- under higher request rates or low speculation accuracy, it paradoxically increases latency. Furthermore, there is no best speculation length work for all workloads under different system loads. Based on the observations, we develop a dynamic framework SmartSpec. SmartSpec dynamically determines the best speculation length for each request (from 0, i.e., no speculation, to many tokens) -- hence the associated speculative execution costs -- based on a new metric called goodput, which characterizes the current observed load of the entire system and the speculation accuracy. We show that SmartSpec consistently reduces average request latency by up to 3.2x compared to non-speculative decoding baselines across different sizes of target models, draft models, request rates, and datasets. Moreover, SmartSpec can be applied to different styles of speculative decoding, including traditional, model-based approaches as well as model-free methods like prompt lookup and tree-style decoding.

- **中文摘要**: 降低大型语言模型（LLMs）的推理延迟至关重要，而推测性解码（Speculative Decoding, SD）作为最有效的技术之一脱颖而出。推测性解码不是让LLM直接生成所有标记，而是利用有效的代理来预测潜在的输出，然后由LLM进行验证，而不会影响生成质量。然而，在实际的在线LLM服务系统（具有持续批处理功能）中部署SD并不总能带来改进——在请求率较高或推测准确性较低的情况下，反而会增加延迟。此外，对于不同系统负载下的所有工作负载，并没有最佳的推测长度。基于这些观察，我们开发了一个动态框架SmartSpec。SmartSpec根据一个新的指标——称为“有效产出”（goodput），该指标表征了整个系统的当前观测负载和推测准确性——动态地为每个请求确定最佳的推测长度（从0，即不进行推测，到多个标记），从而确定相关的推测执行成本。我们展示了SmartSpec在不同大小的目标模型、草稿模型、请求率和数据集下，相比非推测性解码基线，持续地将平均请求延迟减少了高达3.2倍。此外，SmartSpec可以应用于不同风格的推测性解码，包括传统的基于模型的方法以及无模型方法，如提示查找和树状解码。

### Amphista: Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style
- **Authors**: Zeping Li, Xinlong Yang, Ziheng Gao, Ji Liu, Zhuang Liu, Dong Li, Jinzhang Peng, Lu Tian, Emad Barsoum
- **Published**: 2024-06-19
- **Link**: [http://arxiv.org/abs/2406.13170v1](http://arxiv.org/abs/2406.13170v1)
- **Summary**: Large Language Models (LLMs) inherently use autoregressive decoding, which lacks parallelism in inference and results in significantly slow inference speeds, especially when hardware parallel accelerators and memory bandwidth are not fully utilized. In this work, we propose Amphista, a speculative decoding algorithm that adheres to a non-autoregressive decoding paradigm. Owing to the increased parallelism, our method demonstrates higher efficiency in inference compared to autoregressive methods. Specifically, Amphista models an Auto-embedding Block capable of parallel inference, incorporating bi-directional attention to enable interaction between different drafting heads. Additionally, Amphista implements Staged Adaptation Layers to facilitate the transition of semantic information from the base model's autoregressive inference to the drafting heads' non-autoregressive speculation, thereby achieving paradigm transformation and feature fusion. We conduct a series of experiments on a suite of Vicuna models using MT-Bench and Spec-Bench. For the Vicuna 33B model, Amphista achieves up to 2.75$\times$ and 1.40$\times$ wall-clock acceleration compared to vanilla autoregressive decoding and Medusa, respectively, while preserving lossless generation quality.

- **中文摘要**: 大型语言模型（LLMs）本质上采用自回归解码，这种解码方式在推理过程中缺乏并行性，导致推理速度显著减慢，尤其是在硬件并行加速器和内存带宽未被充分利用的情况下。在这项工作中，我们提出了Amphista，这是一种遵循非自回归解码范式的推测性解码算法。由于增加了并行性，我们的方法在推理效率上相比自回归方法表现更高。具体而言，Amphista设计了一个能够并行推理的自动嵌入块，并结合双向注意力机制，以实现不同草稿头之间的交互。此外，Amphista还实现了分阶段适应层，以促进语义信息从基础模型的自回归推理向草稿头的非自回归推测过渡，从而实现范式转换和特征融合。我们在一系列Vicuna模型上使用MT-Bench和Spec-Bench进行了实验。对于Vicuna 33B模型，Amphista相比原始自回归解码和Medusa分别实现了高达2.75倍和1.40倍的时钟加速，同时保持了无损生成质量。

### Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding
- **Authors**: Kaiyan Zhang, Jianyu Wang, Ning Ding, Biqing Qi, Ermo Hua, Xingtai Lv, Bowen Zhou
- **Published**: 2024-06-18
- **Link**: [http://arxiv.org/abs/2406.12295v1](http://arxiv.org/abs/2406.12295v1)
- **Summary**: Large Language Models (LLMs) demonstrate impressive performance in diverse applications, yet they face significant drawbacks, including high inference latency, expensive training cost, and generation of hallucination. Collaborative decoding between large and small language models (SLMs) offers a novel approach to address these challenges. Inspired by dual-process cognitive theory, we integrate these methods into a unified framework termed Fast and Slow Generating (FS-GEN). This paper explores several techniques within the FS-GEN framework, including speculative decoding, contrastive decoding, and emulator or proxy fine-tuning. We provide a comprehensive analysis of these methodologies, offering insights into their similarities and differences under this framework. Our study delves into the differential knowledge capabilities of LLMs versus SLMs through the FS-GEN lens, revealing that fewer than 20% of collaborative interactions are required across various methods. These interactions adhere to a scaling law relative to the parameter ratios, thereby facilitating predictable collaboration. Furthermore, we investigate the specific positions where collaboration is most effective from an uncertainty perspective, yielding novel insights that could refine FS-GEN methods. Our findings reveal that the essential difference between models of different sizes lies in the uncertainty of the next token prediction, where interventions by larger models are most needed to assist the smaller ones. Code for Reproduction: https://github.com/TsinghuaC3I/FS-GEN

- **中文摘要**: 大型语言模型（LLMs）在多样化的应用中展现了令人瞩目的性能，但它们也面临着显著的缺陷，包括高推理延迟、昂贵的训练成本以及生成幻觉的问题。大模型与小模型（SLMs）之间的协作解码提供了一种新颖的方法来应对这些挑战。受双过程认知理论的启发，我们将这些方法整合到一个统一的框架中，称为“快与慢生成”（FS-GEN）。本文探讨了FS-GEN框架内的几种技术，包括推测解码、对比解码以及模拟器或代理的微调。我们对该框架下的这些方法进行了全面的分析，提供了关于它们相似性和差异性的见解。我们的研究通过FS-GEN的视角深入探讨了LLMs与SLMs在知识能力上的差异，发现不同方法中所需的协作交互不到20%。这些交互遵循相对于参数比例的缩放定律，从而促进了可预测的协作。此外，我们从不确定性的角度研究了协作最有效的具体位置，得出了可能改进FS-GEN方法的新见解。我们的研究结果表明，不同规模模型之间的本质区别在于下一个标记预测的不确定性，此时大模型的干预最为必要，以协助小模型。重现代码：https://github.com/TsinghuaC3I/FS-GEN

### When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models
- **Authors**: Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Yingyan Celine Lin
- **Published**: 2024-06-11
- **Link**: [http://arxiv.org/abs/2406.07368v2](http://arxiv.org/abs/2406.07368v2)
- **Summary**: Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.

- **中文摘要**: 自回归大型语言模型（LLMs）在语言任务中取得了显著的性能，但面临着两个重大瓶颈：（1）随着token数量的增加，注意力模块的复杂度呈二次增长；（2）由于自回归LLMs在生成过程中采用顺序处理方式，导致效率受限。尽管线性注意力和推测性解码提供了潜在的解决方案，但它们在增强自回归LLMs方面的适用性和协同潜力仍不确定。我们首次对现有线性注意力方法在自回归LLMs中的有效性进行了全面研究，并将它们与推测性解码相结合。我们引入了一种线性注意力的增强技术，确保其与推测性解码的兼容性，从而实现更高效的LLMs训练和服务。通过涉及七种现有线性注意力模型和五种基于编码器/解码器的LLMs的大量实验和消融研究，一致验证了我们增强的线性化LLMs的有效性。值得注意的是，我们的方法在LLaMA模型上实现了高达6.67的困惑度降低，并且在生成过程中相比之前的线性注意力方法实现了高达2倍的加速。代码和模型可在https://github.com/GATECH-EIC/Linearized-LLM获取。

### Proofread: Fixes All Errors with One Tap
- **Authors**: Renjie Liu, Yanxiang Zhang, Yun Zhu, Haicheng Sun, Yuanbo Zhang, Michael Xuelin Huang, Shanqing Cai, Lei Meng, Shumin Zhai
- **Published**: 2024-06-06
- **Link**: [http://arxiv.org/abs/2406.04523v1](http://arxiv.org/abs/2406.04523v1)
- **Summary**: The impressive capabilities in Large Language Models (LLMs) provide a powerful approach to reimagine users' typing experience. This paper demonstrates Proofread, a novel Gboard feature powered by a server-side LLM in Gboard, enabling seamless sentence-level and paragraph-level corrections with a single tap. We describe the complete system in this paper, from data generation, metrics design to model tuning and deployment. To obtain models with sufficient quality, we implement a careful data synthetic pipeline tailored to online use cases, design multifaceted metrics, employ a two-stage tuning approach to acquire the dedicated LLM for the feature: the Supervised Fine Tuning (SFT) for foundational quality, followed by the Reinforcement Learning (RL) tuning approach for targeted refinement. Specifically, we find sequential tuning on Rewrite and proofread tasks yields the best quality in SFT stage, and propose global and direct rewards in the RL tuning stage to seek further improvement. Extensive experiments on a human-labeled golden set showed our tuned PaLM2-XS model achieved 85.56\% good ratio. We launched the feature to Pixel 8 devices by serving the model on TPU v5 in Google Cloud, with thousands of daily active users. Serving latency was significantly reduced by quantization, bucket inference, text segmentation, and speculative decoding. Our demo could be seen in \href{https://youtu.be/4ZdcuiwFU7I}{Youtube}.

- **中文摘要**: 大型语言模型（LLMs）的显著能力为重新构想用户的打字体验提供了一种强大的方法。本文展示了Proofread，这是一个由Gboard服务器端LLM驱动的新型Gboard功能，能够通过一次点击实现无缝的句子级和段落级修正。我们在本文中描述了完整的系统，从数据生成、指标设计到模型调优和部署。为了获得具有足够质量的模型，我们实施了一个精心设计的在线使用场景数据合成管道，设计了多方面的指标，采用两阶段调优方法来获取该功能的专用LLM：首先是监督微调（SFT）以奠定基础质量，随后是强化学习（RL）调优方法以进行针对性优化。具体而言，我们发现对重写和校对任务进行顺序调优在SFT阶段产生了最佳质量，并在RL调优阶段提出了全局和直接奖励以寻求进一步改进。在人工标注的金标准数据集上的广泛实验表明，我们调优的PaLM2-XS模型达到了85.56%的良好比率。我们将该功能发布到Pixel 8设备上，通过在Google Cloud的TPU v5上提供模型服务，拥有数千名每日活跃用户。通过量化、分桶推理、文本分割和推测性解码，服务延迟显著降低。我们的演示可以在\href{https://youtu.be/4ZdcuiwFU7I}{Youtube}上观看。

### Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson Sampling Control Mechanism
- **Authors**: Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai
- **Published**: 2024-06-06
- **Link**: [http://arxiv.org/abs/2406.03853v1](http://arxiv.org/abs/2406.03853v1)
- **Summary**: The recent advancements in large language models (LLMs) have been extraordinary, yet the escalating inference costs associated with them present challenges in real-world applications. To address these challenges, we propose a novel approach called Early-exiting Speculative Decoding (EESD) with lossless acceleration. Specifically, EESD utilizes a segment of the LLM to generate draft tokens, incorporating Early-exiting structures after the first N layers. To enhance the quality of draft tokens, a self-distillation method is integrated. This early-exiting design not only reduces deployment and training costs but also significantly accelerates the token generation speed. Moreover, we introduce a novel sampling mechanism that leverages Thompson Sampling to regulate the generation processes, automatically determining the quantity of draft tokens in each round. The original LLM is then employed to validate these draft tokens through a single forward pass, and thus guarantees that the final output text maintains a distribution consistent with vanilla auto-regressive decoding. The experimental results on both 13B and 70B models demonstrate that our approach decodes tokens at a markedly accelerated rate compared to prior methods, showing the effectiveness of our approach.

- **中文摘要**: 大型语言模型（LLMs）的最新进展令人瞩目，但其不断攀升的推理成本在实际应用中带来了挑战。为应对这些挑战，我们提出了一种名为无损加速的早期退出推测解码（Early-exiting Speculative Decoding, EESD）的新方法。具体而言，EESD利用LLM的一部分生成草稿令牌，并在前N层之后引入早期退出结构。为提升草稿令牌的质量，我们整合了自蒸馏方法。这种早期退出设计不仅降低了部署和训练成本，还显著加快了令牌生成速度。此外，我们引入了一种新的采样机制，利用汤普森采样来调控生成过程，自动确定每轮生成的草稿令牌数量。随后，原始LLM通过一次前向传递验证这些草稿令牌，从而确保最终输出文本与普通自回归解码的分布一致。在13B和70B模型上的实验结果表明，与先前方法相比，我们的方法在解码令牌时显著提速，展示了我们方法的有效性。

### SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices
- **Authors**: Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin
- **Published**: 2024-06-04
- **Link**: [http://arxiv.org/abs/2406.02532v2](http://arxiv.org/abs/2406.02532v2)
- **Summary**: As large language models gain widespread adoption, running them efficiently becomes crucial. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD. When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities. SpecExec takes the most probable tokens continuation from the draft model to build a "cache" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights.

- **中文摘要**: 随着大型语言模型得到广泛应用，如何高效运行它们变得至关重要。最近关于大语言模型（LLM）推理的研究采用了推测性解码技术，以实现极速加速。然而，大多数这些研究隐含地为高端数据中心硬件设计了算法。在这项工作中，我们提出了相反的问题：我们能在消费级机器上以多快的速度运行大语言模型？消费级GPU已经无法容纳当前最大的模型（500亿+参数），必须将它们卸载到RAM或SSD中。当使用卸载参数运行时，推理引擎可以同时处理数百或数千个标记，而不仅仅是一个标记，这使其自然适合推测性解码。我们提出了SpecExec（推测性执行），一种简单的并行解码方法，能够在每次目标模型迭代中生成多达20个标记，适用于流行的大语言模型家族。SpecExec利用了现代大语言模型中标记概率分布的高峰性和模型输出概率之间的高度一致性。SpecExec从草稿模型中获取最可能的标记延续，为目标模型构建一个“缓存”树，然后在一次遍历中进行验证。通过使用SpecExec，我们展示了在消费级GPU上使用RAM卸载的500亿+参数大语言模型的推理速度，4位量化时为每秒4-6个标记，16位权重时为每秒2-3个标记。

### S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs
- **Authors**: Wei Zhong, Manasa Bharadwaj
- **Published**: 2024-05-30
- **Link**: [http://arxiv.org/abs/2405.20314v2](http://arxiv.org/abs/2405.20314v2)
- **Summary**: Speculative decoding (SD) has attracted a significant amount of research attention due to the substantial speedup it can achieve for LLM inference. However, despite the high speedups they offer, speculative decoding methods often achieve optimal performance on high-end devices or with a substantial GPU memory overhead. Given limited memory and the necessity of quantization, a high-performing model on a high-end GPU can slow down by up to 7 times. To this end, we propose Skippy Simultaneous Speculative Decoding (or S3D), a cost-effective self-speculative SD method based on simultaneous multi-token decoding and mid-layer skipping. When compared against recent effective open-source SD systems, our method has achieved one of the top performance-memory ratios while requiring minimal architecture changes and training data. Leveraging our memory efficiency, we created a smaller yet more effective SD model based on Phi-3. It is 1.4 to 2 times faster than the quantized EAGLE model and operates in half-precision while using less VRAM.

- **中文摘要**: 推测性解码（Speculative Decoding, SD）因其能够显著加速大型语言模型（LLM）的推理过程而受到了大量研究关注。然而，尽管这些方法提供了高速的推理加速，它们通常在高端设备上或在需要大量GPU内存开销的情况下才能达到最佳性能。在内存有限且必须进行量化的前提下，高端GPU上的高性能模型可能会导致推理速度降低多达7倍。为此，我们提出了Skippy同时推测性解码（或称S3D），这是一种基于同时多令牌解码和中层跳过的经济型自推测性SD方法。与近期有效的开源SD系统相比，我们的方法在性能-内存比率上达到了顶尖水平，同时仅需最小的架构改动和训练数据。借助我们的内存效率，我们基于Phi-3创建了一个更小但更有效的SD模型。该模型比量化后的EAGLE模型快1.4至2倍，并且在半精度下运行时使用的VRAM更少。

### SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths
- **Authors**: Kaixuan Huang, Xudong Guo, Mengdi Wang
- **Published**: 2024-05-30
- **Link**: [http://arxiv.org/abs/2405.19715v2](http://arxiv.org/abs/2405.19715v2)
- **Summary**: Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model. Its performance depends on a hyperparameter K -- the candidate length, i.e., the number of candidate tokens for the target model to verify in each round. However, previous methods often use simple heuristics to choose K, which may result in sub-optimal performance. We study the choice of the candidate length K and formulate it as a Markov Decision Process. We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value. Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold. We implement SpecDec++ and apply it to the llama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (an additional 7.2% improvement over the baseline speculative decoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively.

- **中文摘要**: 推测性解码通过利用更小、更快的草稿模型来减少目标大型语言模型的推理延迟。其性能取决于一个超参数K——候选长度，即目标模型在每一轮中需要验证的候选词元数量。然而，以往的方法通常使用简单的启发式方法来选择K，这可能导致性能不佳。我们研究了候选长度K的选择，并将其形式化为马尔可夫决策过程。我们理论上证明了该马尔可夫决策过程的最优策略采取阈值策略的形式，即当获得拒绝的概率超过某个阈值时，当前的推测应停止并进行验证。受此理论启发，我们提出了SpecDec++，这是推测性解码的增强版本，能够动态自适应地确定候选长度。我们在草稿模型上增加了一个经过训练的接受预测头，用于预测候选词元的条件接受概率。当预测到至少有一个词元被拒绝的概率超过某个阈值时，SpecDec++将停止当前的推测。我们实现了SpecDec++，并将其应用于llama-2-chat 7B & 70B模型对。我们的自适应方法在Alpaca数据集上实现了2.04倍的加速（比基线推测性解码额外提升了7.2%）。在GSM8K和HumanEval数据集上，我们的方法分别实现了2.26倍（提升9.4%）和2.23倍（提升11.1%）的加速。

### Nearest Neighbor Speculative Decoding for LLM Generation and Attribution
- **Authors**: Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin
- **Published**: 2024-05-29
- **Link**: [http://arxiv.org/abs/2405.19325v2](http://arxiv.org/abs/2405.19325v2)
- **Summary**: Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B.

- **中文摘要**: 大型语言模型（LLMs）常常产生幻觉，并且缺乏为其生成内容提供归属的能力。半参数化语言模型（如kNN-LM）通过利用非参数化数据存储中的最近邻匹配来优化给定提示的模型输出，从而应对这些局限性。然而，这些模型通常推理速度较慢，且生成的文本不够流畅。本文介绍了一种新颖的半参数化语言建模方法——最近邻推测解码（NEST），该方法能够将任意长度的真实文本片段融入语言模型生成内容中，并为其来源提供归属。NEST在每次推理步骤中进行词元级别的检索，以计算半参数化混合分布，并在语料库中识别有前景的片段延续。随后，它采用一种近似的推测解码程序，接受检索到的片段前缀或生成新词元。NEST在多种知识密集型任务中显著提升了基础语言模型的生成质量和归属率，超越了传统的kNN-LM方法，并在上下文检索增强方面表现出色。此外，NEST大幅提升了生成速度，应用于Llama-2-Chat 70B时，推理时间加速达1.8倍。

### Faster Cascades via Speculative Decoding
- **Authors**: Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, Sanjiv Kumar
- **Published**: 2024-05-29
- **Link**: [http://arxiv.org/abs/2405.19261v1](http://arxiv.org/abs/2405.19261v1)
- **Summary**: Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for "hard" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines.

- **中文摘要**: 级联和推测性解码是两种常见的提高语言模型推理效率的方法。这两种方法都涉及不同大小的模型的交错使用，但通过根本不同的机制：级联采用一种延迟规则，仅在处理“困难”输入时调用较大的模型，而推测性解码则使用推测执行，主要在并行验证模式下调用较大的模型。这些机制提供了不同的优势：从经验上看，级联往往能够产生比较大模型更好的质量，而从理论上讲，推测性解码提供了质量中立性的保证。在本文中，我们通过设计新的推测性级联技术，利用了这两种方法的优点，这些技术通过推测执行来实现其延迟规则。我们描述了推测性级联的最优延迟规则，并采用了一种对最优规则的插件近似。通过在基准语言任务上使用T5模型进行的实验，我们展示了所提出的方法在成本-质量权衡方面优于级联和推测性解码的基线方法。

### Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference
- **Authors**: Hao Mark Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko, Stylianos I. Venieris, Hongxiang Fan
- **Published**: 2024-05-28
- **Link**: [http://arxiv.org/abs/2405.18628v2](http://arxiv.org/abs/2405.18628v2)
- **Summary**: The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput. Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost. To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. Inspired by the human natural language generation process, $PPD$ approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions. Furthermore, we present a hardware-aware dynamic sparse tree technique that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs. Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004$%. More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding, showing up to $1.22\times$ further speed improvement. Our code is available at https://github.com/hmarkc/parallel-prompt-decoding.

- **中文摘要**: 大型语言模型（LLMs）的自回归解码在其硬件性能上导致了显著的开销。尽管最近的研究探索了多种用于多标记生成的推测性解码技术，但这些努力主要集中在提高处理速度如吞吐量上。至关重要的是，它们往往忽视了其他对于实际部署至关重要的指标，如内存消耗和训练成本。为了克服这些限制，我们提出了一种新颖的并行提示解码方法，该方法仅需$0.0002\%$的可训练参数，使得在单个A100-40GB GPU上仅需16小时即可完成高效训练。受人类自然语言生成过程的启发，$PPD$通过使用多个提示标记并行地近似生成未来时间步的输出。这种方法部分恢复了多标记生成所需的条件依赖信息，从而使长距离预测的接受率提高了多达28%。此外，我们提出了一种硬件感知的动态稀疏树技术，该技术自适应地优化此解码方案，以充分利用不同GPU的计算能力。通过在从MobileLlama到Vicuna-13B的一系列LLM上进行的广泛实验，我们的方法在各种基准测试中展示了高达2.49$\times$的加速，并保持了仅$0.0004\%$的极低运行时内存开销。更重要的是，我们的并行提示解码可以作为一种正交优化方法，与现有的推测性解码协同集成，进一步显示出高达1.22$\times$的速度提升。我们的代码可在https://github.com/hmarkc/parallel-prompt-decoding获取。

### Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation
- **Authors**: Boyuan Chen, Mingzhi Zhu, Brendan Dolan-Gavitt, Muhammad Shafique, Siddharth Garg
- **Published**: 2024-05-24
- **Link**: [http://arxiv.org/abs/2405.15842v1](http://arxiv.org/abs/2405.15842v1)
- **Summary**: The rapid development of large language models (LLMs) has led to significant advancements in code completion tasks. While larger models have higher accuracy, they also cost much more to run. Meanwhile, model cascading has been proven effective to conserve computational resources while enhancing accuracy in LLMs on natural language generation tasks. It generates output with the smallest model in a set, and only queries the larger models when it fails to meet predefined quality criteria. However, this strategy has not been used in code completion tasks, primarily because assessing the quality of code completions differs substantially from assessing natural language, where the former relies heavily on the functional correctness. To address this, we propose letting each model generate and execute a set of test cases for their solutions, and use the test results as the cascading threshold. We show that our model cascading strategy reduces computational costs while increases accuracy compared to generating the output with a single model. We also introduce a heuristics to determine the optimal combination of the number of solutions, test cases, and test lines each model should generate, based on the budget. Compared to speculative decoding, our method works on black-box models, having the same level of cost-accuracy trade-off, yet providing much more choices based on the server's budget. Ours is the first work to optimize cost-accuracy trade-off for LLM code generation with model cascading.

- **中文摘要**: 大型语言模型（LLMs）的快速发展在代码补全任务中取得了显著进展。虽然较大的模型具有更高的准确性，但它们的运行成本也更高。同时，模型级联已被证明在自然语言生成任务中能够有效节省计算资源并提高准确性。它使用模型集中最小的模型生成输出，仅在未达到预定义质量标准时才查询更大的模型。然而，这种策略尚未应用于代码补全任务，主要原因是评估代码补全质量与评估自然语言大相径庭，前者高度依赖于功能正确性。为此，我们提出让每个模型为其解决方案生成并执行一组测试用例，并将测试结果作为级联阈值。我们展示了我们的模型级联策略在降低计算成本的同时提高了准确性，相比于仅使用单一模型生成输出。我们还引入了一种启发式方法，根据预算确定每个模型应生成的解决方案数量、测试用例数量和测试行数的最佳组合。与推测性解码相比，我们的方法适用于黑盒模型，具有相同的成本-准确性权衡水平，但根据服务器预算提供了更多的选择。我们的工作是首个通过模型级联优化LLM代码生成成本-准确性权衡的研究。

### A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models
- **Authors**: Mahsa Khoshnoodi, Vinija Jain, Mingye Gao, Malavika Srikanth, Aman Chadha
- **Published**: 2024-05-15
- **Link**: [http://arxiv.org/abs/2405.13019v2](http://arxiv.org/abs/2405.13019v2)
- **Summary**: Despite the crucial importance of accelerating text generation in large language models (LLMs) for efficiently producing content, the sequential nature of this process often leads to high inference latency, posing challenges for real-time applications. Various techniques have been proposed and developed to address these challenges and improve efficiency. This paper presents a comprehensive survey of accelerated generation techniques in autoregressive language models, aiming to understand the state-of-the-art methods and their applications. We categorize these techniques into several key areas: speculative decoding, early exiting mechanisms, and non-autoregressive methods. We discuss each category's underlying principles, advantages, limitations, and recent advancements. Through this survey, we aim to offer insights into the current landscape of techniques in LLMs and provide guidance for future research directions in this critical area of natural language processing.

- **中文摘要**: 尽管在大语言模型（LLMs）中加速文本生成对于高效内容产出具有至关重要的意义，但这一过程的顺序性往往导致高推理延迟，给实时应用带来挑战。为应对这些挑战并提升效率，多种技术已被提出并发展。本文对自回归语言模型中的加速生成技术进行了全面综述，旨在理解当前最先进的方法及其应用。我们将这些技术归类为几个关键领域：推测性解码、早期退出机制和非自回归方法。我们探讨了每一类技术的基本原理、优势、局限性及最新进展。通过此次综述，我们旨在洞察LLMs中技术的当前格局，并为自然语言处理这一关键领域的未来研究方向提供指导。

### EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models
- **Authors**: Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, Yunhe Wang
- **Published**: 2024-05-13
- **Link**: [http://arxiv.org/abs/2405.07542v1](http://arxiv.org/abs/2405.07542v1)
- **Summary**: Speculative decoding emerges as a pivotal technique for enhancing the inference speed of Large Language Models (LLMs). Despite recent research aiming to improve prediction efficiency, multi-sample speculative decoding has been overlooked due to varying numbers of accepted tokens within a batch in the verification phase. Vanilla method adds padding tokens in order to ensure that the number of new tokens remains consistent across samples. However, this increases the computational and memory access overhead, thereby reducing the speedup ratio. We propose a novel method that can resolve the issue of inconsistent tokens accepted by different samples without necessitating an increase in memory or computing overhead. Furthermore, our proposed method can handle the situation where the prediction tokens of different samples are inconsistent without the need to add padding tokens. Sufficient experiments demonstrate the efficacy of our method. Our code is available at https://github.com/niyunsheng/EMS-SD.

- **中文摘要**: 推测性解码作为一种关键技术，旨在提升大型语言模型（LLMs）的推理速度。尽管近期研究致力于提高预测效率，但由于验证阶段内批次中各样本接受令牌数量不一，多样本推测性解码一直未得到充分关注。传统方法通过添加填充令牌来确保各样本新生成令牌数量一致，但这增加了计算和内存访问开销，从而降低了加速比。我们提出了一种新颖的方法，能够在不增加内存或计算开销的前提下，解决不同样本接受令牌数量不一致的问题。此外，我们的方法还能处理不同样本预测令牌不一致的情况，无需添加填充令牌。充分的实验证明了我们方法的有效性。我们的代码已公开，详见https://github.com/niyunsheng/EMS-SD。

### Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models
- **Authors**: Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy Schwartz
- **Published**: 2024-05-07
- **Link**: [http://arxiv.org/abs/2405.04304v4](http://arxiv.org/abs/2405.04304v4)
- **Summary**: Speculative decoding is commonly used for reducing the inference latency of large language models. Its effectiveness depends highly on the speculation lookahead (SL)-the number of tokens generated by the draft model at each iteration. In this work we show that the common practice of using the same SL for all iterations (static SL) is suboptimal. We introduce DISCO (DynamIc SpeCulation lookahead Optimization), a novel method for dynamically selecting the SL. Our experiments with four datasets show that DISCO reaches an average speedup of 10% compared to the best static SL baseline, while generating the exact same text.

- **中文摘要**: 推测性解码常用于减少大型语言模型的推理延迟。其效果高度依赖于推测性前瞻（Speculation Lookahead, SL）——即每次迭代中草稿模型生成的标记数量。本文中我们指出，在所有迭代中使用相同的SL（静态SL）的做法并非最优。我们提出了DISCO（动态推测性前瞻优化），一种新颖的动态选择SL的方法。通过对四个数据集的实验，我们发现DISCO相较于最佳的静态SL基线，平均加速达到了10%，同时生成了完全相同的文本。

### Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge
- **Authors**: Bin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin Cui
- **Published**: 2024-05-01
- **Link**: [http://arxiv.org/abs/2405.00263v1](http://arxiv.org/abs/2405.00263v1)
- **Summary**: Large language models (LLMs) suffer from low efficiency as the mismatch between the requirement of auto-regressive decoding and the design of most contemporary GPUs. Specifically, billions to trillions of parameters must be loaded to the GPU cache through its limited memory bandwidth for computation, but only a small batch of tokens is actually computed. Consequently, the GPU spends most of its time on memory transfer instead of computation. Recently, parallel decoding, a type of speculative decoding algorithms, is becoming more popular and has demonstrated impressive efficiency improvement in generation. It introduces extra decoding heads to large models, enabling them to predict multiple subsequent tokens simultaneously and verify these candidate continuations in a single decoding step. However, this approach deviates from the training objective of next token prediction used during pre-training, resulting in a low hit rate for candidate tokens. In this paper, we propose a new speculative decoding algorithm, Clover, which integrates sequential knowledge into the parallel decoding process. This enhancement improves the hit rate of speculators and thus boosts the overall efficiency. Clover transmits the sequential knowledge from pre-speculated tokens via the Regressive Connection, then employs an Attention Decoder to integrate these speculated tokens. Additionally, Clover incorporates an Augmenting Block that modifies the hidden states to better align with the purpose of speculative generation rather than next token prediction. The experiment results demonstrate that Clover outperforms the baseline by up to 91% on Baichuan-Small and 146% on Baichuan-Large, respectively, and exceeds the performance of the previously top-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on Baichuan-Large, respectively.

- **中文摘要**: 大型语言模型（LLMs）在效率上存在不足，这是由于自回归解码的需求与大多数当代GPU设计之间的不匹配所导致的。具体来说，数十亿到数万亿的参数必须通过有限的内存带宽加载到GPU缓存中进行计算，但实际上只有一小批次的令牌被实际计算。因此，GPU大部分时间都花在内存传输上，而不是计算上。最近，一种名为并行解码的推测性解码算法逐渐流行起来，并在生成效率上展示了显著的改进。它为大型模型引入了额外的解码头，使其能够同时预测多个后续令牌，并在单个解码步骤中验证这些候选的延续。然而，这种方法偏离了预训练期间使用的下一个令牌预测的训练目标，导致候选令牌的命中率较低。在本文中，我们提出了一种新的推测性解码算法，名为Clover，它将顺序知识整合到并行解码过程中。这一改进提高了推测器的命中率，从而提升了整体效率。Clover通过回归连接将顺序知识从预推测的令牌传递，然后使用注意力解码器整合这些推测的令牌。此外，Clover还包含一个增强块，用于修改隐藏状态，使其更好地符合推测生成的目的，而不是下一个令牌预测。实验结果表明，Clover在Baichuan-Small上比基线高出最多91%，在Baichuan-Large上高出146%，并且在Baichuan-Small上比之前表现最好的方法Medusa高出最多37%，在Baichuan-Large上高出57%。

### Accelerating Production LLMs with Combined Token/Embedding Speculators
- **Authors**: Davis Wertheimer, Joshua Rosenkranz, Thomas Parnell, Sahil Suneja, Pavithra Ranganathan, Raghu Ganti, Mudhakar Srivatsa
- **Published**: 2024-04-29
- **Link**: [http://arxiv.org/abs/2404.19124v2](http://arxiv.org/abs/2404.19124v2)
- **Summary**: This technical report describes the design and training of novel speculative decoding draft models, for accelerating the inference speeds of large language models in a production environment. By conditioning draft predictions on both context vectors and sampled tokens, we can train our speculators to efficiently predict high-quality n-grams, which the base model then accepts or rejects. This allows us to effectively predict multiple tokens per inference forward pass, accelerating wall-clock inference speeds of highly optimized base model implementations by a factor of 2-3x. We explore these initial results and describe next steps for further improvements.

- **中文摘要**: 本技术报告描述了新型推测解码草稿模型的设计和训练，旨在加速大型语言模型在生产环境中的推理速度。通过将草稿预测条件设置为上下文向量和采样令牌，我们可以训练推测器高效预测高质量的n-gram，然后由基础模型接受或拒绝。这使得我们能够在每次推理前向传递中有效预测多个令牌，将高度优化的基础模型实现的推理速度加速2-3倍。我们探讨了这些初步结果，并描述了进一步改进的下一步措施。

### Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting
- **Authors**: Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, Yunhe Wang
- **Published**: 2024-04-29
- **Link**: [http://arxiv.org/abs/2404.18911v1](http://arxiv.org/abs/2404.18911v1)
- **Summary**: Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution. However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly. Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework \emph{Kangaroo}, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model. We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability. It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model. To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens. Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold. Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo. Under single-sequence verification, Kangaroo achieves speedups up to $1.68\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\% fewer additional parameters (67M compared to 591M). The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.

- **中文摘要**: 推测性解码在加速大型语言模型的推理过程中展示了其有效性，同时保持了稳定的采样分布。然而，传统方法通过训练一个独立的草稿模型来达到令人满意的令牌接受率可能会带来高昂的成本。受早期退出机制的启发，我们提出了一种新颖的自推测性解码框架——**袋鼠（Kangaroo）**，该框架使用一个固定的浅层子网络作为自草稿模型，剩余的层则作为更大的目标模型。我们在子网络之上训练了一个轻量且高效的适配器模块，以弥合子网络与完整模型表示能力之间的差距。值得注意的是，自草稿模型的推理延迟相对于大型模型可能不再可以忽略不计，因此需要在最小化小型模型草稿步骤的同时提高令牌接受率。为了应对这一挑战，我们引入了一种额外的早期退出机制来生成草稿令牌。具体来说，一旦当前令牌的置信度低于某个阈值，我们在草稿阶段就会停止小型模型的后续预测。在Spec-Bench上的广泛实验证明了袋鼠的有效性。在单序列验证下，袋鼠在Spec-Bench上实现了高达1.68倍的加速，超过了Medusa-1，减少了88.7%的额外参数（67M对比591M）。袋鼠的代码可在https://github.com/Equationliu/Kangaroo获取。

### LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding
- **Authors**: Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu
- **Published**: 2024-04-25
- **Link**: [http://arxiv.org/abs/2404.16710v2](http://arxiv.org/abs/2404.16710v2)
- **Summary**: We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.

- **中文摘要**: 我们提出了LayerSkip，这是一种加速大型语言模型（LLMs）推理的端到端解决方案。首先，在训练过程中，我们应用了层级dropout，早期层的dropout率较低，而后期层的dropout率较高，并且所有transformer层共享相同的提前退出损失。其次，在推理阶段，我们展示了这种训练方法提高了早期层提前退出的准确性，而无需向模型中添加任何辅助层或模块。第三，我们提出了一种新颖的自推测解码方案，其中我们在早期层退出，并利用模型的剩余层进行验证和修正。我们提出的自推测解码方法相比其他推测解码方法具有更小的内存占用，并受益于草稿和验证阶段的共享计算和激活。我们在不同类型的训练上对不同大小的Llama模型进行了实验：从头开始的预训练、持续预训练、特定数据域的微调以及特定任务的微调。我们实现了我们的推理解决方案，并展示了在CNN/DM文档摘要任务上最高2.16倍的加速，在编码任务上1.82倍的加速，以及在TOPv2语义解析任务上2.0倍的加速。

### BASS: Batched Attention-optimized Speculative Sampling
- **Authors**: Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, Anoop Deoras
- **Published**: 2024-04-24
- **Link**: [http://arxiv.org/abs/2404.15778v2](http://arxiv.org/abs/2404.15778v2)
- **Summary**: Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.

- **中文摘要**: 推测性解码作为一种强大的方法，已经在提高大型语言模型托管中的延迟和吞吐量方面崭露头角。然而，大多数现有的实现都集中在生成单个序列上。现实世界中的生成式AI应用通常需要多个响应，如何在批处理环境中执行推测性解码的同时保持其延迟优势，提出了非同小可的挑战。本文描述了一种批量推测性解码系统，该系统在多序列生成延迟方面达到了新的技术水平，并展示了卓越的GPU利用率以及在时间预算内的生成质量。例如，对于单个A100 GPU上的7.8B规模模型和8的批量大小，每个序列的生成速度平均为每令牌5.8毫秒，总体吞吐量为每秒1.1K令牌。这些结果代表了最先进的延迟，并且比优化的常规解码快2.15倍。在常规解码无法完成的时限内，我们的系统能够生成序列，HumanEval Pass@First达到43%，Pass@All达到61%，远远超过了单序列推测性解码的可行性。在解码过程中，我们的峰值GPU利用率高达15.8%，是常规解码最高值的3倍以上，大约是单序列推测性解码的10倍。

### Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models
- **Authors**: Chen Zhang, Zhuorui Liu, Dawei Song
- **Published**: 2024-04-23
- **Link**: [http://arxiv.org/abs/2404.14897v1](http://arxiv.org/abs/2404.14897v1)
- **Summary**: With the increasingly giant scales of (causal) large language models (LLMs), the inference efficiency comes as one of the core concerns along the improved performance. In contrast to the memory footprint, the latency bottleneck seems to be of greater importance as there can be billions of requests to a LLM (e.g., GPT-4) per day. The bottleneck is mainly due to the autoregressive innateness of LLMs, where tokens can only be generated sequentially during decoding. To alleviate the bottleneck, the idea of speculative execution, which originates from the field of computer architecture, is introduced to LLM decoding in a \textit{draft-then-verify} style. Under this regime, a sequence of tokens will be drafted in a fast pace by utilizing some heuristics, and then the tokens shall be verified in parallel by the LLM. As the costly sequential inference is parallelized, LLM decoding speed can be significantly boosted. Driven by the success of LLMs in recent couple of years, a growing literature in this direction has emerged. Yet, there lacks a position survey to summarize the current landscape and draw a roadmap for future development of this promising area. To meet this demand, we present the very first survey paper that reviews and unifies literature of speculative execution in LLMs (e.g., blockwise parallel decoding, speculative decoding, etc.) in a comprehensive framework and a systematic taxonomy. Based on the taxonomy, we present a critical review and comparative analysis of the current arts. Finally we highlight various key challenges and future directions to further develop the area.

- **中文摘要**: 随着（因果）大型语言模型（LLMs）规模的日益庞大，推理效率成为与性能提升并列的核心关注点之一。相较于内存占用，延迟瓶颈显得更为重要，因为每天可能有数十亿次请求涌向一个LLM（例如GPT-4）。这一瓶颈主要源于LLMs的自回归本质，即在解码过程中，标记只能按顺序生成。为缓解这一瓶颈，源自计算机体系结构的推测执行思想被引入到LLM解码中，采用了一种“先草拟后验证”的方式。在这种模式下，通过利用某些启发式方法，可以快速草拟出一串标记，然后由LLM并行验证这些标记。由于昂贵的顺序推理过程得以并行化，LLM的解码速度可以显著提升。近年来，随着LLMs的巨大成功，这一领域的文献数量不断增长。然而，目前尚缺乏一份综述来总结当前的研究现状并为这一充满前景的领域绘制未来发展路线图。为满足这一需求，我们撰写了首篇综述论文，全面回顾并统一了LLMs中推测执行的相关文献（如分块并行解码、推测解码等），构建了一个综合框架和系统分类体系。基于这一分类体系，我们对当前的研究成果进行了批判性评述和比较分析。最后，我们指出了该领域面临的各种关键挑战，并展望了未来的发展方向。

### Parallel Decoding via Hidden Transfer for Lossless Large Language Model Acceleration
- **Authors**: Pengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, Dongyan Zhao
- **Published**: 2024-04-18
- **Link**: [http://arxiv.org/abs/2404.12022v1](http://arxiv.org/abs/2404.12022v1)
- **Summary**: Large language models (LLMs) have recently shown remarkable performance across a wide range of tasks. However, the substantial number of parameters in LLMs contributes to significant latency during model inference. This is particularly evident when utilizing autoregressive decoding methods, which generate one token in a single forward process, thereby not fully capitalizing on the parallel computing capabilities of GPUs. In this paper, we propose a novel parallel decoding approach, namely \textit{hidden transfer}, which decodes multiple successive tokens simultaneously in a single forward pass. The idea is to transfer the intermediate hidden states of the previous context to the \textit{pseudo} hidden states of the future tokens to be generated, and then the pseudo hidden states will pass the following transformer layers thereby assimilating more semantic information and achieving superior predictive accuracy of the future tokens.   Besides, we use the novel tree attention mechanism to simultaneously generate and verify multiple candidates of output sequences, which ensure the lossless generation and further improves the generation efficiency of our method. Experiments demonstrate the effectiveness of our method. We conduct a lot of analytic experiments to prove our motivation. In terms of acceleration metrics, we outperform all the single-model acceleration techniques, including Medusa and Self-Speculative decoding.

- **中文摘要**: 大型语言模型（LLMs）在广泛的任务中近期展示了显著的性能。然而，LLMs中大量的参数导致了模型推理过程中的显著延迟。在使用自回归解码方法时，这一问题尤为明显，因为自回归解码每次前向过程只生成一个token，未能充分利用GPU的并行计算能力。在本文中，我们提出了一种新颖的并行解码方法，即\textit{隐藏传递}，该方法在一次前向过程中同时解码多个连续的token。其核心思想是将前文上下文的中间隐藏状态传递给待生成未来token的\textit{伪}隐藏状态，随后伪隐藏状态通过后续的transformer层，从而吸收更多的语义信息，实现对未来token的更优预测准确性。此外，我们采用了新颖的树状注意力机制，以同时生成和验证多个输出序列的候选，确保生成过程的无损性，并进一步提升了我们方法的生成效率。实验证明了我们方法的有效性。我们进行了大量的分析实验以验证我们的动机。在加速指标方面，我们优于所有单一模型的加速技术，包括Medusa和自推测解码。

### TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding
- **Authors**: Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen
- **Published**: 2024-04-18
- **Link**: [http://arxiv.org/abs/2404.11912v3](http://arxiv.org/abs/2404.11912v3)
- **Summary**: With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\times$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token$\unicode{x2014}$only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\times$ on our optimized offloading system. Additionally, TriForce performs 4.86$\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.

- **中文摘要**: 随着大型语言模型（LLMs）在长内容生成中的广泛部署，对高效长序列推理支持的需求日益增加。然而，存储以避免重新计算的关键值（KV）缓存已成为一个关键瓶颈，其大小随序列长度线性增长。由于LLMs的自回归特性，每次生成令牌时都需要加载整个KV缓存，导致计算核心利用率低和延迟高。尽管提出了各种KV缓存压缩方法来缓解这一问题，但它们在生成质量上有所下降。我们引入了TriForce，这是一个可扩展的长序列生成层次化推测解码系统。该方法利用原始模型权重和通过检索的动态稀疏KV缓存作为草稿模型，作为层次结构中的中间层，并由一个更小的模型进一步推测以减少其草稿延迟。TriForce不仅在A100 GPU上实现了Llama2-7B-128K高达2.31倍的加速，而且在处理更长上下文时展示了可扩展性。在两块RTX 4090 GPU的卸载设置下，TriForce实现了0.108秒/令牌的速度——仅比A100上的自回归基线慢一半，而在我们优化的卸载系统上达到了7.78倍的加速。此外，TriForce在单块RTX 4090 GPU上的表现比DeepSpeed-Zero-Inference快4.86倍。TriForce的鲁棒性通过其在各种温度下始终出色的表现得以体现。代码可在https://github.com/Infini-AI-Lab/TriForce获取。

### On Speculative Decoding for Multimodal Large Language Models
- **Authors**: Mukul Gagrani, Raghavv Goel, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott
- **Published**: 2024-04-13
- **Link**: [http://arxiv.org/abs/2404.08856v1](http://arxiv.org/abs/2404.08856v1)
- **Summary**: Inference with Multimodal Large Language Models (MLLMs) is slow due to their large-language-model backbone which suffers from memory bandwidth bottleneck and generates tokens auto-regressively. In this paper, we explore the application of speculative decoding to enhance the inference efficiency of MLLMs, specifically the LLaVA 7B model. We show that a language-only model can serve as a good draft model for speculative decoding with LLaVA 7B, bypassing the need for image tokens and their associated processing components from the draft model. Our experiments across three different tasks show that speculative decoding can achieve a memory-bound speedup of up to 2.37$\times$ using a 115M parameter language model that we trained from scratch. Additionally, we introduce a compact LLaVA draft model incorporating an image adapter, which shows marginal performance gains in image captioning while maintaining comparable results in other tasks.

- **中文摘要**: 多模态大语言模型（MLLMs）的推理速度较慢，这主要归因于其庞大的语言模型骨干网络存在内存带宽瓶颈，并且以自回归方式生成标记。在本文中，我们探讨了将推测性解码应用于提升MLLMs推理效率的可能性，特别是针对LLaVA 7B模型。我们发现，仅使用语言模型作为推测性解码的草稿模型，可以有效绕过对图像标记及其相关处理组件的需求。通过在三个不同任务上的实验，我们验证了推测性解码能够利用我们从头训练的115M参数语言模型，实现高达2.37倍的内存受限加速。此外，我们还引入了一种包含图像适配器的紧凑LLaVA草稿模型，该模型在图像描述任务中表现出轻微的性能提升，同时在其他任务中保持了相当的成果。

### DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference
- **Authors**: Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin
- **Published**: 2024-03-30
- **Link**: [http://arxiv.org/abs/2404.00242v2](http://arxiv.org/abs/2404.00242v2)
- **Summary**: Given the increasing demand for tree-structured interactions with LLMs, we introduce DeFT (Decoding with Flash Tree-Attention), an IO-aware tree attention algorithm tailored for tree-structured inference. Unlike traditional sequence-based decoding, tree-structured decoding better accommodates modern task requirements, including self-consistency, few-shot prompting, multi-step reasoning, and multi-model/head coordination. However, existing sequence-based inference systems are ill-suited for tree-structured decoding, resulting in redundancy in computation, memory footprints, and memory access, thereby undermining inference efficiency. To address this challenge, DeFT maintains memory-efficient attention calculation with low memory footprints through two key stages: (1) QKV Preparation: We propose a KV-Guided Grouping Strategy with Tree Split to intelligently group QKV, optimizing GPU resource utilization while minimizing memory reads/writes for KV cache between GPU global memory and on-chip shared memory; (2)Attention Calculation: We compute partial attention of each QKV group in a fused kernel and employ a Tree-topology-aware Global Reduction strategy to obtain final attention. By reducing 73-99% KV cache IO and nearly 100% IO for partial results during attention calculation (e.g., Softmax), DeFT achieves up to 2.52/3.82x speedup in the end-to-end/attention latency across three practical tree-based workloads: namely, few-shot prompting, multi-step reasoning, and speculative decoding, over state-of-the-art attention algorithms.

- **中文摘要**: 鉴于对与大型语言模型（LLMs）进行树状交互的需求日益增长，我们引入了DeFT（Decoding with Flash Tree-Attention），这是一种专为树状结构推理设计的IO感知树注意力算法。与传统的基于序列的解码不同，树状结构解码更好地适应了现代任务需求，包括自一致性、少样本提示、多步推理以及多模型/多头协调。然而，现有的基于序列的推理系统并不适合树状结构解码，导致计算、内存占用和内存访问的冗余，从而降低了推理效率。为了应对这一挑战，DeFT通过两个关键阶段实现了内存高效的注意力计算，同时保持了低内存占用：（1）QKV准备：我们提出了一种KV引导的分组策略与树分割方法，智能地对QKV进行分组，优化GPU资源利用率，同时最小化GPU全局内存与片上共享内存之间KV缓存的读写操作；（2）注意力计算：我们在融合内核中计算每个QKV组的局部注意力，并采用一种树状拓扑感知的全局归约策略来获得最终的注意力。通过减少73-99%的KV缓存IO和接近100%的注意力计算过程中局部结果（如Softmax）的IO，DeFT在三种实际的树状工作负载（即少样本提示、多步推理和推测性解码）上，相对于最先进的注意力算法，实现了高达2.52/3.82倍的端到端/注意力延迟加速。

### SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens
- **Authors**: Chengbo Liu, Yong Zhu
- **Published**: 2024-03-27
- **Link**: [http://arxiv.org/abs/2403.18647v2](http://arxiv.org/abs/2403.18647v2)
- **Summary**: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.

- **中文摘要**: 我们提出了一种通过语义自适应标记（SDSAT）进行推测解码的大语言模型（LLM）加速方案。该设计的主要目标是提高LLM模型生成草稿标记的准确性，同时不降低模型的精度。核心策略包括：1）通过引入具有灵活解码能力的语义自适应标记来微调模型，这些标记在不改变模型结构的情况下，能够生成高质量的草稿标记。2）采用不影响标准标记的训练方法，使模型在其原有框架上获得并行解码能力，且训练开销最小化。3）我们设计了“两步草稿-然后验证”生成策略，结合贪心搜索和核采样。在CodeLlama-13B和7B模型上进行的实验分别实现了超过3.5倍和3.0倍的速度提升。请参阅https://github.com/hasuoshenyun/SDSAT。

### Block Verification Accelerates Speculative Decoding
- **Authors**: Ziteng Sun, Uri Mendlovic, Yaniv Leviathan, Asaf Aharoni, Ahmad Beirami, Jae Hun Ro, Ananda Theertha Suresh
- **Published**: 2024-03-15
- **Link**: [http://arxiv.org/abs/2403.10444v2](http://arxiv.org/abs/2403.10444v2)
- **Summary**: Speculative decoding is an effective method for lossless acceleration of large language models during inference. It uses a fast model to draft a block of tokens which are then verified in parallel by the target model, and provides a guarantee that the output is distributed identically to a sample from the target model. In prior works, draft verification is performed independently token-by-token. Surprisingly, we show that this approach is not optimal. We propose Block Verification, a simple draft verification algorithm that verifies the entire block jointly and provides additional wall-clock speedup. We prove that the proposed mechanism is optimal in the expected number of tokens produced each iteration and specifically is never worse than the standard token-level verification. Empirically, block verification provides modest but consistent wall-clock speedups over the standard token verification algorithm of 5%-8% in a range of tasks and datasets. Given that block verification does not increase code complexity, maintains the strong lossless guarantee of the standard speculative decoding verification algorithm, cannot deteriorate performance, and, in fact, consistently improves it, it can be used as a good default in speculative decoding implementations.

- **中文摘要**: 推测性解码是一种在推理过程中实现大型语言模型无损加速的有效方法。它利用一个快速模型来草拟一组标记，然后由目标模型并行验证这些标记，并保证输出与从目标模型中采样的分布完全相同。在先前的研究中，草稿验证是逐个标记独立进行的。令人惊讶的是，我们发现这种方法并非最优。我们提出了一种名为“块验证”的简单草稿验证算法，该算法联合验证整个块，并提供了额外的时钟加速。我们证明了所提出的机制在每次迭代中生成的预期标记数量方面是最优的，并且明确指出它永远不会比标准的逐标记验证更差。从经验上看，块验证在各种任务和数据集上，相比标准的逐标记验证算法，提供了5%-8%的适度但持续的时钟加速。鉴于块验证不会增加代码复杂性，保持了标准推测性解码验证算法的强无损保证，不会降低性能，并且实际上始终能提升性能，因此它可以作为推测性解码实现中的一个良好默认选项。

### Recurrent Drafter for Fast Speculative Decoding in Large Language Models
- **Authors**: Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng
- **Published**: 2024-03-14
- **Link**: [http://arxiv.org/abs/2403.09919v3](http://arxiv.org/abs/2403.09919v3)
- **Summary**: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attention structure only for inference in Medusa. We empirically demonstrate the effectiveness of the proposed method on several popular open source language models, along with a comprehensive analysis of the trade-offs involved in adopting this approach.

- **中文摘要**: 本文介绍了一种改进的推测性解码方法，旨在提高大型语言模型服务的效率。我们的方法结合了两种成熟技术的优势：经典的二模型推测性解码方法和近期出现的单模型方法——Medusa。受Medusa启发，我们的方法采用了单模型策略进行推测性解码。然而，我们的方法通过使用一个轻量级的草稿头，并采用循环依赖设计，与经典推测性解码中使用的小型草稿模型在本质上相似，但避免了完整Transformer架构的复杂性。由于循环依赖，我们可以利用草稿头快速使用束搜索过滤掉不理想的候选者。这种方法结合了单模型设计的简洁性，并避免了Medusa中仅用于推理的数据依赖树注意力结构的创建。我们通过在几个流行的开源语言模型上进行实验，验证了所提出方法的有效性，并全面分析了采用这种方法所涉及的权衡。

### Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
- **Authors**: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott
- **Published**: 2024-02-29
- **Link**: [http://arxiv.org/abs/2403.00858v4](http://arxiv.org/abs/2403.00858v4)
- **Summary**: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional alignment procedure. For the finetuning step, we use instruction-response pairs generated by target model for distillation in plausible data distribution, and propose a new Total Variation Distance++ (TVD++) loss that incorporates variance reduction techniques inspired from the policy gradient method in reinforcement learning. Our empirical results show that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3 block efficiency and 2.4$\times$ speed-up relative to autoregressive decoding on various tasks with no further task-specific fine-tuning.

- **中文摘要**: 大型语言模型（LLMs）的文本生成由于其自回归特性、庞大的参数数量以及有限的内存带宽，通常会导致较低的token生成速率，因此被认为是内存受限的。推测性解码已被提出作为加速LLM推理的解决方案。然而，由于现代开源LLM家族（如Llama 2 7B）中通常没有现成的草稿模型，因此需要训练一个高质量的草稿模型以通过推测性解码实现推理加速。在本文中，我们提出了一种简单的草稿模型训练框架，该框架直接对齐于具备聊天能力的靶模型。利用该框架，我们训练了Llama 2 Chat Drafter 115M，这是一个适用于Llama 2 Chat 7B或更大模型的草稿模型，其大小仅为原始模型的1.64%。我们的训练框架仅包括预训练、蒸馏数据集生成以及使用知识蒸馏的微调，无需额外的对齐过程。在微调步骤中，我们使用靶模型生成的指令-响应对进行蒸馏，这些数据具有合理的数据分布，并提出了一种新的总变差距离++（TVD++）损失，该损失结合了从强化学习中的策略梯度方法中启发而来的方差减少技术。我们的实证结果表明，在各种任务上，Llama 2 Chat Drafter 115M结合推测性解码，相对于自回归解码，实现了高达2.3倍的块效率和2.4倍的速度提升，且无需进一步的任务特定微调。

### Minions: Accelerating Large Language Model Inference with Adaptive and Collective Speculative Decoding
- **Authors**: Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng, Xin You, Yongjun Bao, Yi Liu, Zhongzhi Luan, Depei Qian
- **Published**: 2024-02-24
- **Link**: [http://arxiv.org/abs/2402.15678v1](http://arxiv.org/abs/2402.15678v1)
- **Summary**: Large language models (LLM) have recently attracted surging interest due to their outstanding capabilities across various domains. However, enabling efficient LLM inference is challenging due to its autoregressive decoding that generates tokens only one at a time. Although research works apply pruning or quantization to speed up LLM inference, they typically require fine-tuning the LLM, incurring significant time and economic costs. Meanwhile, speculative decoding has been proposed to use small speculative models (SSMs) to accelerate the inference of LLM. However, the low acceptance rate of SSM and the high verification cost of LLM prohibit further performance improvement of inference. In this paper, we propose Minions, an LLM inference system that accelerates LLM inference with a collective and adaptive speculative generation. Specifically, Minions proposes a majority-voted mechanism to leverage multiple SSMs to jointly speculate the outputs of LLM, which improves the inference performance without introducing prohibitive computation costs for LLM. To better trade off the number of tokens speculated from SSM and the verification cost of LLM, Minions proposes an adaptive mechanism to dynamically determine the optimal speculation length of SSM, which can achieve better inference performance across different models, datasets, and hyper-parameters. In addition, Minions decouples the SSM decoding and LLM verification efficiently and adopts a pipelined execution mechanism to further improve the inference performance of LLM. By comparing with the state-of-the-art LLM inference systems, we demonstrate that Minions can achieve higher inference throughput and lower inference time.

- **中文摘要**: 大型语言模型（LLM）因其跨多个领域的卓越能力，近期引起了广泛关注。然而，由于其自回归解码方式，即一次只能生成一个标记，使得高效的LLM推理变得极具挑战性。尽管已有研究通过剪枝或量化来加速LLM推理，但这些方法通常需要对LLM进行微调，从而带来显著的时间和经济成本。同时，推测性解码已被提出，通过使用小型推测模型（SSM）来加速LLM的推理。然而，SSM的低接受率和LLM的高验证成本限制了推理性能的进一步提升。本文中，我们提出了Minions，这是一个通过集体和自适应推测生成来加速LLM推理的系统。具体而言，Minions提出了一种多数投票机制，利用多个SSM共同推测LLM的输出，从而在不引入LLM的过高计算成本的情况下提升推理性能。为了更好地权衡从SSM推测的标记数量与LLM的验证成本，Minions提出了一种自适应机制，动态确定SSM的最佳推测长度，从而在不同模型、数据集和超参数下实现更好的推理性能。此外，Minions高效地解耦了SSM解码与LLM验证，并采用流水线执行机制以进一步提高LLM的推理性能。通过与最先进的LLM推理系统进行比较，我们展示了Minions能够实现更高的推理吞吐量和更低的推理时间。

### Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement
- **Authors**: Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher Lott
- **Published**: 2024-02-21
- **Link**: [http://arxiv.org/abs/2402.14160v2](http://arxiv.org/abs/2402.14160v2)
- **Summary**: Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the diversity of the tree. During RSD's drafting, the tree is built by either Gumbel-Top-$k$ trick that draws tokens without replacement in parallel or Stochastic Beam Search that samples sequences without replacement while early-truncating unlikely draft sequences and reducing the computational cost of LLM. We empirically evaluate RSD with Llama 2 and OPT models, showing that RSD outperforms the baseline methods, consistently for fixed draft sequence length and in most cases for fixed computational budgets at LLM.

- **中文摘要**: 推测性解码是一种针对大型语言模型（LLMs）的推理加速方法，其中一个小型语言模型生成一个草稿标记序列，该序列随后由目标LLM并行验证。最近的研究通过建立草稿标记树，将这一方法推进，实现了优于单一序列推测性解码的性能。然而，这些工作在树的每一层独立生成标记，未能充分利用树的整体多样性。此外，它们的实证优越性仅在固定长度的序列上得到展示，隐含地为基于树的方法赋予了更多的计算资源给LLM。尽管固定目标计算预算对资源受限设备的重要性，但现有研究中没有任何一项在固定计算预算下进行实证研究。我们提出了递归推测性解码（RSD），这是一种新颖的基于树的方法，通过无放回抽样草稿标记，最大化树的多样性。在RSD的草稿生成过程中，树的构建通过Gumbel-Top-$k$技巧并行无放回地抽取标记，或通过随机束搜索无放回地抽样序列，同时早期截断不太可能的草稿序列，减少LLM的计算成本。我们通过Llama 2和OPT模型对RSD进行了实证评估，结果显示RSD在固定草稿序列长度和大多数情况下在固定LLM计算预算下均优于基线方法。

### Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding
- **Authors**: Weilin Zhao, Yuxiang Huang, Xu Han, Wang Xu, Chaojun Xiao, Xinrong Zhang, Yewei Fang, Kaihuo Zhang, Zhiyuan Liu, Maosong Sun
- **Published**: 2024-02-21
- **Link**: [http://arxiv.org/abs/2402.13720v2](http://arxiv.org/abs/2402.13720v2)
- **Summary**: Speculative decoding is a widely used method that accelerates the generation process of large language models (LLMs) with no compromise in model performance. It achieves this goal by using an existing smaller model for drafting and then employing the target LLM to verify the draft in a low-cost parallel manner. Under such a drafting-verification framework, drafting efficiency has become a bottleneck in the final speedup of speculative decoding. Therefore, generating longer drafts at less cost can lead to better decoding speedup. To achieve this, we introduce Ouroboros, which can generate draft phrases to parallelize the drafting process and meanwhile lengthen drafts in a training-free manner. The experimental results on various typical text generation tasks show that Ouroboros can achieve speedups of up to $2.4\times$ over speculative decoding and $3.9\times$ over vanilla decoding, without fine-tuning draft and target models.

- **中文摘要**: 推测性解码是一种广泛使用的方法，它能够在不牺牲大型语言模型（LLMs）性能的前提下，加速其生成过程。该方法通过利用现有的较小模型进行草稿拟定，然后以低成本的并行方式使用目标LLM来验证草稿，从而实现这一目标。在这种草稿-验证框架下，草稿拟定的效率已成为最终加速效果的瓶颈。因此，以更低的成本生成更长的草稿能够带来更好的解码加速效果。为此，我们引入了Ouroboros，它能够生成草稿短语以并行化草稿拟定过程，同时以无需训练的方式延长草稿长度。在各种典型文本生成任务的实验结果表明，Ouroboros相较于推测性解码可实现高达2.4倍的加速，相较于传统解码则可达到3.9倍的加速，且无需对草稿和目标模型进行微调。

### Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding
- **Authors**: Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, Beidi Chen
- **Published**: 2024-02-19
- **Link**: [http://arxiv.org/abs/2402.12374v2](http://arxiv.org/abs/2402.12374v2)
- **Summary**: As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\times$, $3.73\times$, and $2.27\times$. For offloading setting on L40, Sequoia achieves as low as 0.56 s/token for exact Llama2-70B inference latency, which is $9.96\times$ on our optimized offloading system (5.6 s/token), $9.7\times$ than DeepSpeed-Zero-Inference, $19.5\times$ than Huggingface Accelerate.

- **中文摘要**: 随着大型语言模型（LLMs）的使用日益广泛，如何高效地进行这些模型的推理变得越来越重要。尽管推测性解码最近作为一种有前景的加速推理方向出现，但现有方法在扩展到更大的推测预算、适应不同的超参数和硬件方面存在局限性。本文介绍了Sequoia，一种可扩展、稳健且硬件感知的推测性解码算法。为了实现更好的可扩展性，Sequoia引入了一种动态规划算法来寻找推测性标记的最佳树结构。为了实现稳健的推测性能，Sequoia采用了一种新颖的采样和验证方法，该方法在不同的解码温度下均优于先前的工作。最后，Sequoia引入了一个硬件感知的树优化器，通过自动选择给定硬件平台的标记树大小和深度，最大化推测性能。评估显示，Sequoia在A100上将Llama2-7B、Llama2-13B和Vicuna-33B的解码速度分别提高了最多4.04倍、3.73倍和2.27倍。在L40的卸载设置下，Sequoia实现了低至0.56秒/标记的精确Llama2-70B推理延迟，这是我们优化后的卸载系统（5.6秒/标记）的9.96倍，比DeepSpeed-Zero-Inference快9.7倍，比Huggingface Accelerate快19.5倍。

### Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
- **Authors**: Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, Rong Xiao
- **Published**: 2024-02-19
- **Link**: [http://arxiv.org/abs/2402.11809v3](http://arxiv.org/abs/2402.11809v3)
- **Summary**: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality.

- **中文摘要**: 本研究旨在加速拥有数十亿参数的大型语言模型（LLMs）的推理速度。我们提出了**S**mart **P**arallel **A**uto-**C**orrect **E**ncoding（SPACE），这是一种创新方法，旨在实现LLMs的无损加速。通过整合半自回归推理和推测性解码能力，SPACE独特地使自回归LLMs能够并行化令牌生成和验证。这一过程通过专门的半自回归监督微调实现，使现有LLMs具备同时预测多个令牌的能力。此外，一种自动校正解码算法促进了在单次模型调用中同时生成和验证令牌序列。通过对一系列LLMs进行广泛实验，SPACE在HumanEval-X上展示了2.7x-4.0x的推理加速，同时保持了输出质量。

### Speculative Streaming: Fast LLM Inference without Auxiliary Models
- **Authors**: Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, Mahyar Najibi
- **Published**: 2024-02-16
- **Link**: [http://arxiv.org/abs/2402.11131v1](http://arxiv.org/abs/2402.11131v1)
- **Summary**: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while using ~10000X fewer extra parameters, making it well-suited for resource-constrained devices.

- **中文摘要**: 推测性解码是一种显著的技术，通过辅助草稿模型的预测来加速大型目标语言模型的推理。尽管有效，但在特定应用场景中，通常需要对草稿模型和目标模型进行微调以实现高接受率。随着下游任务数量的增加，这些草稿模型给推理系统带来了显著的复杂性。我们提出了推测性流式处理（Speculative Streaming），这是一种单模型推测性解码方法，通过将微调目标从下一个词预测改为未来n-gram预测，将草稿功能融合到目标模型中。在摘要生成、结构化查询和意义表示等多样化任务中，推测性流式处理将解码速度提高了1.8至3.1倍，且不牺牲生成质量。此外，推测性流式处理具有参数效率。它在速度提升方面与Medusa风格的架构相当甚至更高，同时使用的额外参数减少了约10000倍，非常适合资源受限的设备。

### Tandem Transformers for Inference Efficient LLMs
- **Authors**: Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli
- **Published**: 2024-02-13
- **Link**: [http://arxiv.org/abs/2402.08644v3](http://arxiv.org/abs/2402.08644v3)
- **Summary**: The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.

- **中文摘要**: 传统大型语言模型（LLMs）的自回归特性本质上限制了推理速度，因为令牌是按顺序生成的。尽管推测性和并行解码技术试图缓解这一问题，但它们面临局限性：要么依赖生成准确性较低的小型模型，要么无法充分利用基础LLM的表示能力。我们引入了一种新颖的架构——Tandem transformers，以解决这些问题。该架构独特地结合了（1）一个小型自回归模型和（2）一个以块模式运行的大型模型（同时处理多个令牌）。通过赋予小型模型对大型模型更丰富表示的关注，其预测准确性得到了显著提升。在PaLM2预训练数据集上，PaLM2-Bison和PaLM2-Gecko的组合在下一个令牌预测准确性上比单独的PaLM2-Gecko提高了3.3%，与下游性能相当的PaLM2-Otter模型相比，提供了1.16倍的加速。我们进一步将这种组合模型整合到推测性解码（SPEED）框架中，其中大型模型验证来自小型模型的令牌。这确保了PaLM2-Bison和PaLM2-Gecko的组合在保持下游任务准确性不变的同时，实现了显著的加速（比在SPEED中使用原始PaLM2-Gecko快约1.14倍）。

### Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding
- **Authors**: Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, William Brandon
- **Published**: 2024-02-07
- **Link**: [http://arxiv.org/abs/2402.05109v1](http://arxiv.org/abs/2402.05109v1)
- **Summary**: To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design space of Hydra head training objectives and architectures, and propose a carefully-tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by 1.31x and 2.71x compared to Medusa decoding and autoregressive decoding, respectively. Overall, Hydra heads are a simple intervention on standard draft heads that significantly improve the end-to-end speed of draft head based speculative decoding.

- **中文摘要**: 为了应对自回归大型语言模型（LLM）推理中内存带宽受限的特性，先前的研究提出了推测性解码框架。为了执行推测性解码，一个小型的草稿模型会提出输入序列的候选延续，然后由基础模型并行验证这些候选延续。最近在Medusa解码框架中使用的一种指定草稿模型的方法是，将其作为一个轻量级头部的集合，称为草稿头，这些头部在基础模型的隐藏状态上进行操作。迄今为止，所有现有的草稿头都是顺序独立的，这意味着它们在候选延续中推测的标记与候选延续中的任何先前标记无关。在这项工作中，我们提出了Hydra头，这是一种顺序依赖的、标准草稿头的即插即用替代品，显著提高了推测的准确性。使用Hydra头进行解码相比使用标准草稿头的Medusa解码提高了吞吐量。我们进一步探索了Hydra头训练目标和架构的设计空间，并提出了一种精心调优的Hydra头配方，我们称之为Hydra++，相比Medusa解码和自回归解码，分别提高了1.31倍和2.71倍的解码吞吐量。总的来说，Hydra头是对标准草稿头的简单干预，显著提高了基于草稿头的推测性解码的端到端速度。

### Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation
- **Authors**: Luca Beurer-Kellner, Marc Fischer, Martin Vechev
- **Published**: 2024-02-07
- **Link**: [http://arxiv.org/abs/2403.06988v1](http://arxiv.org/abs/2403.06988v1)
- **Summary**: To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin.

- **中文摘要**: 为了确保大型语言模型（LLMs）生成的文本符合预期格式，受限解码提出在生成过程中强制执行严格的正式语言约束。然而，正如我们在本文中所展示的，这些方法不仅在生成过程中带来了性能开销，而且在未能正确对齐底层LLM子词词汇与外部约束的情况下，还会显著损害任务准确性。为了解决这一问题，我们提出了一种新颖的解码算法——DOMINO，它能够在完全子词对齐的方式下强制执行约束，同时利用预计算和推测性解码，实现几乎无开销，甚至在某些情况下比无约束解码快近2倍——从而大幅超越现有方法。

### GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding
- **Authors**: Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, Yang You
- **Published**: 2024-02-03
- **Link**: [http://arxiv.org/abs/2402.02082v1](http://arxiv.org/abs/2402.02082v1)
- **Summary**: Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.

- **中文摘要**: 推测性解码是一种相对较新的解码框架，它利用小型且高效的草稿模型来减少大型语言模型（LLM）的延迟。在本研究中，我们引入了GliDe和CaPE，这两个对传统推测性解码的低成本改进，以进一步提高冻结LLM的解码速度。具体而言，GliDe是一种修改后的草稿模型架构，它重用了目标LLM的缓存键和值，而CaPE则是一种提案扩展方法，利用草稿模型的置信度分数来帮助选择额外的候选令牌进行验证。在不同基准上的广泛实验表明，我们提出的GliDe草稿模型显著降低了预期的解码延迟。通过使用实际时间（walltime）进行的额外评估显示，GliDe可以将Vicuna模型的速度提升至2.17倍，而结合CaPE后，这一改进可以进一步扩展至2.61倍。我们将发布我们的代码、数据以及训练好的草稿模型。

### Break the Sequential Dependency of LLM Inference Using Lookahead Decoding
- **Authors**: Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang
- **Published**: 2024-02-03
- **Link**: [http://arxiv.org/abs/2402.02057v1](http://arxiv.org/abs/2402.02057v1)
- **Summary**: Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding

- **中文摘要**: 大型语言模型（LLMs）的自回归解码受限于内存带宽，导致高延迟和现代加速器并行处理能力的大量浪费。现有的加速LLM解码的方法通常需要一个草稿模型（例如，推测性解码），这不容易获得且无法泛化。在本文中，我们介绍了Lookahead解码，这是一种精确的、并行的解码算法，能够在不需要辅助模型或数据存储的情况下加速LLM解码。它允许通过减少总解码步骤来交换每一步的对数（FLOPs），在单个或多个现代加速器上更具并行性，并且与并发内存高效的注意力机制（例如，FlashAttention）兼容。我们的Lookahead解码实现可以在MT-bench上将自回归解码速度提升至多1.8倍，在多GPU上的代码完成任务中通过强扩展性提升至多4倍。我们的代码可在https://github.com/hao-ai-lab/LookaheadDecoding获取。

### Decoding Speculative Decoding
- **Authors**: Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman
- **Published**: 2024-02-02
- **Link**: [http://arxiv.org/abs/2402.01528v3](http://arxiv.org/abs/2402.01528v3)
- **Summary**: Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using speculative decoding and delineate the factors that affect the performance gain provided by speculative decoding. Our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model's capability in language modeling does not correlate strongly with its performance in speculative decoding. Based on these insights we explore a new design space for draft models and design hardware-efficient draft models for speculative decoding. Our newly designed draft model for LLaMA-65B can provide 111% higher throughput than existing draft models and can generalize further to the LLaMA-2 model family and supervised fine-tuned models.

- **中文摘要**: 推测性解码是一种广泛使用的技术，可以在不牺牲质量的情况下加快大型语言模型（LLMs）的推理速度。在进行推理时，推测性解码使用一个较小的草稿模型来生成推测性标记，然后使用目标LLM来验证这些草稿标记。推测性解码提供的加速效果在很大程度上取决于草稿模型的选择。在这项工作中，我们进行了详细的研究，包括对LLaMA-65B和OPT-66B进行的超过350次实验，使用推测性解码并阐明了影响推测性解码性能增益的因素。我们的实验表明，推测性解码的性能在很大程度上取决于草稿模型的延迟，而草稿模型在语言建模方面的能力与其在推测性解码中的表现没有很强的相关性。基于这些见解，我们探索了草稿模型的新设计空间，并设计了适用于推测性解码的硬件高效草稿模型。我们为LLaMA-65B新设计的草稿模型可以比现有草稿模型提供111%的更高吞吐量，并且可以进一步推广到LLaMA-2模型家族和监督微调模型。

### MambaByte: Token-free Selective State Space Model
- **Authors**: Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush
- **Published**: 2024-01-24
- **Link**: [http://arxiv.org/abs/2401.13660v3](http://arxiv.org/abs/2401.13660v3)
- **Summary**: Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.

- **中文摘要**: 无令牌语言模型直接从原始字节中学习，并消除了子词分词带来的归纳偏差。然而，以字节为单位进行操作会导致序列长度显著增加。在这种情况下，标准的自回归Transformer在扩展性方面表现不佳，因为所需的有效内存会随着序列长度的增加而增长。最近开发的Mamba状态空间模型（SSM）提供了一种有吸引力的替代方法，它具有固定大小的内存状态和高效的解码能力。我们提出了MambaByte，这是一种无令牌的Mamba SSM自回归训练在字节序列上的适应模型。在模型性能方面，我们展示了MambaByte在与最先进的子词Transformer进行语言建模任务时具有竞争力，甚至在某些情况下表现更优，同时保持了无令牌语言模型的优势，如对噪声的鲁棒性。在效率方面，我们开发了一种基于推测解码的适应方法，结合了令牌化草稿和字节级验证。这使得标准MambaByte实现的推理速度提高了2.6倍，显示出与子词Mamba相似的解码效率。这些发现确立了SSM在实现无令牌语言建模方面的可行性。

### Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads
- **Authors**: Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao
- **Published**: 2024-01-19
- **Link**: [http://arxiv.org/abs/2401.10774v3](http://arxiv.org/abs/2401.10774v3)
- **Summary**: Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.   Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.

- **中文摘要**: 大型语言模型（LLMs）采用自回归解码，这种解码方式需要顺序计算，每一步的计算都依赖于上一步的输出。这造成了一个瓶颈，因为每一步都需要将完整的模型参数从高带宽内存（HBM）移动到加速器的缓存中。尽管已经提出了如推测性解码等方法来解决这个问题，但它们的实施受到了获取和维护一个单独的草稿模型的挑战的阻碍。在本文中，我们提出了Medusa，这是一种高效的方法，通过添加额外的解码头来增强LLM的推理能力，以并行预测多个后续的token。利用基于树的注意力机制，Medusa在每个解码步骤中构建多个候选的延续，并同时验证它们。通过利用并行处理，Medusa显著减少了所需的解码步骤数量。我们为Medusa提供了两种级别的微调程序，以满足不同使用场景的需求：Medusa-1：Medusa直接在冻结的骨干LLM之上进行微调，实现了无损的推理加速。Medusa-2：Medusa与骨干LLM一起进行微调，使得Medusa头的预测准确性更高，加速效果更好，但需要一种特殊的训练方法来保持骨干模型的能力。此外，我们提出了几种扩展，以改进或扩展Medusa的实用性，包括一种自蒸馏方法，用于在没有训练数据的情况下处理情况，以及一种典型的接受方案，以提高接受率同时保持生成质量。我们在不同大小和训练过程的模型上评估了Medusa。我们的实验表明，Medusa-1可以在不牺牲生成质量的情况下实现超过2.2倍的加速，而Medusa-2进一步将加速效果提升到2.3-3.6倍。

### Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding
- **Authors**: Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui
- **Published**: 2024-01-15
- **Link**: [http://arxiv.org/abs/2401.07851v3](http://arxiv.org/abs/2401.07851v3)
- **Summary**: To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.

- **中文摘要**: 为了缓解大型语言模型（LLMs）中自回归解码带来的高推理延迟问题，推测性解码作为一种新颖的LLM推理解码范式应运而生。在每个解码步骤中，该方法首先高效地草拟出几个未来标记，然后并行地进行验证。与自回归解码不同，推测性解码支持每一步同时解码多个标记，从而加速推理过程。本文对这一有前景的解码范式进行了全面的概述和分析。我们首先提供了推测性解码的正式定义和公式化描述。接着，我们深入探讨了其关键方面，如草稿模型选择和验证策略。此外，我们还对第三方测试环境中领先的方法进行了比较分析。我们期望这项工作能够成为推动推测性解码进一步研究的催化剂，最终为更高效的LLM推理做出贡献。

### APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding
- **Authors**: Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, Yuxiao Dong
- **Published**: 2024-01-12
- **Link**: [http://arxiv.org/abs/2401.06761v1](http://arxiv.org/abs/2401.06761v1)
- **Summary**: The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks.

- **中文摘要**: 大规模采用大型语言模型（LLMs）需要高效的部署策略。然而，大多数LLMs生成文本所依赖的自回归解码过程，对实现高效服务提出了挑战。在这项工作中，我们引入了一种并行自回归生成方法。通过在包含层次结构的一般领域数据上进行指令微调，我们使LLMs能够独立规划其生成过程，并执行自动并行自回归（APAR）生成，显著减少了生成步骤的数量。仅APAR就能实现高达2倍的速度提升，而与推测性解码结合时，速度提升可达4倍。此外，APAR还减少了生成过程中的键值缓存消耗和注意力计算。与最先进的部署框架相比，在高吞吐量场景下，这带来了20-70%的吞吐量增加和20-35%的延迟减少。

### Multi-Candidate Speculative Decoding
- **Authors**: Sen Yang, Shujian Huang, Xinyu Dai, Jiajun Chen
- **Published**: 2024-01-12
- **Link**: [http://arxiv.org/abs/2401.06706v1](http://arxiv.org/abs/2401.06706v1)
- **Summary**: Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding.

- **中文摘要**: 大型语言模型在多种自然语言处理任务中展现了令人瞩目的能力，但其自回归生成文本的过程耗时较长。加速这一过程的一种方法是推测性解码，它通过一个快速的草稿模型生成候选片段（即一系列标记），然后由目标模型并行验证这些候选片段。然而，候选标记的接受率受到多种因素的限制，如模型、数据集和解码设置。本文提出从草稿模型中采样多个候选片段，并将其组织成批次进行验证。我们设计了高效的多种候选验证算法，同时保持目标模型的分布特性。我们的方法在多个数据集和模型上显著提高了接受率，持续优于标准的推测性解码方法。

### Cascade Speculative Drafting for Even Faster LLM Inference
- **Authors**: Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Kevin Chen-Chuan Chang, Jie Huang
- **Published**: 2023-12-18
- **Link**: [http://arxiv.org/abs/2312.11462v4](http://arxiv.org/abs/2312.11462v4)
- **Summary**: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves up to an 81 percent additional speedup over speculative decoding in our experiments, while maintaining the same output distribution as the target model. Our code is publicly available at https://github.com/lfsszd/CS-Drafting.

- **中文摘要**: 为了提高大型语言模型（LLM）推理的效率，推测性解码（Speculative Decoding）引入了由较小模型生成草稿的机制。随后，一个更大的目标模型会对这个草稿进行审查，以确保其输出与目标模型一致。一旦目标模型接受了这个草稿，就会减少目标模型的运行次数，从而提升整体效率。然而，推测性解码中的草稿生成过程包含了缓慢的自回归生成，并且无论生成的标记的重要性如何，都分配了相同的时间。这些低效因素共同导致了推测性解码性能的欠佳。为了进一步优化LLM推理，我们提出了级联推测草稿（Cascade Speculative Drafting，简称CS Drafting），这是一种结合了两种级联类型的推测执行算法。垂直级联消除了神经模型中的自回归生成，而水平级联则优化了草稿生成中的时间分配，以提高效率。通过结合这两种级联，CS Drafting在我们的实验中实现了比推测性解码高达81%的额外加速，同时保持了与目标模型相同的输出分布。我们的代码已在https://github.com/lfsszd/CS-Drafting公开发布。

### Speculative Contrastive Decoding
- **Authors**: Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou
- **Published**: 2023-11-15
- **Link**: [http://arxiv.org/abs/2311.08981v2](http://arxiv.org/abs/2311.08981v2)
- **Summary**: Large language models~(LLMs) exhibit exceptional performance in language tasks, yet their auto-regressive inference is limited due to high computational requirements and is sub-optimal due to the exposure bias. Inspired by speculative decoding and contrastive decoding, we introduce Speculative Contrastive Decoding~(SCD), a straightforward yet powerful decoding approach that leverages predictions from smaller language models~(LMs) to achieve both decoding acceleration and quality improvement. Extensive evaluations and analyses on four diverse language tasks demonstrate the effectiveness of SCD, showing that decoding efficiency and quality can compatibly benefit from one smaller LM.

- **中文摘要**: 大型语言模型（LLMs）在语言任务中表现出色，但由于高计算需求和暴露偏差，其自回归推理受到限制且效果欠佳。受推测性解码和对比解码的启发，我们提出了推测性对比解码（Speculative Contrastive Decoding，简称SCD），这是一种简单而强大的解码方法，利用较小语言模型（LMs）的预测来实现解码加速和质量提升。通过对四种不同语言任务的广泛评估和分析，我们展示了SCD的有效性，表明解码效率和质量可以同时受益于一个较小的LM。

### REST: Retrieval-Based Speculative Decoding
- **Authors**: Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, Di He
- **Published**: 2023-11-14
- **Link**: [http://arxiv.org/abs/2311.08252v2](http://arxiv.org/abs/2311.08252v2)
- **Summary**: We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm designed to speed up language model generation. The key insight driving the development of REST is the observation that the process of text generation often includes certain common phases and patterns. Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens. This method draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context. Its plug-and-play nature allows for seamless integration and acceleration of any language models, all without necessitating additional training. When benchmarked on 7B and 13B language models in a single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on code or text generation. The code of REST is available at https://github.com/FasterDecoding/REST.

- **中文摘要**: 我们介绍了一种名为基于检索的推测解码（Retrieval-Based Speculative Decoding, REST）的新算法，旨在加速语言模型的生成过程。推动REST开发的关键洞察在于观察到文本生成过程通常包含某些常见的阶段和模式。与以往依赖草稿语言模型进行推测解码的方法不同，REST利用检索的力量来生成草稿标记。这种方法从现有知识库中汲取信息，根据当前上下文检索并使用相关标记。其即插即用的特性使得任何语言模型都能无缝集成并加速，且无需额外的训练。在单批次设置下，对7B和13B语言模型进行基准测试时，REST在代码或文本生成方面实现了1.62倍到2.36倍的显著加速。REST的代码可在https://github.com/FasterDecoding/REST获取。

### Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling
- **Authors**: Sanchit Gandhi, Patrick von Platen, Alexander M. Rush
- **Published**: 2023-11-01
- **Link**: [http://arxiv.org/abs/2311.00430v1](http://arxiv.org/abs/2311.00430v1)
- **Summary**: As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. In this work, we leverage pseudo-labelling to assemble a large-scale open-source dataset which we use to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, we select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51% fewer parameters, while performing to within 1% WER on out-of-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. To facilitate further research in this domain, we make our training code, inference code and models publicly accessible.

- **中文摘要**: 随着预训练语音识别模型规模的增大，在低延迟或资源受限的环境中运行这些大型模型变得具有挑战性。在这项工作中，我们利用伪标签技术构建了一个大规模的开源数据集，并使用该数据集将Whisper模型蒸馏成一个更小的变体，称为Distil-Whisper。通过简单的词错误率（WER）启发式方法，我们仅选择最高质量的伪标签进行训练。蒸馏后的模型在参数减少51%的情况下，速度提高了5.8倍，同时在零样本迁移设置下，对分布外测试数据的WER表现仅下降1%。Distil-Whisper保持了Whisper模型对困难声学条件的鲁棒性，同时在长音频上更不容易出现幻觉错误。Distil-Whisper设计用于与Whisper模型配对进行推测性解码，实现了2倍的速度提升，同时在数学上确保与原始模型输出相同的结果。为了促进该领域的进一步研究，我们将训练代码、推理代码和模型公开发布。

### The Synergy of Speculative Decoding and Batching in Serving Large Language Models
- **Authors**: Qidong Su, Christina Giannoula, Gennady Pekhimenko
- **Published**: 2023-10-28
- **Link**: [http://arxiv.org/abs/2310.18813v1](http://arxiv.org/abs/2310.18813v1)
- **Summary**: Large Language Models (LLMs) like GPT are state-of-the-art text generation models that provide significant assistance in daily routines. However, LLM execution is inherently sequential, since they only produce one token at a time, thus incurring low hardware utilization on modern GPUs. Batching and speculative decoding are two techniques to improve GPU hardware utilization in LLM inference. To study their synergy, we implement a prototype implementation and perform an extensive characterization analysis on various LLM models and GPU architectures. We observe that the optimal speculation length depends on the batch size used. We analyze the key observation and build a quantitative model to explain it. Based on our analysis, we propose a new adaptive speculative decoding strategy that chooses the optimal speculation length for different batch sizes. Our evaluations show that our proposed method can achieve equal or better performance than the state-of-the-art speculation decoding schemes with fixed speculation length.

- **中文摘要**: 像GPT这样的大型语言模型（LLMs）是先进的文本生成模型，为日常工作提供了显著的帮助。然而，LLM的执行本质上是有序的，因为它们一次只能生成一个标记，因此在现代GPU上的硬件利用率较低。批处理和推测性解码是两种提高LLM推理中GPU硬件利用率的技术。为了研究它们的协同作用，我们实现了一个原型，并对各种LLM模型和GPU架构进行了广泛的特性分析。我们观察到，最佳的推测长度取决于所使用的批次大小。我们分析了这一关键观察结果，并建立了一个定量模型来解释它。基于我们的分析，我们提出了一种新的自适应推测性解码策略，该策略为不同的批次大小选择最佳的推测长度。我们的评估表明，我们提出的方法可以实现与现有最先进的具有固定推测长度的推测性解码方案相等或更好的性能。

### SpecTr: Fast Speculative Decoding via Optimal Transport
- **Authors**: Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu
- **Published**: 2023-10-23
- **Link**: [http://arxiv.org/abs/2310.15141v2](http://arxiv.org/abs/2310.15141v2)
- **Summary**: Autoregressive sampling from large language models has led to state-of-the-art results in several natural language tasks. However, autoregressive sampling generates tokens one at a time making it slow, and even prohibitive in certain tasks. One way to speed up sampling is $\textit{speculative decoding}$: use a small model to sample a $\textit{draft}$ (block or sequence of tokens), and then score all tokens in the draft by the large language model in parallel. A subset of the tokens in the draft are accepted (and the rest rejected) based on a statistical method to guarantee that the final output follows the distribution of the large model. In this work, we provide a principled understanding of speculative decoding through the lens of optimal transport (OT) with $\textit{membership cost}$. This framework can be viewed as an extension of the well-known $\textit{maximal-coupling}$ problem. This new formulation enables us to generalize the speculative decoding method to allow for a set of $k$ candidates at the token-level, which leads to an improved optimal membership cost. We show that the optimal draft selection algorithm (transport plan) can be computed via linear programming, whose best-known runtime is exponential in $k$. We then propose a valid draft selection algorithm whose acceptance probability is $(1-1/e)$-optimal multiplicatively. Moreover, it can be computed in time almost linear with size of domain of a single token. Using this $new draft selection$ algorithm, we develop a new autoregressive sampling algorithm called $\textit{SpecTr}$, which provides speedup in decoding while ensuring that there is no quality degradation in the decoded output. We experimentally demonstrate that for state-of-the-art large language models, the proposed approach achieves a wall clock speedup of 2.13X, a further 1.37X speedup over speculative decoding on standard benchmarks.

- **中文摘要**: 从大型语言模型中进行自回归采样在多个自然语言任务中取得了最先进的结果。然而，自回归采样一次生成一个标记，这使得它速度缓慢，甚至在某些任务中变得不可行。加速采样的一种方法是$\textit{推测性解码}$：使用一个小模型来生成一个$\textit{草稿}$（标记块或序列），然后通过大型语言模型并行地对草稿中的所有标记进行评分。根据一种统计方法，草稿中的一部分标记被接受（其余被拒绝），以保证最终输出遵循大型模型的分布。在这项工作中，我们通过最优传输（OT）与$\textit{成员成本}$的视角，为推测性解码提供了原则性的理解。这个框架可以被视为著名的$\textit{最大耦合}$问题的扩展。这种新的表述使我们能够将推测性解码方法推广到允许在标记级别上有一组$k$个候选者，从而导致改进的最优成员成本。我们表明，最优草稿选择算法（传输计划）可以通过线性规划计算，其已知的最优运行时间是$k$的指数级。然后，我们提出了一种有效的草稿选择算法，其接受概率是$(1-1/e)$倍的最优。此外，它可以在与单个标记的域大小几乎成线性的时间内计算出来。使用这种$\textit{新的草稿选择}$算法，我们开发了一种新的自回归采样算法，称为$\textit{SpecTr}$，它在解码过程中提供了加速，同时确保解码输出的质量没有下降。我们通过实验证明，对于最先进的大型语言模型，所提出的方法在标准基准测试中实现了2.13倍的挂钟加速，比推测性解码进一步加速了1.37倍。

### DistillSpec: Improving Speculative Decoding via Knowledge Distillation
- **Authors**: Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, Rishabh Agarwal
- **Published**: 2023-10-12
- **Link**: [http://arxiv.org/abs/2310.08461v2](http://arxiv.org/abs/2310.08461v2)
- **Summary**: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.

- **中文摘要**: 推测性解码（Speculative Decoding, SD）通过使用更快的草稿模型生成多个标记，然后由更大的目标模型并行验证这些标记，从而加速大型语言模型的推理过程，最终生成符合目标模型分布的文本。然而，识别一个与目标模型高度一致的紧凑草稿模型是一个挑战。为了解决这一问题，我们提出了DistillSpec，该方法在应用SD之前，利用知识蒸馏技术更好地将草稿模型与目标模型对齐。DistillSpec做出了两个关键设计选择，我们通过系统的研究证明，这两个选择对于提高草稿模型与目标模型的对齐度至关重要：利用草稿模型生成的策略数据，以及根据任务和解码策略定制差异函数。值得注意的是，DistillSpec在标准基准测试中，无论是采用贪婪采样还是非贪婪采样，都比标准的SD实现了显著的10-45%的速度提升。此外，我们将DistillSpec与有损SD结合，实现了对延迟与任务性能之间权衡的精细控制。最后，在实际应用场景中，面对不同大小的模型，首先通过蒸馏提升目标模型的性能，然后应用DistillSpec训练一个高度对齐的草稿模型，相比不使用蒸馏的标准解码方法，可以将解码延迟降低6-10倍，同时性能下降最小。

### MatFormer: Nested Transformer for Elastic Inference
- **Authors**: Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain
- **Published**: 2023-10-11
- **Link**: [http://arxiv.org/abs/2310.07707v1](http://arxiv.org/abs/2310.07707v1)
- **Summary**: Transformer models are deployed in a wide range of settings, from multi-accelerator clusters to standalone mobile phones. The diverse inference constraints in these scenarios necessitate practitioners to train foundation models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes. Due to significant training costs, only a select few model sizes are trained and supported, limiting more fine-grained control over relevant tradeoffs, including latency, cost, and accuracy. This work introduces MatFormer, a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks. This training procedure allows for the Mix'n'Match of model granularities across layers -- i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models, which were never explicitly optimized. We empirically demonstrate MatFormer's effectiveness across different model classes (decoders & encoders), modalities (language & vision), and scales (up to 2.6B parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM) allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting comparable validation loss and one-shot downstream evaluations to their independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and consistent submodels extracted from MatFormer can further reduce inference latency.

- **中文摘要**: Transformer模型被部署在各种环境中，从多加速器集群到独立的移动设备。这些场景中的多样化推理约束要求从业者训练一系列不同大小的基础模型，如PaLM 2、Llama和ViTs。由于训练成本高昂，只有少数几个模型大小被训练和维护，限制了对包括延迟、成本和准确性在内的相关权衡的更精细控制。本研究引入了MatFormer，这是一种嵌套的Transformer架构，旨在在各种部署约束下提供弹性。MatFormer模型的每个前馈网络（FFN）块都与几个嵌套的较小FFN块共同优化。这种训练过程允许在不同层之间混合匹配模型粒度——即，一个经过训练的通用MatFormer模型能够提取出数百个从未明确优化过的较小但准确度高的模型。我们通过实证证明了MatFormer在不同模型类别（解码器和编码器）、模态（语言和视觉）以及规模（高达26亿参数）上的有效性。我们发现，一个26亿参数的仅解码器MatFormer语言模型（MatLM）使我们能够提取从15亿到26亿参数的小型模型，每个模型在验证损失和一次性下游评估中都与其独立训练的对应模型表现相当。此外，我们观察到，从通用MatFormer基础的ViT（MatViT）编码器中提取的小型编码器保留了适应大规模检索的度量空间结构。最后，我们展示了使用从MatFormer中提取的准确且一致的子模型进行推测性解码，可以进一步减少推理延迟。

### Online Speculative Decoding
- **Authors**: Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, Hao Zhang
- **Published**: 2023-10-11
- **Link**: [http://arxiv.org/abs/2310.07177v4](http://arxiv.org/abs/2310.07177v4)
- **Summary**: Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding to address this challenge. The main idea is to continuously update the (multiple) draft model(s) on observed user query data. Adapting to query distribution mitigates the shifts between the training distribution of the draft model and the query distribution, enabling the draft model to more accurately predict the target model's outputs. We develop a prototype of online speculative decoding based on knowledge distillation and evaluate it using both synthetic and real query data. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, bringing 1.42x to 2.17x latency reduction. Our code is available at https://github.com/LiuXiaoxuanPKU/OSD.

- **中文摘要**: 推测性解码是一种关键技术，通过使用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLMs）的推理。然而，由于草稿模型的预测准确性较低，尤其是在面对多样化的文本输入以及草稿模型与目标模型之间存在显著能力差距时，其效果可能受到限制。我们引入了在线推测性解码来应对这一挑战。其主要思想是持续在观察到的用户查询数据上更新（多个）草稿模型。适应查询分布可以缓解草稿模型的训练分布与查询分布之间的偏移，使草稿模型能够更准确地预测目标模型的输出。我们基于知识蒸馏开发了在线推测性解码的原型，并使用合成和真实的查询数据对其进行了评估。结果显示，令牌接受率显著提高了0.1至0.65，带来了1.42倍至2.17倍的延迟减少。我们的代码可在https://github.com/LiuXiaoxuanPKU/OSD获取。

### Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding
- **Authors**: Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad Mehrotra
- **Published**: 2023-09-15
- **Link**: [http://arxiv.org/abs/2309.08168v2](http://arxiv.org/abs/2309.08168v2)
- **Summary**: We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting. Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM. Moreover, the proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its variants demonstrated a speedup up to 1.99$\times$.

- **中文摘要**: 我们提出了一种新颖的推理方案——自推测解码，用于加速大型语言模型（LLMs），而无需辅助模型。该方法的特点是分为两个阶段：草稿生成和验证。草稿生成阶段以稍低但更快的质量生成草稿标记，这是通过在草稿生成过程中有选择地跳过某些中间层来实现的。随后，验证阶段使用原始LLM在一次前向传递中验证这些草稿输出标记。这一过程确保最终输出与未经修改的LLM生成的输出完全一致。此外，所提出的方法不需要额外的神经网络训练，也不增加额外的内存占用，使其成为一种即插即用且成本效益高的推理加速解决方案。在LLaMA-2及其变体的基准测试中，速度提升最高可达1.99倍。

### Accelerating LLM Inference with Staged Speculative Decoding
- **Authors**: Benjamin Spector, Chris Re
- **Published**: 2023-08-08
- **Link**: [http://arxiv.org/abs/2308.04623v1](http://arxiv.org/abs/2308.04623v1)
- **Summary**: Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of speculative decoding. Taken together, we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L model while perfectly preserving output quality.

- **中文摘要**: 大型语言模型（LLM）的最新进展展示了其多样化的能力。我们提出了一种新颖的算法——分阶段推测性解码，旨在加速LLM在小批量、设备端场景中的推理。我们通过改进先前的推测性解码工作，解决了小批量推理的低算术强度问题。首先，我们将推测性批量重构为树结构，从而降低了生成成本并增加了每批次的预期token数量。其次，我们引入了第二阶段的推测性解码。综合来看，我们在完美保留输出质量的同时，使用762M参数的GPT-2-L模型将单批次解码延迟减少了3.16倍。

### Speculative Decoding with Big Little Decoder
- **Authors**: Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, Kurt Keutzer
- **Published**: 2023-02-15
- **Link**: [http://arxiv.org/abs/2302.07863v4](http://arxiv.org/abs/2302.07863v4)
- **Summary**: The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment and makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially without leveraging token-level parallelization. To address this, we propose Big Little Decoder (BiLD), a framework that can improve inference efficiency and latency for a wide range of text generation applications. The BiLD framework contains two models with different sizes that collaboratively generate text. The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally to refine the small model's inaccurate predictions in a non-autoregressive manner. To coordinate the small and large models, BiLD introduces two simple yet effective policies: (1) the fallback policy that determines when to hand control over to the large model; and (2) the rollback policy that determines when the large model needs to correct the small model's inaccurate predictions. To evaluate our framework across different tasks and models, we apply BiLD to various text generation scenarios encompassing machine translation on IWSLT 2017 De-En and WMT 2014 De-En, and summarization on XSUM and CNN/DailyMail. On an NVIDIA T4 GPU, our framework achieves a speedup of up to 2.12x speedup with minimal generation quality degradation. Furthermore, our framework is fully plug-and-play and can be applied without any modifications in the training process or model architecture. Our code is open-sourced

- **中文摘要**: 基于Transformer架构的大型语言模型（LLM）的最新出现，极大地推动了自然语言处理领域的发展。然而，这些模型的推理延迟较长，限制了它们的部署，并使得它们在各种实时应用中变得过于昂贵。由于自回归生成任务需要模型迭代运行以顺序生成标记，而无法利用标记级别的并行化，因此推理延迟进一步加剧。为了解决这一问题，我们提出了Big Little Decoder（BiLD）框架，该框架能够提高广泛文本生成应用的推理效率和延迟。BiLD框架包含两个不同大小的模型，它们协同生成文本。小模型以自回归方式生成文本，具有较低的推理成本，而大模型则偶尔以非自回归方式调用，以修正小模型的不准确预测。为了协调小模型和大模型，BiLD引入了两个简单而有效的策略：（1）回退策略，决定何时将控制权交给大模型；（2）回滚策略，决定大模型何时需要修正小模型的不准确预测。为了在不同任务和模型上评估我们的框架，我们将BiLD应用于各种文本生成场景，包括IWSLT 2017 De-En和WMT 2014 De-En的机器翻译，以及XSUM和CNN/DailyMail的摘要生成。在NVIDIA T4 GPU上，我们的框架实现了最高2.12倍的加速，且生成质量几乎没有下降。此外，我们的框架是完全即插即用的，可以在不修改训练过程或模型架构的情况下应用。我们的代码已开源。

### Fast Inference from Transformers via Speculative Decoding
- **Authors**: Yaniv Leviathan, Matan Kalman, Yossi Matias
- **Published**: 2022-11-30
- **Link**: [http://arxiv.org/abs/2211.17192v2](http://arxiv.org/abs/2211.17192v2)
- **Summary**: Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.

- **中文摘要**: 像Transformer这样的大型自回归模型的推理速度较慢——解码K个token需要模型进行K次串行运行。在这项工作中，我们引入了推测性解码——一种通过并行计算多个token来加速自回归模型采样的算法，且无需对输出进行任何更改。我们方法的核心在于以下观察：(1) 复杂的语言建模任务通常包含可以通过更高效模型较好近似的简单子任务；(2) 通过使用推测执行和一种新颖的采样方法，我们可以在近似模型的输出上并行运行大型模型，从而实现精确解码的加速，可能同时生成多个token，且不改变分布。我们的方法无需重新训练或架构调整即可加速现有的现成模型。我们在T5-XXL上进行了验证，与标准的T5X实现相比，实现了2倍到3倍的加速，且输出完全相同。

### Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation
- **Authors**: Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, Zhifang Sui
- **Published**: 2022-03-30
- **Link**: [http://arxiv.org/abs/2203.16487v6](http://arxiv.org/abs/2203.16487v6)
- **Summary**: We propose Speculative Decoding (SpecDec), for the first time ever, to formally study exploiting the idea of speculative execution to accelerate autoregressive (AR) decoding. Speculative Decoding has two innovations: Spec-Drafter -- an independent model specially optimized for efficient and accurate drafting -- and Spec-Verification -- a reliable method for verifying the drafted tokens efficiently in the decoding paradigm. Experimental results on various seq2seq tasks including machine translation and abstractive summarization show our approach can achieve around $5\times$ speedup for the popular Transformer architectures with comparable generation quality to beam search decoding, refreshing the impression that the draft-then-verify paradigm introduces only $1.4\times$$\sim$$2\times$ speedup. In addition to the remarkable speedup, we also demonstrate 3 additional advantages of SpecDec, revealing its practical value for accelerating generative models in real-world applications. Our models and codes are available at https://github.com/hemingkx/SpecDec.

- **中文摘要**: 我们首次提出了推测性解码（Speculative Decoding, SpecDec），正式研究利用推测执行的思想来加速自回归（AR）解码。推测性解码包含两项创新：Spec-Drafter——一个专门优化用于高效且准确起草的独立模型，以及Spec-Verification——一种在解码范式中高效验证起草令牌的可靠方法。在包括机器翻译和抽象摘要在内的各种序列到序列任务上的实验结果表明，我们的方法能够在保持与束搜索解码相当生成质量的同时，为流行的Transformer架构实现约5倍的加速，刷新了起草-验证范式仅能带来1.4倍至2倍加速的印象。除了显著的加速效果外，我们还展示了SpecDec的3个额外优势，揭示了其在实际应用中加速生成模型的实用价值。我们的模型和代码可在https://github.com/hemingkx/SpecDec获取。

